{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First watch this: https://www.youtube.com/watch?v=JuH79E8rdKc \n",
        "\n",
        "</br>\n",
        "\n",
        "Mildenhall et al. 2020, NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis: https://arxiv.org/abs/2003.08934\n",
        "tensorflow implementation of tiny version: https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb\n",
        "\n",
        "Dataset: https://www.matthewtancik.com/nerf\n",
        "\n",
        "</br>\n",
        "\n",
        "Many cameras to 3d. We map position and direction to color and opacity  \n",
        "</br>\n",
        "$F_\\Theta : (x, y, z, \\theta, \\phi) \\mapsto (R, G, B, \\sigma)$. \n",
        "\n",
        "</br>\n",
        "\n",
        "Positional Encoding: \n",
        "\n",
        "</br>\n",
        "\n",
        "$\\gamma(p) = (\\sin(2^0 \\pi p), \\cos(2^0 \\pi p), \\dots, \\sin(2^{L-1} \\pi p), \\cos(2^{L-1} \\pi p)).$\n",
        "\n",
        "$\\gamma$  for each 5 input dimensions $(x, y, z, \\theta, \\phi)$. \n",
        "\n",
        "</br>\n",
        "\n",
        "camera ray $\\mathbf r(t) = \\mathbf o + t \\mathbf d$,\n",
        "\n",
        "origin $\\mathbf o$, \n",
        "\n",
        "direction $\\mathbf d$,\n",
        "\n",
        "color $\\mathbf c(\\mathbf r(t), \\mathbf d)$  \n",
        "\n",
        "density $\\sigma(\\mathbf r(t))$, \n",
        "\n",
        "render the expected color at $\\mathbf r(t)$ :\n",
        "\n",
        "</br>\n",
        "\n",
        "$C(\\mathbf r) = \\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf r(t)) \\mathbf c(\\mathbf r(t), \\mathbf d) dt,$\n",
        "\n",
        "$T(t) = \\exp \\left( - \\int_{t_n}^t \\sigma(\\mathbf r(s)) ds \\right),$\n",
        "\n",
        "</br>\n",
        "\n",
        "discretization of it:\n",
        "\n",
        "</br>\n",
        "\n",
        "$\\hat C(\\mathbf r) = \\sum_{i=1}^N T_i (1 - \\exp(- \\sigma_i \\delta_i)) \\mathbf c_i,$\n",
        "\n",
        "$T_i = \\exp \\left( - \\sum_{j=1}^{i-1} \\sigma_j \\delta_j \\right),$\n",
        "\n",
        "$\\delta_i = t_{i+1} - t_i$ "
      ],
      "metadata": {
        "id": "iHpreQ_wDKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def batch_generator(inputs, batch_size):\n",
        "    \"\"\"\n",
        "    Generates batches of `batch_size` from `inputs` array.\n",
        "    \"\"\"\n",
        "    l = inputs.shape[0]\n",
        "    for i in range(0, l, batch_size):\n",
        "        yield inputs[i:min(i + batch_size, l)]"
      ],
      "metadata": {
        "id": "rJcfNEJ4BdN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMje4aK7ZZXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    !wget https://bmild.github.io/nerf/tiny_nerf_data.npz\n",
        "\n",
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "testimg, testpose = images[101], poses[101]\n",
        "# use the first 100 images for training\n",
        "images = images[:100,...,:3]\n",
        "poses = poses[:100]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "plt.show()\n",
        "\n",
        "images = torch.from_numpy(images).to(device)\n",
        "poses = torch.from_numpy(poses).to(device)\n",
        "testimg = torch.from_numpy(testimg).to(device)\n",
        "testpose = torch.from_numpy(testpose).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rays(height, width, focal_length, cam2world):\n",
        "    \"\"\"\n",
        "    Compute the rays (origins and directions) passing through an image with \n",
        "    `height` and `width` (in pixels). `focal_length` (in pixels) is a property \n",
        "    of the camera. `cam2world` represents and transform tensor from a 3D point\n",
        "    in the \"camera\" frame of reference to the \"world\" frame of reference (the \n",
        "    `pose` in our dataset).\n",
        "    \"\"\"\n",
        "    i, j = torch.meshgrid(\n",
        "        torch.arange(width).to(cam2world),\n",
        "        torch.arange(height).to(cam2world),\n",
        "        indexing=\"xy\"\n",
        "    )\n",
        "    dirs = torch.stack([\n",
        "        (i.cpu() - width / 2) / focal_length,\n",
        "        - (j.cpu() - height / 2) / focal_length,\n",
        "        - torch.ones_like(i.cpu())\n",
        "    ], dim=-1).to(cam2world)\n",
        "    rays_d = torch.sum(dirs[..., None, :] * cam2world[:3, :3], dim=-1)\n",
        "    rays_o = cam2world[:3, -1].expand(rays_d.shape)\n",
        "    return rays_o, rays_d"
      ],
      "metadata": {
        "id": "IwTUrkq1fUJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(x, L_embed=6):\n",
        "    \"\"\"\n",
        "    Returns tensor representing positional encoding $\\gamma(x)$ of `x` with\n",
        "    `L_embed` corresponding to $L$ in the above.\n",
        "    \"\"\"\n",
        "    rets = [x]\n",
        "    for i in range(L_embed):\n",
        "        for fn in [torch.sin, torch.cos]:\n",
        "            rets.append(fn(2 ** i * x))\n",
        "    return torch.cat(rets, dim=-1)"
      ],
      "metadata": {
        "id": "Z3MuDPrzEgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyNeRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements 4 layer MLP as a tiny example of the NeRF design\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=128, L_embed=6):\n",
        "        super().__init__()\n",
        "        in_dim = 3 + 3 * 2 * L_embed\n",
        "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.layer3 = nn.Linear(hidden_dim + in_dim, hidden_dim)\n",
        "        self.layer4 = nn.Linear(hidden_dim, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.layer1(x))\n",
        "        out = F.relu(self.layer2(out))\n",
        "        out = F.relu(self.layer3(torch.cat([out, x], dim=-1)))\n",
        "        out = self.layer4(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "v0gx9TeKB7vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_rays(\n",
        "    model, rays_o, rays_d, near, far, N_samples, encoding_fn, rand=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Use `model` to render the rays parameterized by `rays_o` and `rays_d`\n",
        "    between `near` and `far` limits with `N_samples`.\n",
        "    \"\"\"\n",
        "    # sample query pts\n",
        "    z_vals = torch.linspace(near, far, N_samples).to(rays_o)\n",
        "    if rand:\n",
        "        z_vals = (\n",
        "            torch.rand(list(rays_o.shape[:-1]) + [N_samples]) \n",
        "            * (far - near) / N_samples\n",
        "        ).to(rays_o) + z_vals\n",
        "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
        "\n",
        "    # run query pts through model to get radiance fields\n",
        "    pts_flat = pts.reshape((-1, 3))\n",
        "    encoded_pts_flat = encoding_fn(pts_flat)\n",
        "    batches = batch_generator(encoded_pts_flat, batch_size=BATCH_SIZE)\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        preds.append(model(batch))\n",
        "    radiance_fields_flat = torch.cat(preds, dim=0)\n",
        "    radiance_fields = torch.reshape(\n",
        "        radiance_fields_flat, list(pts.shape[:-1]) + [4]\n",
        "    )\n",
        "\n",
        "    # compute densities and colors\n",
        "    sigma_a = F.relu(radiance_fields[..., 3])\n",
        "    rgb = torch.sigmoid(radiance_fields[..., :3])\n",
        "\n",
        "    # do volume rendering\n",
        "    oneE10 = torch.tensor([1e10], dtype=rays_o.dtype, device=rays_o.device)\n",
        "    dists = torch.cat([\n",
        "        z_vals[..., 1:] - z_vals[..., :-1],\n",
        "        oneE10.expand(z_vals[..., :1].shape)\n",
        "    ], dim=-1)\n",
        "    alpha = 1 - torch.exp(-sigma_a * dists)\n",
        "    weights = torch.roll(torch.cumprod(1 - alpha + 1e-10, dim=-1), 1, dims=-1)\n",
        "    weights[..., 0] = 1\n",
        "    weights = alpha * weights\n",
        "\n",
        "    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
        "    depth_map = (weights * z_vals).sum(dim=-1)\n",
        "    acc_map = weights.sum(dim=-1)\n",
        "    return rgb_map, depth_map, acc_map"
      ],
      "metadata": {
        "id": "7XtlYS7WRuK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "NUM_ENCODING_FUNCTIONS = 6\n",
        "NEAR = 2\n",
        "FAR = 6\n",
        "DEPTH_SAMPLES = 64\n",
        "LEARNING_RATE = 5e-3\n",
        "BATCH_SIZE = 16384\n",
        "NUM_EPOCHS = 1000\n",
        "DISPLAY_EVERY = 100\n",
        "HEIGHT, WIDTH = images.shape[1:3]\n",
        "FOCAL = data['focal']\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# initialize encoding function, model, loss, and optimizer\n",
        "encoding_fn = lambda x: positional_encoding(x, L_embed=NUM_ENCODING_FUNCTIONS)\n",
        "model = TinyNeRF(L_embed=NUM_ENCODING_FUNCTIONS)\n",
        "model.to(device)\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# for plotting the loss and iteration during training\n",
        "psnrs = []\n",
        "iternums = []\n",
        "\n",
        "for i in range(NUM_EPOCHS + 1):\n",
        "    # sample an image from our training set\n",
        "    img_idx = np.random.randint(images.shape[0]) \n",
        "    target = images[img_idx].to(device)\n",
        "    pose = poses[img_idx].to(device)\n",
        "\n",
        "    # get the rays passing through the image and forward pass the model\n",
        "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, pose)\n",
        "    rgb, _, _ = render_rays(\n",
        "        model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
        "        encoding_fn=encoding_fn\n",
        "    )\n",
        "\n",
        "    # backward pass\n",
        "    loss = loss_fn(rgb, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # plot the model's render of the test image and loss at each iteration\n",
        "    if i % DISPLAY_EVERY == 0:\n",
        "        rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, testpose)\n",
        "        rgb, _, _ = render_rays(\n",
        "            model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
        "            encoding_fn=encoding_fn\n",
        "        )\n",
        "        loss = loss_fn(rgb, testimg)\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "        psnr = -10 * torch.log10(loss)\n",
        "        psnrs.append(psnr.item())\n",
        "        iternums.append(i)\n",
        "\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(rgb.detach().cpu().numpy())\n",
        "        plt.title(f'Iteration: {i}')\n",
        "        plt.subplot(122)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        plt.title('PSNR')\n",
        "        plt.show()\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "hTj3DVGZRv_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some transformation tensors for translations and rotations about\n",
        "# different axes\n",
        "trans_t = lambda t : torch.tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "rot_phi = lambda phi : torch.tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,np.cos(phi),-np.sin(phi),0],\n",
        "    [0,np.sin(phi), np.cos(phi),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "rot_theta = lambda th : torch.tensor([\n",
        "    [np.cos(th),0,-np.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [np.sin(th),0, np.cos(th),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    \"\"\"\n",
        "    Compute a transformation tensor for a spherical coordinates\n",
        "    (`theta`, `phi`, `radius`)\n",
        "    \"\"\"\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w.numpy()\n",
        "    return c2w"
      ],
      "metadata": {
        "id": "413mojvo880z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run poses that encircle the object through our trained model and make a video\n",
        "frames = []\n",
        "for th in np.linspace(0., 360., 120, endpoint=False):\n",
        "    c2w = pose_spherical(th, -30, 4)\n",
        "    c2w = torch.from_numpy(c2w).to(device).float()\n",
        "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, c2w[:3,:4])\n",
        "    rgb, _, _ = render_rays(\n",
        "        model, rays_o, rays_d, NEAR, FAR, N_samples=DEPTH_SAMPLES,\n",
        "        encoding_fn=encoding_fn\n",
        "    )\n",
        "    frames.append((255*np.clip(rgb.cpu().detach().numpy(),0,1)).astype(np.uint8))\n",
        "\n",
        "import imageio\n",
        "f = 'video.mp4'\n",
        "imageio.mimwrite(f, frames, fps=30, quality=7)"
      ],
      "metadata": {
        "id": "m-86_Y3_9R8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed the video in the notebook\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "JJLKD0dY_Vgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}