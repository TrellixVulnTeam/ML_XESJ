{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First watch the explanations:\n",
        "\n",
        "https://www.youtube.com/watch?v=R39tWYYKNcI&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=38\n",
        "\n",
        "https://www.youtube.com/watch?v=ChoV5h7tw5A&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=38\n",
        "\n",
        "https://www.youtube.com/watch?v=xY-DMAJpIP4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=39\n",
        "\n",
        "https://www.youtube.com/watch?v=b1I5X3UfEYI&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=40\n",
        "\n",
        "https://www.youtube.com/watch?v=QgkLfjfGul8&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=41\n",
        "\n",
        "The algorithm is simple:\n",
        "\n",
        "1. Pass the input image through a pre-trained model.\n",
        "2. Extract the layer values at pre-defined layers.\n",
        "3. Pass the generated image (initially this is just copy of content image) through the model and extract its values at the\n",
        "same pre-defined layers.\n",
        "4. Calculate the content loss at each layer corresponding to the content image\n",
        "and generated image.\n",
        "5. Pass the style image through multiple layers of the model and calculate the\n",
        "gram matrix values of the style image.\n",
        "6. Pass the generated image through the same layers that the style image is\n",
        "passed through and calculate its corresponding gram matrix values.\n",
        "7. Extract the squared difference of the gram matrix values of the two images.\n",
        "This is the style loss.\n",
        "8. The overall loss is the weighted average of the style loss and content\n",
        "loss.\n",
        "9. The input image that minimizes the overall loss is the final image."
      ],
      "metadata": {
        "id": "CrmJPCAVk669"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Z8_f4nwsSuBW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms as T\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import requests\n",
        "\n",
        "from torchvision.models import vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gS8B0jxLTcX5"
      },
      "outputs": [],
      "source": [
        "# img_data = requests.get('https://onedrive.live.com/?authkey=%21ACaMtLcuyHivepE&cid=0A89CD65B585D519&id=A89CD65B585D519%21118&parId=A89CD65B585D519%21117&o=OneUp').content\n",
        "# with open('content_image.JPG', 'wb') as handler:\n",
        "#     handler.write(img_data)\n",
        "\n",
        "# img_data = requests.get('https://onedrive.live.com/?authkey=%21AOPHmN3mCPTgg3s&cid=0A89CD65B585D519&id=A89CD65B585D519%21120&parId=A89CD65B585D519%21117&o=OneUp').content\n",
        "# with open('style_image.PNG', 'wb') as handler:\n",
        "#     handler.write(img_data)\n",
        "\n",
        "# you can generally use this only if the url is just image ending something.png, something.jpg etc\n",
        "# !wget https://onedrive.live.com/?authkey=%21ACaMtLcuyHivepE&cid=0A89CD65B585D519&id=A89CD65B585D519%21118&parId=A89CD65B585D519%21117&o=OneUp -O content_image.jpg \n",
        "# !wget https://onedrive.live.com/?authkey=%21AOPHmN3mCPTgg3s&cid=0A89CD65B585D519&id=A89CD65B585D519%21120&parId=A89CD65B585D519%21117&o=OneUp -O style_image.png\n",
        "\n",
        "# here you have to use one drive sdk or download images from one drive and upload in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "He5DfS7HTPru"
      },
      "outputs": [],
      "source": [
        "# for best result create this statistics according to your data\n",
        "\n",
        "preprocess = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    T.Lambda(lambda x: x.mul_(255))\n",
        "])\n",
        "postprocess = T.Compose([\n",
        "    T.Lambda(lambda x: x.mul_(1./255)),\n",
        "    T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ZWfxO7pPTlVR"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "imgs = [Image.open(path).resize((512,512)).convert('RGB') for path in ['style_image.PNG', 'content_image.JPG']]\n",
        "style_image, content_image = [preprocess(img).to(device)[None] for img in imgs]\n",
        "\n",
        "opt_img = content_image.data.clone()\n",
        "opt_img.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XHkDr9SYTRso"
      },
      "outputs": [],
      "source": [
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b,c,h,w = input.size()\n",
        "        feat = input.view(b, c, h*w)\n",
        "        G = feat@feat.transpose(1,2)     #gram matrix, basically just unnormalized multiple covariotion\n",
        "        G.div_(h*w)\n",
        "        return G\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        out = F.mse_loss(GramMatrix()(input), target)\n",
        "        return(out)\n",
        "class vgg19_modified(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        features = list(vgg19(pretrained = True).features)\n",
        "        self.features = nn.ModuleList(features).eval() \n",
        "    def forward(self, x, layers=[]):\n",
        "        order = np.argsort(layers)\n",
        "        _results, results = [], []\n",
        "        for ix, model in enumerate(self.features):\n",
        "            x = model(x)\n",
        "            if ix in layers: _results.append(x)\n",
        "        for o in order: results.append(_results[o])\n",
        "        return results if layers is not [] else x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROz5hb3OTpB9"
      },
      "outputs": [],
      "source": [
        "vgg = vgg19_modified().to(device)\n",
        "\n",
        "style_layers = [0, 5, 10, 19, 28]     #which layers we want to use for style transfer learning\n",
        "content_layers = [21]                 #layers we use for content learning\n",
        "loss_layers = style_layers + content_layers\n",
        "\n",
        "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
        "loss_fns = [loss_fn.to(device) for loss_fn in loss_fns]\n",
        "\n",
        "style_weights = [1000/n**2 for n in [64,128,256,512,512]] \n",
        "content_weights = [1]\n",
        "weights = style_weights + content_weights\n",
        "\n",
        "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
        "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
        "targets = style_targets + content_targets\n",
        "\n",
        "max_iters = 100     #best is at least up to 400\n",
        "optimizer = torch.optim.LBFGS([opt_img])   # impements limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm\n",
        "                                           # remember initially opt_img is content_images copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UszIgCUYTwPn"
      },
      "outputs": [],
      "source": [
        "iters = 0\n",
        "trainloss = []\n",
        "while iters < max_iters:\n",
        "    def closure():\n",
        "        global iters\n",
        "        iters += 1\n",
        "        optimizer.zero_grad()\n",
        "        out = vgg(opt_img, loss_layers)\n",
        "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
        "        loss = sum(layer_losses)\n",
        "        loss.backward()\n",
        "        trainloss.append(loss.clone().detach())\n",
        "        print('Epoch {}/{} --- Loss: {:.4f}'.format(iters, max_iters, loss))\n",
        "        return loss\n",
        "    optimizer.step(closure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhxEfN37T0WD"
      },
      "outputs": [],
      "source": [
        "plt.plot(trainloss)\n",
        "plt.title('Loss');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ_Oh-bQU8zy"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img = postprocess(opt_img[0])     # postprocess(opt_img[0]).permute(1, 2, 0) if plt.imshow(img);\n",
        "\n",
        "T.ToPILImage()(img)\n"
      ]
    }
  ]
}