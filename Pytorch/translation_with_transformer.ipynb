{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data : https://www.statmt.org/ (see european parlaiment proceedings)\n",
        "\n",
        "REFERENCE: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "Got help : [here](https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb) and [here](https://github.com/dksifoua/Neural-Machine-Translation/blob/master/notebooks/5%20-%20SeqToSeq%20Model%20with%20Transformer.ipynb)"
      ],
      "metadata": {
        "id": "k_HdrmWMlpD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import tqdm\n",
        "import pickle\n",
        "from pickle import dump, load\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "j_Kf8tiNj6G6"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.statmt.org/europarl/v7/fr-en.tgz\n",
        "!tar -xf fr-en.tgz"
      ],
      "metadata": {
        "id": "ixjYxQp7DHe5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title preprocess raw data\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "    return doc.strip().split('\\n')\n",
        "\n",
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "    lengths = [len(s.split()) for s in sentences]\n",
        "    return min(lengths), max(lengths)\n",
        "\n",
        "# clean lines\n",
        "def clean_lines(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for line in lines:\n",
        "        # normalize unicode characters\n",
        "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
        "        line = line.decode('UTF-8')\n",
        "        # tokenize on white space\n",
        "        line = line.split()\n",
        "        # convert to lower case\n",
        "        line = [word.lower() for word in line]\n",
        "        # remove punctuation from each token\n",
        "        line = [word.translate(table) for word in line]\n",
        "        # remove non-printable chars form each token\n",
        "        line = [re_print.sub('', w) for w in line]\n",
        "        # remove tokens with numbers in them\n",
        "        line = [word for word in line if word.isalpha()]\n",
        "        # store as string\n",
        "        cleaned.append(' '.join(line))\n",
        "    return cleaned\n",
        "\n",
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences),\n",
        "minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'English.pkl'\n",
        "\n",
        "with open(filename,'wb') as fl:\n",
        "  outfile = fl\n",
        "  pickle.dump(cleanf,outfile)\n",
        "#outfile.close()             #do this with context manager\n",
        "  print(filename,\" saved\")\n",
        "\n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences),\n",
        "minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'French.pkl'\n",
        "with open(filename,'wb') as fl:\n",
        "  outfile = fl\n",
        "  pickle.dump(cleanf,outfile)\n",
        "  print(filename,\" saved\")\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    with open(filename, 'rb') as fl:\n",
        "      loaded = load(fl)\n",
        "    return loaded\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "    with open(filename, 'wb') as fl:\n",
        "        dump(sentences, fl)\n",
        "        print('Saved: %s' % filename)\n",
        "\n",
        "# create a frequency table for all words\n",
        "def to_vocab(lines):\n",
        "    vocab = Counter()\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "# remove all words with a frequency below a threshold\n",
        "def trim_vocab(vocab, min_occurance):\n",
        "    tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "    return set(tokens)\n",
        "\n",
        "# mark all out-of-vocabulary words with \"unk\" for all lines\n",
        "def update_dataset(lines, vocab):\n",
        "    new_lines = list()\n",
        "    for line in lines:\n",
        "        new_tokens = list()\n",
        "        for token in line.split():\n",
        "\n",
        "           if token in vocab:\n",
        "              new_tokens.append(token)\n",
        "           else:\n",
        "              new_tokens.append('unk')\n",
        "        new_line = ' '.join(new_tokens)\n",
        "        new_lines.append(new_line)\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "## load English dataset\n",
        "filename = 'English.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('English Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New English Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'english_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "    print(\"line\",i,\":\",lines[i])\n",
        "\n",
        "\n",
        "## load French dataset\n",
        "filename = 'French.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('French Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New French Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'french_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "    print(\"line\",i,\":\",lines[i])\n",
        "\n",
        "\n",
        "###----------------------------------------------------\n",
        "# ff = 'french_vocab.pkl'\n",
        "# french_lines = load_clean_sentences(ff)\n",
        "#french_vocab = to_vocab(french_lines)\n",
        "#print('French Vocabulary: %d' % len(french_vocab))\n",
        "\n",
        "# ef = 'english_vocab.pkl'\n",
        "# english_lines = load_clean_sentences(ef)\n",
        "#english_vocab = to_vocab(english_lines)\n",
        "#print('english Vocabulary: %d' % len(english_vocab))\n",
        "\n",
        "#print([english_vocab[token] for token in ['this', 'is', 'an', 'example']]) "
      ],
      "metadata": {
        "id": "zPaf2v9ADNBx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title custom dataset and dataloader\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "# to_vocab() is unsorted so not to do same thing twice integrate this into to_vocab function above \n",
        "def sorted_tokentoint(lines):\n",
        "    token_vocab = Counter()\n",
        "    for line in lines:\n",
        "        tokens = tokenizer(line)\n",
        "        token_vocab.update(tokens)\n",
        "    sorted_by_freq_tuples = sorted(token_vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "    vocab = torchtext.vocab.vocab(ordered_dict)\n",
        "    vocab.insert_token(\"<pad>\", 0)\n",
        "    vocab.insert_token(\"<unk>\", 1)\n",
        "    vocab.insert_token(\"<sos>\", 2)\n",
        "    vocab.insert_token(\"<eos>\", 3)\n",
        "    vocab.set_default_index(1)    # for out of vocabulary words\n",
        "    return vocab\n",
        "\n",
        "\n",
        "ef = 'english_vocab.pkl'\n",
        "english_lines = load_clean_sentences(ef)\n",
        "ff = 'french_vocab.pkl'\n",
        "french_lines = load_clean_sentences(ff)\n",
        "\n",
        "sorted_french_vocab = sorted_tokentoint(french_lines)\n",
        "sorted_english_vocab = sorted_tokentoint(english_lines)\n",
        "\n",
        "# text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "def text_pipeline(text, language_vocab):\n",
        "    return [language_vocab[token] for token in tokenizer(text)]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    textin_list, textout_list, lengthsin, lengthsout = [], [], [], []\n",
        "    for _textin, _textout in batch:\n",
        "        _textin = '<sos> '+_textin +' <eos>'\n",
        "        _textout = '<sos> '+_textout+' <eos>'\n",
        "        processed_textin = torch.tensor(text_pipeline(_textin, sorted_english_vocab), \n",
        "                                      dtype=torch.int64)\n",
        "        processed_textout = torch.tensor(text_pipeline(_textout, sorted_french_vocab), \n",
        "                                      dtype=torch.int64)\n",
        "       \n",
        "        textin_list.append(processed_textin)\n",
        "        textout_list.append(processed_textout)\n",
        "        # lengthsin.append(processed_textin.size(0))\n",
        "        # lengthsout.append(processed_textout.size(0))\n",
        "\n",
        "    # lengthsin = torch.tensor(lengthsin)\n",
        "    # lengthsout = torch.tensor(lengthsout)\n",
        "\n",
        "    padded_textin_list = nn.utils.rnn.pad_sequence(\n",
        "        textin_list, batch_first=True)\n",
        "    padded_textout_list = nn.utils.rnn.pad_sequence(\n",
        "        textout_list, batch_first=True)\n",
        "    \n",
        "    return padded_textin_list.to(device), padded_textout_list.to(device) \n",
        "    #, lengthsin.to(device), lengthsout.to(device)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, in_lang_lines, out_lang_lines, custom_test_length=None):\n",
        "        super().__init__()\n",
        "        self.in_lang_lines = in_lang_lines\n",
        "        self.out_lang_lines = out_lang_lines\n",
        "        self.custom_test_length = custom_test_length\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.out_lang_lines==self.out_lang_lines\n",
        "        return len(self.out_lang_lines) if not self.custom_test_length else self.custom_test_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        in_sentence = self.in_lang_lines[idx]  \n",
        "        out_sentence = self.out_lang_lines[idx]  \n",
        "        return  in_sentence, out_sentence       ###tokenizer(in_sentence), tokenizer(out_sentence)  \n",
        "\n",
        "\n",
        "\n",
        "length = len(english_lines[:128])  ## note, for full scale learning you should remove [:128] everywhere\n",
        "trn = int(length*0.8)\n",
        "val = int(length*0.1)\n",
        "tst = length - trn - val\n",
        "\n",
        "dataset = TranslationDataset(english_lines[:128], french_lines[:128], custom_test_length=128) ##remove 128 for full scale learning\n",
        "torch.manual_seed(1)\n",
        "train_dataset, valid_dataset, test_dataset = random_split(list(dataset), [trn, val, tst])\n",
        "\n",
        "batch_size = 64  \n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)   #, drop_last=True\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "W5cr8wGusC2x"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title multihead attention\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_size = d_model // n_heads\n",
        "        self.fc_q = nn.Linear(d_model, d_model)\n",
        "        self.fc_k = nn.Linear(d_model, d_model)\n",
        "        self.fc_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, q_len, d_model] query\n",
        "        :param Tensor[batch_size, k_len, d_model] key\n",
        "        :param Tensor[batch_size, v_len, d_model] value\n",
        "        :param Tensor[batch_size, ..., k_len] mask\n",
        "        :return Tensor[batch_size, q_len, d_model] context\n",
        "        :return Tensor[batch_size, n_heads, q_len, k_len] attention_weights\n",
        "        \"\"\"\n",
        "        Q = self.fc_q(query) # [batch_size, q_len, d_model]\n",
        "        K = self.fc_k(key) # [batch_size, k_len, d_model]\n",
        "        V = self.fc_v(value) # [batch_size, v_len, d_model]\n",
        "\n",
        "        Q = Q.view(Q.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, q_len, head_size]\n",
        "        K = K.view(K.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, k_len, head_size]\n",
        "        V = V.view(V.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, v_len, head_size]\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) # [batch_size, n_heads, q_len, k_len]\n",
        "        scores = scores / torch.sqrt(torch.FloatTensor([self.head_size]).to(Q.device))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e18)  # also see https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_\n",
        "        attention_weights = F.softmax(scores , dim=-1) # [batch_size, n_heads, q_len, k_len]                \n",
        "        \n",
        "        context = torch.matmul(attention_weights, V) # [batch_size, n_heads, q_len, v_len]\n",
        "        context = context.permute(0, 2, 1, 3).contiguous() # [batch_size, q_len, n_heads, v_len]\n",
        "        context = context.view(context.size(0), -1, self.d_model)\n",
        "        context = self.fc_o(context) # [batch_size, q_len, d_model]\n",
        "\n",
        "        return context, attention_weights"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BrOB-aYe7Yf4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title positionwise feedforward and positional encoding \n",
        "\n",
        "# positionwise feedforward\n",
        "\n",
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, hidden_size):\n",
        "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc_in = nn.Linear(d_model, hidden_size)\n",
        "        self.fc_ou = nn.Linear(hidden_size, d_model)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, d_model] inputs\n",
        "        :return Tensor[batch_size, seq_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        outputs = F.relu(self.fc_in(inputs)) # [batch_size, seq_len, hidden_size]\n",
        "        return self.fc_ou(outputs) # [batch_size, seq_len, d_model]\n",
        "\n",
        "# positional encoding layer is designed for giving positional / sequential awarenes of the words in the sentence to the model\n",
        "\n",
        "class PositionalEncodingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super(PositionalEncodingLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def get_angles(self, positions, indexes):\n",
        "        d_model_tensor = torch.FloatTensor([[self.d_model]]).to(positions.device) # to same device as positions (generally and d_model are made to be same)\n",
        "        angle_rates = torch.pow(10000, (2 * (indexes // 2)) / d_model_tensor)   #torch.pow(input, exponent) takes pover \n",
        "        return positions / angle_rates\n",
        "\n",
        "    def forward(self, input_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len] input_sequences\n",
        "        :return Tensor[batch_size, seq_len, d_model] position_encoding\n",
        "        \"\"\"\n",
        "        positions = torch.arange(input_sequences.size(1)).unsqueeze(1).to(input_sequences.device) # [seq_len, 1]\n",
        "        indexes = torch.arange(self.d_model).unsqueeze(0).to(input_sequences.device) # [1, d_model]\n",
        "        angles = self.get_angles(positions, indexes) # [seq_len, d_model]\n",
        "        angles[:, 0::2] = torch.sin(angles[:, 0::2]) # apply sin to even indices in the tensor; 2i\n",
        "        angles[:, 1::2] = torch.cos(angles[:, 1::2]) # apply cos to odd indices in the tensor; 2i\n",
        "        position_encoding = angles.unsqueeze(0).repeat(input_sequences.size(0), 1, 1) # [batch_size, seq_len, d_model]\n",
        "        return position_encoding\n",
        "\n",
        "pos_encoding = PositionalEncodingLayer(d_model=512)(torch.randint(100, size=(64, 50))).numpy()\n",
        "print(pos_encoding.shape)\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PItRNXZj7Z0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title encoder block and layer\n",
        "\n",
        "class EncoderBlockLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, hidden_size, dropout):\n",
        "        super(EncoderBlockLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(d_model=d_model, hidden_size=hidden_size)\n",
        "        self.position_wise_feed_forward_layer_norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, src_inputs, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len, d_model] src_inputs\n",
        "        :param Tensor[batch_size,  src_len] src_mask\n",
        "        :return Tensor[batch_size, src_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        context, _ = self.multi_head_attention_layer(query=src_inputs, key=src_inputs, value=src_inputs, mask=src_mask)\n",
        "        context = self.multi_head_attention_layer_norm(self.dropout(context) + src_inputs)\n",
        "        \n",
        "        outputs = self.position_wise_feed_forward_layer(context)\n",
        "        outputs = self.position_wise_feed_forward_layer_norm(self.dropout(outputs) + context)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, max_len, d_model, n_heads, hidden_size, dropout, n_layers):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncodingLayer(d_model=d_model, max_len=max_len)\n",
        "        self.encoder_block_layers = nn.ModuleList([EncoderBlockLayer(d_model=d_model, n_heads=n_heads, hidden_size=hidden_size,\n",
        "                                                                     dropout=dropout) for _ in range(n_layers)])\n",
        "    \n",
        "    def forward(self, src_sequences, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :param Tensor[batch_size, src_len] src_mask\n",
        "        :return Tensor[batch_size, src_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        token_embedded = self.token_embedding(src_sequences) # [batch_size, src_len, d_model]\n",
        "        position_encoded = self.position_encoding(src_sequences) # [batch_size, src_len, d_model]\n",
        "        outputs = self.dropout(token_embedded) + position_encoded # [batch_size, src_len, d_model]\n",
        "        for layer in self.encoder_block_layers:\n",
        "            outputs = layer(src_inputs=outputs, src_mask=src_mask) # [batch_size, src_len, d_model]\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "-O1By8GL75g9",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title decoder block and layer\n",
        "\n",
        "class DecoderBlockLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, hidden_size, dropout):\n",
        "        super(DecoderBlockLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.mask_multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.mask_multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(d_model=d_model, hidden_size=hidden_size)\n",
        "        self.position_wise_feed_forward_layer_norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, dest_inputs, src_encoded, dest_mask, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, dest_len, d_model] dest_inputs\n",
        "        :param Tensor[batch_size, src_len, d_model] src_encoded\n",
        "        :param Tensor[batch_size,  dest_len] dest_mask\n",
        "        :param Tensor[batch_size,  src_len] src_mask\n",
        "        :return Tensor[batch_size, dest_len, d_model] outputs\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        masked_context, _ = self.mask_multi_head_attention_layer(query=dest_inputs, key=dest_inputs, value=dest_inputs, mask=dest_mask)\n",
        "        masked_context = self.mask_multi_head_attention_layer_norm(self.dropout(masked_context) + dest_inputs)\n",
        "        \n",
        "        context, attention_weights = self.multi_head_attention_layer(query=masked_context, key=src_encoded, value=src_encoded, mask=src_mask)\n",
        "        context = self.multi_head_attention_layer_norm(self.dropout(context) + masked_context)\n",
        "        \n",
        "        outputs = self.position_wise_feed_forward_layer(context)\n",
        "        outputs = self.position_wise_feed_forward_layer_norm(self.dropout(outputs) + context)\n",
        "        \n",
        "        return outputs, attention_weights\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, max_len, d_model, n_heads, hidden_size, dropout, n_layers):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncodingLayer(d_model=d_model, max_len=max_len)\n",
        "        self.decoder_block_layers = nn.ModuleList([DecoderBlockLayer(d_model=d_model, n_heads=n_heads, hidden_size=hidden_size, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "    \n",
        "    def forward(self, dest_sequences, src_encoded, dest_mask, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :param Tensor[batch_size, src_len, d_model] src_encoded\n",
        "        :param Tensor[batch_size, dest_len, d_model] dest_mask\n",
        "        :param Tensor[batch_size, src_len, d_model] src_mask\n",
        "        :return Tensor[batch_size, dest_len, vocab_size] logits\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        token_embedded = self.token_embedding(dest_sequences) # [batch_size, dest_len, d_model]\n",
        "        position_encoded = self.position_encoding(dest_sequences) # [batch_size, dest_len, d_model]\n",
        "        outputs = self.dropout(token_embedded) + position_encoded # [batch_size, dest_len, d_model]\n",
        "        for layer in self.decoder_block_layers:\n",
        "            outputs, attention_weights = layer(dest_inputs=outputs, src_encoded=src_encoded, dest_mask=dest_mask, src_mask=src_mask)\n",
        "        logits = self.fc(outputs)\n",
        "        return logits, attention_weights"
      ],
      "metadata": {
        "id": "45QO8sS78DtF",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title combining all in one transformer model\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder, src_pad_index, dest_pad_index):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_index = src_pad_index\n",
        "        self.dest_pad_index = dest_pad_index\n",
        "\n",
        "    def make_src_mask(self, src_sequences):\n",
        "        \"\"\"Mask <pad> tokens.\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :return Tensor[batch size, 1, 1, src len] src_mask\n",
        "        \"\"\"        \n",
        "        src_mask = (src_sequences != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "    \n",
        "    def make_dest_mask(self, dest_sequences):\n",
        "        \"\"\"Mask <pad> tokens and future tokens as well.\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :return tensor[batch_size, 1, dest_len, dest_len] dest_mask\n",
        "        \"\"\"\n",
        "        mask = (dest_sequences != self.dest_pad_index).unsqueeze(1).unsqueeze(2) # [batch size, 1, 1, trg len]\n",
        "        # torch.tril() lower triangular part of the matrix \n",
        "        sub_mask = torch.tril(torch.ones((dest_sequences.size(1), dest_sequences.size(1))).to(dest_sequences.device)).bool() # [trg len, trg len]        \n",
        "        return mask & sub_mask      #both hold for masking: mask--> true everywhere except dest_pad_index; sub_mask-->'triangular' filter\n",
        "    \n",
        "    def forward(self, src_sequences, dest_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :return Tensor[batch_size, dest_len, vocab_size] logits\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        src_mask, dest_mask = self.make_src_mask(src_sequences), self.make_dest_mask(dest_sequences)\n",
        "        src_encoded = self.encoder(src_sequences=src_sequences, src_mask=src_mask)\n",
        "        logits, attention_weights = self.decoder(dest_sequences=dest_sequences, src_encoded=src_encoded, dest_mask=dest_mask, src_mask=src_mask)\n",
        "        return logits, attention_weights"
      ],
      "metadata": {
        "id": "qiFmlPkD8b88",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title building parts for training\n",
        "\n",
        "class Metrics:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count\n",
        "\n",
        "def accuracy(outputs, target_sequences, k=5):\n",
        "    \"\"\" Calculate Top-k accuracy\n",
        "    :param Tensor[batch_size, dest_seq_len, vocab_size] outputs\n",
        "    :param Tensor[batch_size, dest_seq_len] target_sequences\n",
        "    :return float Top-k accuracy\n",
        "    \"\"\"\n",
        "    batch_size = target_sequences.size(0)\n",
        "    _, indices = outputs.topk(k, dim=2, largest=True, sorted=True) # [batch_size, dest_seq_len, 5]  #also see torch.topk() https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "    correct = indices.eq(target_sequences.unsqueeze(-1).expand_as(indices))  #also see torch.eq() https://pytorch.org/docs/stable/generated/torch.eq.html\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / indices.numel())\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "    \n",
        "    def train_step(self, loader, epoch, grad_clip):\n",
        "        loss_tracker, acc_tracker = Metrics(), Metrics()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            src, trg = batch[0], batch[1]\n",
        "            self.optimizer.zero_grad()\n",
        "            logits, _ = self.model(src, trg[:, :-1]) # [batch_size, dest_len, vocab_size]\n",
        "            loss = self.criterion(logits.contiguous().view(-1, self.model.decoder.vocab_size), trg[:, 1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            loss_tracker.update(loss.item())\n",
        "            acc_tracker.update(accuracy(logits, trg[:, 1:]))\n",
        "            loss_, ppl_, acc_ = loss_tracker.average, np.exp(loss_tracker.average), acc_tracker.average\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_:.3f} -     ppl: {ppl_:.3f} -     acc: {acc_:.3f}%')\n",
        "        return loss_tracker.average, np.exp(loss_tracker.average), acc_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch):\n",
        "        loss_tracker, acc_tracker = Metrics(), Metrics()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                src, trg = batch[0], batch[1]\n",
        "                logits, _ = self.model(src, trg[:, :-1]) # [batch_size, dest_len, vocab_size]\n",
        "                loss = self.criterion(logits.contiguous().view(-1, self.model.decoder.vocab_size), trg[:, 1:].contiguous().view(-1))\n",
        "                loss_tracker.update(loss.item())\n",
        "                acc_tracker.update(accuracy(logits, trg[:, 1:]))\n",
        "                loss_, ppl_, acc_ = loss_tracker.average, np.exp(loss_tracker.average), acc_tracker.average\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_:.3f} - val_ppl: {ppl_:.3f} - val_acc: {acc_:.3f}%')\n",
        "        return loss_tracker.average, np.exp(loss_tracker.average), acc_tracker.average\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip):\n",
        "        history, best_loss = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': [], 'ppl': [], 'val_ppl': []}, np.inf\n",
        "        for epoch in range(n_epochs):\n",
        "            loss, ppl, acc = self.train_step(train_loader, epoch, grad_clip)\n",
        "            val_loss, val_ppl, val_acc = self.validate(valid_loader, epoch)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './transformer.pth')\n",
        "            history['acc'].append(acc); history['val_acc'].append(val_acc)\n",
        "            history['ppl'].append(ppl); history['val_ppl'].append(val_ppl)\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "        return history"
      ],
      "metadata": {
        "id": "o41qH06h8w0V",
        "cellView": "form"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training\n",
        "\n",
        "D_MODEL = 256\n",
        "N_LAYERS = 2\n",
        "N_HEADS = 8\n",
        "HIDDEN_SIZE = 512\n",
        "MAX_LEN = 50\n",
        "DROPOUT = 0.25\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "N_EPOCHS = 10\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "\n",
        "transformer = Transformer(\n",
        "    encoder=EncoderLayer(\n",
        "        vocab_size=len(sorted_english_vocab),\n",
        "        max_len=MAX_LEN,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        dropout=DROPOUT,\n",
        "        n_layers=N_LAYERS\n",
        "    ),\n",
        "    decoder=DecoderLayer(\n",
        "        vocab_size=len(sorted_french_vocab),\n",
        "        max_len=MAX_LEN,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        dropout=DROPOUT,\n",
        "        n_layers=N_LAYERS\n",
        "    ),\n",
        "    src_pad_index= 0, #after using torch.nn.utils.rnn.pad_sequence() the default value for padded element == 0 \n",
        "    dest_pad_index= 0, \n",
        ").to(device)\n",
        "optimizer = optim.Adam(params=transformer.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss() #nn.CrossEntropyLoss(ignore_index=0) \n",
        "# print(f'Number of parameters of the model: {sum(p.numel() for p in transformer.parameters() if p.requires_grad):,}')\n",
        "# print(transformer)\n",
        "trainer = Trainer(model=transformer, optimizer=optimizer, criterion=criterion)\n",
        "\n",
        "\n",
        "history = trainer.train(train_loader=train_dl, valid_loader=valid_dl, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)\n",
        "\n",
        "### visualize training results\n",
        "titlelist = ['Loss', 'Perplexity', 'Top-5 Accuracy & BLEU-4']\n",
        "eval_type_list = ['Loss', 'Perplexity', 'Accuracy & BLEU-4 (%)']\n",
        "def plot(titlelist, eval_type_list):\n",
        "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    for i in range(3):\n",
        "        a, b = list(history.keys())[:i+2][-2:]\n",
        "        axes[i].plot(history[a], label='train')\n",
        "        axes[i].plot(history[b], label='valid')\n",
        "        axes[i].set_title(titlelist[i])\n",
        "        axes[i].set_xlabel('Epoch')\n",
        "        axes[i].set_ylabel(eval_type_list[i])\n",
        "        axes[i].grid(True)\n",
        "        axes[i].legend();\n",
        "    plt.show()\n",
        "\n",
        "plot(titlelist, eval_type_list)"
      ],
      "metadata": {
        "id": "ECVcj_0y9Q7F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title translate and test\n",
        "\n",
        "transformer = Transformer(\n",
        "    encoder=EncoderLayer(\n",
        "        vocab_size=len(sorted_english_vocab),\n",
        "        max_len=50,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        hidden_size=512,\n",
        "        dropout=0.25,\n",
        "        n_layers=2\n",
        "    ),\n",
        "    decoder=DecoderLayer(\n",
        "        vocab_size=len(sorted_french_vocab),\n",
        "        max_len=50,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        hidden_size=512,\n",
        "        dropout=0.25,\n",
        "        n_layers=2\n",
        "    ),\n",
        "    src_pad_index= 0, \n",
        "    dest_pad_index= 0, \n",
        ").to(device)\n",
        "\n",
        "transformer.load_state_dict(torch.load('./transformer.pth'))\n",
        "transformer.to(device)\n",
        "\n",
        "\n",
        "def visualize_attention(attentions, inputtext, outputtext):\n",
        "  np.random.seed(0)\n",
        "  sns.set_theme()\n",
        "  xticks = inputtext.split(' ')                         #[''] + inputtext.split(' ') + ['<eos>']\n",
        "  outputtext = outputtext.split(' ')\n",
        "  ax = sns.heatmap(attentions.cpu().detach().numpy(), xticklabels=xticks, yticklabels=outputtext)\n",
        "  ax.set_yticklabels(ax.get_yticklabels(), rotation = 90, fontsize = 16)\n",
        "  print('input =', inputtext)\n",
        "  print('output =', ' '.join(outputtext))\n",
        "\n",
        "\n",
        "def translate(sentence, model=transformer, max_len=5, device=device):\n",
        "\n",
        "    src_sequence = text_pipeline(sentence, sorted_english_vocab)\n",
        "\n",
        "    translated_sentence, attention_weights, pred_logps = [], [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        src_sequence = torch.LongTensor(src_sequence).unsqueeze(0).to(device)\n",
        "        src_mask = model.make_src_mask(src_sequence)\n",
        "        src_encoded = model.encoder(src_sequences=src_sequence, src_mask=src_mask)\n",
        "        predicted = [sorted_english_vocab['<sos>']]\n",
        "\n",
        "        for i in range(max_len):   \n",
        "            already_translated = torch.LongTensor(predicted).unsqueeze(0).to(device)\n",
        "            dest_mask = model.make_dest_mask(already_translated)\n",
        "            logit, attn_weights = model.decoder(dest_sequences=already_translated, src_encoded=src_encoded,\n",
        "                                      dest_mask=dest_mask, src_mask=src_mask) # [1, dest_seq_len, vocab_size]                      \n",
        "            logp = F.log_softmax(logit[:, -1, :], dim=1).squeeze(dim=0) # [vocab_size] Get scores                    \n",
        "            \n",
        "            # this part should be removed when model predicts well as it is not necessary\n",
        "            if torch.argmax(logp)==sorted_french_vocab['<pad>']:\n",
        "                logp[sorted_french_vocab['<pad>']] = -float('inf')\n",
        "            #----------------------------------------------------------------------------\n",
        "\n",
        "            predicted_token = torch.argmax(logp)                  \n",
        "            if predicted_token.cpu().item()==sorted_french_vocab['<eos>']: \n",
        "                break\n",
        "            else:\n",
        "              predicted.append(predicted_token.cpu().item())\n",
        "\n",
        "    translation = ' '.join(sorted_french_vocab.lookup_tokens(predicted[1:])) # I generally avoid using vocab functionalities, but this time let it be so\n",
        "    attention = torch.mean(attn_weights.squeeze(0), dim=0)    #you can average across all attention head results\n",
        "    #attention = attn_weights[0][random.choice(range(N_HEADS))]  #randomly choose one of 8 attention head results\n",
        "\n",
        "    return visualize_attention(attention, sentence, translation)           \n",
        "\n",
        "translate('good morning world')"
      ],
      "metadata": {
        "id": "Ry4-DZnOFAxG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "10b0a0b2-0130-4000-8448-06dea5f04f82"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = good morning world\n",
            "output = de de de de de\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}