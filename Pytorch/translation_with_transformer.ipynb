{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIL0o7B0DIV_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pickle import dump\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://www.statmt.org/europarl/v7/fr-en.tgz\n",
        "!tar -xf fr-en.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title preprocess raw data\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "    return doc.strip().split('\\n')\n",
        "\n",
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "    lengths = [len(s.split()) for s in sentences]\n",
        "    return min(lengths), max(lengths)\n",
        "\n",
        "# clean lines\n",
        "def clean_lines(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for line in lines:\n",
        "        # normalize unicode characters\n",
        "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
        "        line = line.decode('UTF-8')\n",
        "        # tokenize on white space\n",
        "        line = line.split()\n",
        "        # convert to lower case\n",
        "        line = [word.lower() for word in line]\n",
        "        # remove punctuation from each token\n",
        "        line = [word.translate(table) for word in line]\n",
        "        # remove non-printable chars form each token\n",
        "        line = [re_print.sub('', w) for w in line]\n",
        "        # remove tokens with numbers in them\n",
        "        line = [word for word in line if word.isalpha()]\n",
        "        # store as string\n",
        "        cleaned.append(' '.join(line))\n",
        "    return cleaned\n",
        "\n",
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences),\n",
        "minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'English.pkl'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(cleanf,outfile)\n",
        "outfile.close()             #do this with context manager\n",
        "print(filename,\" saved\")\n",
        "\n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences),\n",
        "minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'French.pkl'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(cleanf,outfile)\n",
        "outfile.close()               #do this with context manager - do not forget!\n",
        "print(filename,\" saved\")\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "# create a frequency table for all words\n",
        "def to_vocab(lines):\n",
        "    vocab = Counter()\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "# remove all words with a frequency below a threshold\n",
        "def trim_vocab(vocab, min_occurance):\n",
        "    tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "    return set(tokens)\n",
        "\n",
        "# mark all out-of-vocabulary words with \"unk\" for all lines\n",
        "def update_dataset(lines, vocab):\n",
        "    new_lines = list()\n",
        "    for line in lines:\n",
        "        new_tokens = list()\n",
        "        for token in line.split():\n",
        "\n",
        "           if token in vocab:\n",
        "              new_tokens.append(token)\n",
        "           else:\n",
        "              new_tokens.append('unk')\n",
        "        new_line = ' '.join(new_tokens)\n",
        "        new_lines.append(new_line)\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "## load English dataset\n",
        "filename = 'English.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('English Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New English Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'english_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "    print(\"line\",i,\":\",lines[i])\n",
        "\n",
        "\n",
        "## load French dataset\n",
        "filename = 'French.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('French Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New French Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'french_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "    print(\"line\",i,\":\",lines[i])\n",
        "\n",
        "\n",
        "###----------------------------------------------------\n",
        "# ff = 'french_vocab.pkl'\n",
        "# french_lines = load_clean_sentences(ff)\n",
        "#french_vocab = to_vocab(french_lines)\n",
        "#print('French Vocabulary: %d' % len(french_vocab))\n",
        "\n",
        "# ef = 'english_vocab.pkl'\n",
        "# english_lines = load_clean_sentences(ef)\n",
        "#english_vocab = to_vocab(english_lines)\n",
        "#print('english Vocabulary: %d' % len(english_vocab))\n",
        "\n",
        "#print([english_vocab[token] for token in ['this', 'is', 'an', 'example']]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "# to_vocab() is unsorted so not to do same thing twice integrate this into to_vocab function above \n",
        "def sorted_tokentoint(lines):\n",
        "    token_vocab = Counter()\n",
        "    for line in lines:\n",
        "        tokens = tokenizer(line)\n",
        "        token_vocab.update(tokens)\n",
        "    sorted_by_freq_tuples = sorted(token_vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "    vocab = torchtext.vocab.vocab(ordered_dict)\n",
        "    vocab.insert_token(\"<pad>\", 0)\n",
        "    vocab.insert_token(\"<unk>\", 1)\n",
        "    vocab.set_default_index(1)\n",
        "\n",
        "\n",
        "ef = 'english_vocab.pkl'\n",
        "english_lines = load_clean_sentences(ef)\n",
        "ff = 'french_vocab.pkl'\n",
        "french_lines = load_clean_sentences(ff)\n",
        "\n",
        "sorted_french_vocab = sorted_tokentoint(french_lines)\n",
        "sorted_english_vocab = sorted_tokentoint(english_lines)\n",
        "\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    textin_list, textout_list, lengthsin, lengthsout = [], [], [], []\n",
        "    for _textin, _textout in batch:\n",
        "        processed_textin = torch.tensor(text_pipeline(_textin), \n",
        "                                      dtype=torch.int64)\n",
        "        processed_textout = torch.tensor(text_pipeline(_textout), \n",
        "                                      dtype=torch.int64)\n",
        "       \n",
        "        textin_list.append(processed_textin)\n",
        "        lengthsin.append(processed_textin.size(0))\n",
        "        textout_list.append(processed_textout)\n",
        "        lengthsout.append(processed_textout.size(0))\n",
        "\n",
        "    lengthsin = torch.tensor(lengthsin)\n",
        "    lengthsout = torch.tensor(lengthsout)\n",
        "\n",
        "    padded_textin_list = nn.utils.rnn.pad_sequence(\n",
        "        textin_list, batch_first=True)\n",
        "    padded_textout_list = nn.utils.rnn.pad_sequence(\n",
        "        textout_list, batch_first=True)\n",
        "    \n",
        "    return padded_textin_list.to(device), padded_textout_list.to(device) #, lengthsin.to(device), lengthsout.to(device)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, in_lang_lines, out_lang_lines, custom_test_length=None):\n",
        "        super().__init__()\n",
        "        self.in_lang_lines = in_lang_lines\n",
        "        self.out_lang_lines = out_lang_lines\n",
        "        self.custom_test_length = custom_test_length\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.out_lang_lines==self.out_lang_lines\n",
        "        return len(self.out_lang_lines) if not self.custom_test_length else self.custom_test_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        in_sentence = self.in_lang_lines[idx]  \n",
        "        out_sentence = self.out_lang_lines[idx]  \n",
        "        return tokenizer(in_sentence), tokenizer(out_sentence)  \n",
        "\n",
        "\n",
        "\n",
        "length = len(english_lines[:128])\n",
        "trn = int(length*0.8)\n",
        "val = int(length*0.1)\n",
        "tst = length - trn - val\n",
        "\n",
        "dataset = TranslationDataset(english_lines[:128], french_lines[:128], custom_test_length=128)\n",
        "torch.manual_seed(1)\n",
        "train_dataset, valid_dataset, test_dataset = random_split(list(dataset), [trn, val, tst])\n",
        "\n",
        "batch_size = 32  \n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BrOB-aYe7Yf4"
      },
      "outputs": [],
      "source": [
        "#@title multihead attention\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_size = d_model // n_heads\n",
        "        self.fc_q = nn.Linear(d_model, d_model)\n",
        "        self.fc_k = nn.Linear(d_model, d_model)\n",
        "        self.fc_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, q_len, d_model] query\n",
        "        :param Tensor[batch_size, k_len, d_model] key\n",
        "        :param Tensor[batch_size, v_len, d_model] value\n",
        "        :param Tensor[batch_size, ..., k_len] mask\n",
        "        :return Tensor[batch_size, q_len, d_model] context\n",
        "        :return Tensor[batch_size, n_heads, q_len, k_len] attention_weights\n",
        "        \"\"\"\n",
        "        Q = self.fc_q(query) # [batch_size, q_len, d_model]\n",
        "        K = self.fc_k(key) # [batch_size, k_len, d_model]\n",
        "        V = self.fc_v(value) # [batch_size, v_len, d_model]\n",
        "\n",
        "        Q = Q.view(Q.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, q_len, head_size]\n",
        "        K = K.view(K.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, k_len, head_size]\n",
        "        V = V.view(V.size(0), -1, self.n_heads, self.head_size).permute(0, 2, 1, 3) # [batch_size, n_heads, v_len, head_size]\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) # [batch_size, n_heads, q_len, k_len]\n",
        "        scores = scores / torch.sqrt(torch.FloatTensor([self.head_size]).to(Q.device))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e18)  # also see https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_\n",
        "        attention_weights = F.softmax(scores , dim=-1) # [batch_size, n_heads, q_len, k_len]                \n",
        "        \n",
        "        context = torch.matmul(attention_weights, V) # [batch_size, n_heads, q_len, v_len]\n",
        "        context = context.permute(0, 2, 1, 3).contiguous() # [batch_size, q_len, n_heads, v_len]\n",
        "        context = context.view(context.size(0), -1, self.d_model)\n",
        "        context = self.fc_o(context) # [batch_size, q_len, d_model]\n",
        "\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PItRNXZj7Z0G"
      },
      "outputs": [],
      "source": [
        "#@title positionwise feedforward and positional encoding \n",
        "\n",
        "# positionwise feedforward\n",
        "\n",
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, hidden_size):\n",
        "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc_in = nn.Linear(d_model, hidden_size)\n",
        "        self.fc_ou = nn.Linear(hidden_size, d_model)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, d_model] inputs\n",
        "        :return Tensor[batch_size, seq_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        outputs = F.relu(self.fc_in(inputs)) # [batch_size, seq_len, hidden_size]\n",
        "        return self.fc_ou(outputs) # [batch_size, seq_len, d_model]\n",
        "\n",
        "# positional encoding layer is designed for giving positional / sequential awarenes of the words in the sentence to the model\n",
        "\n",
        "class PositionalEncodingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super(PositionalEncodingLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def get_angles(self, positions, indexes):\n",
        "        d_model_tensor = torch.FloatTensor([[self.d_model]]).to(positions.device) # to same device as positions (generally and d_model are made to be same)\n",
        "        angle_rates = torch.pow(10000, (2 * (indexes // 2)) / d_model_tensor)   #torch.pow(input, exponent) takes pover \n",
        "        return positions / angle_rates\n",
        "\n",
        "    def forward(self, input_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len] input_sequences\n",
        "        :return Tensor[batch_size, seq_len, d_model] position_encoding\n",
        "        \"\"\"\n",
        "        positions = torch.arange(input_sequences.size(1)).unsqueeze(1).to(input_sequences.device) # [seq_len, 1]\n",
        "        indexes = torch.arange(self.d_model).unsqueeze(0).to(input_sequences.device) # [1, d_model]\n",
        "        angles = self.get_angles(positions, indexes) # [seq_len, d_model]\n",
        "        angles[:, 0::2] = torch.sin(angles[:, 0::2]) # apply sin to even indices in the tensor; 2i\n",
        "        angles[:, 1::2] = torch.cos(angles[:, 1::2]) # apply cos to odd indices in the tensor; 2i\n",
        "        position_encoding = angles.unsqueeze(0).repeat(input_sequences.size(0), 1, 1) # [batch_size, seq_len, d_model]\n",
        "        return position_encoding\n",
        "\n",
        "pos_encoding = PositionalEncodingLayer(d_model=512)(torch.randint(100, size=(64, 50))).numpy()\n",
        "print(pos_encoding.shape)\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O1By8GL75g9"
      },
      "outputs": [],
      "source": [
        "#@title encoder block and layer\n",
        "\n",
        "class EncoderBlockLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, hidden_size, dropout):\n",
        "        super(EncoderBlockLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(d_model=d_model, hidden_size=hidden_size)\n",
        "        self.position_wise_feed_forward_layer_norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, src_inputs, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len, d_model] src_inputs\n",
        "        :param Tensor[batch_size,  src_len] src_mask\n",
        "        :return Tensor[batch_size, src_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        context, _ = self.multi_head_attention_layer(query=src_inputs, key=src_inputs, value=src_inputs, mask=src_mask)\n",
        "        context = self.multi_head_attention_layer_norm(self.dropout(context) + src_inputs)\n",
        "        \n",
        "        outputs = self.position_wise_feed_forward_layer(context)\n",
        "        outputs = self.position_wise_feed_forward_layer_norm(self.dropout(outputs) + context)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, max_len, d_model, n_heads, hidden_size, dropout, n_layers):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncodingLayer(d_model=d_model, max_len=max_len)\n",
        "        self.encoder_block_layers = nn.ModuleList([EncoderBlockLayer(d_model=d_model, n_heads=n_heads, hidden_size=hidden_size,\n",
        "                                                                     dropout=dropout) for _ in range(n_layers)])\n",
        "    \n",
        "    def forward(self, src_sequences, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :param Tensor[batch_size, src_len] src_mask\n",
        "        :return Tensor[batch_size, src_len, d_model] outputs\n",
        "        \"\"\"\n",
        "        token_embedded = self.token_embedding(src_sequences) # [batch_size, src_len, d_model]\n",
        "        position_encoded = self.position_encoding(src_sequences) # [batch_size, src_len, d_model]\n",
        "        outputs = self.dropout(token_embedded) + position_encoded # [batch_size, src_len, d_model]\n",
        "        for layer in self.encoder_block_layers:\n",
        "            outputs = layer(src_inputs=outputs, src_mask=src_mask) # [batch_size, src_len, d_model]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45QO8sS78DtF"
      },
      "outputs": [],
      "source": [
        "#@title decoder block and layer\n",
        "\n",
        "class DecoderBlockLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, hidden_size, dropout):\n",
        "        super(DecoderBlockLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.mask_multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.mask_multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.multi_head_attention_layer = MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads)\n",
        "        self.multi_head_attention_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(d_model=d_model, hidden_size=hidden_size)\n",
        "        self.position_wise_feed_forward_layer_norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, dest_inputs, src_encoded, dest_mask, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, dest_len, d_model] dest_inputs\n",
        "        :param Tensor[batch_size, src_len, d_model] src_encoded\n",
        "        :param Tensor[batch_size,  dest_len] dest_mask\n",
        "        :param Tensor[batch_size,  src_len] src_mask\n",
        "        :return Tensor[batch_size, dest_len, d_model] outputs\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        masked_context, _ = self.mask_multi_head_attention_layer(query=dest_inputs, key=dest_inputs, value=dest_inputs, mask=dest_mask)\n",
        "        masked_context = self.mask_multi_head_attention_layer_norm(self.dropout(masked_context) + dest_inputs)\n",
        "        \n",
        "        context, attention_weights = self.multi_head_attention_layer(query=masked_context, key=src_encoded, value=src_encoded, mask=src_mask)\n",
        "        context = self.multi_head_attention_layer_norm(self.dropout(context) + masked_context)\n",
        "        \n",
        "        outputs = self.position_wise_feed_forward_layer(context)\n",
        "        outputs = self.position_wise_feed_forward_layer_norm(self.dropout(outputs) + context)\n",
        "        \n",
        "        return outputs, attention_weights\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, max_len, d_model, n_heads, hidden_size, dropout, n_layers):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncodingLayer(d_model=d_model, max_len=max_len)\n",
        "        self.decoder_block_layers = nn.ModuleList([DecoderBlockLayer(d_model=d_model, n_heads=n_heads, hidden_size=hidden_size, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "    \n",
        "    def forward(self, dest_sequences, src_encoded, dest_mask, src_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :param Tensor[batch_size, src_len, d_model] src_encoded\n",
        "        :param Tensor[batch_size, dest_len, d_model] dest_mask\n",
        "        :param Tensor[batch_size, src_len, d_model] src_mask\n",
        "        :return Tensor[batch_size, dest_len, vocab_size] logits\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        token_embedded = self.token_embedding(dest_sequences) # [batch_size, dest_len, d_model]\n",
        "        position_encoded = self.position_encoding(dest_sequences) # [batch_size, dest_len, d_model]\n",
        "        outputs = self.dropout(token_embedded) + position_encoded # [batch_size, dest_len, d_model]\n",
        "        for layer in self.decoder_block_layers:\n",
        "            outputs, attention_weights = layer(dest_inputs=outputs, src_encoded=src_encoded, dest_mask=dest_mask, src_mask=src_mask)\n",
        "        logits = self.fc(outputs)\n",
        "        return logits, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title combining all in one transformer model\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder, src_pad_index, dest_pad_index):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_index = src_pad_index\n",
        "        self.dest_pad_index = dest_pad_index\n",
        "\n",
        "    def make_src_mask(self, src_sequences):\n",
        "        \"\"\"Mask <pad> tokens.\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :return Tensor[batch size, 1, 1, src len] src_mask\n",
        "        \"\"\"        \n",
        "        src_mask = (src_sequences != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "    \n",
        "    def make_dest_mask(self, dest_sequences):\n",
        "        \"\"\"Mask <pad> tokens and futur tokens as well.\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :return tensor[batch_size, 1, dest_len, dest_len] dest_mask\n",
        "        \"\"\"\n",
        "        mask = (dest_sequences != self.dest_pad_index).unsqueeze(1).unsqueeze(2) # [batch size, 1, 1, trg len]\n",
        "        # torch.tril() lower triangular part of the matrix \n",
        "        sub_mask = torch.tril(torch.ones((dest_sequences.size(1), dest_sequences.size(1))).to(dest_sequences.device)).bool() # [trg len, trg len]        \n",
        "        return mask & sub_mask      #both hold for masking: mask--> true everywhere except dest_pad_index; sub_mask-->'triangular' filter\n",
        "    \n",
        "    def forward(self, src_sequences, dest_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, src_len] src_sequences\n",
        "        :param Tensor[batch_size, dest_len] dest_sequences\n",
        "        :return Tensor[batch_size, dest_len, vocab_size] logits\n",
        "        :return Tensor[batch_size, n_heads, dest_len, src_len] attention_weights\n",
        "        \"\"\"\n",
        "        src_mask, dest_mask = self.make_src_mask(src_sequences), self.make_dest_mask(dest_sequences)\n",
        "        src_encoded = self.encoder(src_sequences=src_sequences, src_mask=src_mask)\n",
        "        logits, attention_weights = self.decoder(dest_sequences=dest_sequences, src_encoded=src_encoded, dest_mask=dest_mask, src_mask=src_mask)\n",
        "        return logits, attention_weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
