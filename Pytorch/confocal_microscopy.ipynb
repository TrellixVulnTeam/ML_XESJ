{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confocal_microscopy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Confocal Microscopy Multi Label Image Classification - Transfer Learning & Regularization"
      ],
      "metadata": {
        "id": "i5ZEiKAYjH2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdBzsG77R--8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, Dataset                  # TensorDataset !!! we import Dataset for custom 'ImageFolder'\n",
        "import torchvision\n",
        "# from torchvision.datasets import ImageFolder # MNIST, CIFAR10 etc\n",
        "# from torchvision.datasets.utils import download_url\n",
        "from torchvision.utils import make_grid                                         # save_image \n",
        "import torchvision.transforms as T # ToTensor, Compose, Normalize, RandomCrop, RandomResizedCrop, RandomHorizontalFlip, RandomRotate, RandomErasing, ColorJitter\n",
        "import torchvision.models as models                                             # pretreinde models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "device = device()\n",
        "\n",
        "def todevice(data_model, device):\n",
        "  if isinstance(data_model, (list, tuple)):\n",
        "    return [todevice(i, device) for i in data_model]\n",
        "  return data_model.to(device, non_blocking=True)\n",
        "\n",
        "class DataDeviceLoader():\n",
        "  def __init__(self, dataloader, device):\n",
        "    self.dataloader = dataloader   \n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    for batch in self.dataloader:\n",
        "      yield todevice(batch, self.device)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataloader)"
      ],
      "metadata": {
        "id": "l95zqC4WSa3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -q kaggle                                       \n",
        "# ! mkdir ~/.kaggle                                                \n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle competitions download -c jovian-pytorch-z2g                         # https://www.kaggle.com/c/jovian-pytorch-z2g     \n",
        "# # ! kaggle datasets download ikarus777/best-artworks-of-all-time\n",
        "# # ! unzip jovian-pytorch-z2g"
      ],
      "metadata": {
        "id": "3RMeaVO0UcEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = './data'\n",
        "traindir = datadir + '/train'\n",
        "testdir = datadir + '/test'\n",
        "\n",
        "train_valid_info = pd.read_csv(traindir)\n",
        "testinfo = pd.read_csv(testdir)\n",
        "\n",
        "### 1. Custom random_split (into training and validation info) by creating masks\n",
        "\n",
        "np.random.seed(42)\n",
        "mask = np.random.rand(len(train_valid_info)) < 0.9              # it gives uniform distribution including zeros, so there will be about 1/10 zeros\n",
        "traininfo_dataframe = train_valid_info[mask].reset_index()      # all zeros are not taken 1/10 and then indexing is reset for new dataframe       \n",
        "validinfo_dataframe = train_valid_info[~mask].reset_index()     # mask reversed (only those are taken that previously were zeros)\n",
        "\n",
        "\n",
        "### 2. Custom Lable Handling (string lable to vector and vice versa)\n",
        "\n",
        "labels = {\n",
        "    0: 'Mitochondria',\n",
        "    1: 'Nuclear bodies',\n",
        "    2: 'Nucleoli',\n",
        "    3: 'Golgi apparatus',\n",
        "    4: 'Nucleoplasm',\n",
        "    5: 'Nucleoli fibrillar center',\n",
        "    6: 'Cytosol',\n",
        "    7: 'Plasma membrane',\n",
        "    8: 'Centrosome',\n",
        "    9: 'Nuclear speckles'\n",
        "}\n",
        "\n",
        "def encode_tovector(stringlabel):               # like '1' , '2', '5 8' etc.\n",
        "  vector = torch.zeros(10)                                                      # if we have 10 classes 0-9 are position labels\n",
        "  for labelpart in str(stringlabel).split(' '):                                 # if string consists of many label parts\n",
        "    vector[int(labelpart)] = 1\n",
        "  return vector\n",
        "\n",
        "def decode_tostringlabel(vectorlabel, namelabels=False, treshold=0.5):          # namelabels = labels   (defined above)\n",
        "  stringlabels =[]\n",
        "  for i, encoding in enumerate(vectorlabel):\n",
        "    if (encoding >= treshold):\n",
        "      if namelabels:\n",
        "        stringlabels.append(namelabels[i] + ' ' + str(i))\n",
        "      else:\n",
        "        stringlabels.append(str(i))\n",
        "  return ' '.join(stringlabels)                                                 # because syntax is ' '.join([...])\n",
        "\n",
        "\n",
        "### 3. Custom Dataclass like ImageFolder (take lables from csv and add those lables as vectors to pictures + put them in transform)\n",
        "\n",
        "class ImageLabelerTransformizer(Dataset):                                       # Dataset - requires this structure of the class!\n",
        "  def __init__(self, infodataframe, datapath=datadir, transformizer=False):\n",
        "    self.infodataframe = infodataframe\n",
        "    self.datapath = datapath\n",
        "    self.transformizer = transformizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.infodataframe)\n",
        "\n",
        "  def __getitem__(self, rowindex):\n",
        "    onepictureinfo = self.infodataframe.loc[rowindex]                           # genarally with rows with numerical names we use iloc() but here we have string versions like '0', '1' etc\n",
        "    imagename, stringlabel = onepictureinfo['Image'], onepictureinfo['Label']\n",
        "    imagefullpathname = self.datapath + '/' + str(imagename) + '.png'\n",
        "    image = Image.open(imagefullpathname)\n",
        "    if self.transformizer:\n",
        "      image = transformizer(image)\n",
        "    return image, encode_tovector(stringlabel)\n",
        "\n",
        "\n",
        "### Standart part\n",
        "\n",
        "RGB_mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                              # you should check statistics are given with data\n",
        "train_tensorized = T.Compose([T.RandomCrop(512, padding=8, padding_mode='reflect'),\n",
        "                              #  T.RandomResizeCrop(256, scale=(0.5, 0.9), ratio=(1, 1)),  # T.CenterCrop(32), Ts.Resize(32)\n",
        "                               T.RandomHorizontalFlip(),\n",
        "                               T.RandomRotation(10),                                       # 10 degrees\n",
        "                              #  T.CollorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "                               T.RandomErasing(), \n",
        "                               T.ToTensor(),\n",
        "                              #  T.Normalize(*RGB_mean_std, inplace=True)\n",
        "                               ])      # to make operation in place or რომ არსებულს ზევიდან გადააწეროს, არ ვიცი\n",
        "validortest_tensorized = T.Compose([T.ToTensor(), \n",
        "                              # T.Normalize(*RGB_mean_std)\n",
        "                              ]) \n",
        "\n",
        "traindataset = ImageLabelerTransformizer(infodataframe=traininfo_dataframe, datapath=datadir, transformizer=train_tensorized)         \n",
        "validset = ImageLabelerTransformizer(infodataframe=validinfo_dataframe, datapath=datadir, transformizer=validortest_tensorized)  \n",
        "testset =  ImageLabelerTransformizer(infodataframe=testinfo, datapath=datadir, transformizer=validortest_tensorized)              \n",
        "\n",
        "batchsize = 128\n",
        "trainloader = DataLoader(traindataset, batch_size=batchsize, shuffle=True, num_workers=2, pin_memory=True)  \n",
        "validloader = DataLoader(validset, batch_size=batchsize*2, num_workers=2, pin_memory=True)\n",
        "testloader = DataLoader(testset, batch_size=batchsize*2, num_workers=2, pin_memory=True)\n",
        "\n",
        "trainload = DataDeviceLoader(trainloader, device)\n",
        "validload = DataDeviceLoader(validloader, device) \n",
        "trainload = DataDeviceLoader(testloader, device) "
      ],
      "metadata": {
        "id": "iRKYz0S0ShUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image, _ = somedataset[0]\n",
        "# print(f\"image shape is {image.shape}\") \n",
        "\n",
        "def gridofimages(trainload, invert=False, number=64):          # without for+break images = next(iter(trainload))\n",
        "  for images, _ in trainload:\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "    data = (1-images[: number]) if invert else images[: number]\n",
        "    plt.imshow(make_grid(data.cpu().detach(), normalize=True, nrow=8).permute(1, 2, 0))   #images.cpu() because we used dataload (which is on gpu, if gpu is available) not dataloader\n",
        "    break\n",
        "                                                  # normalize=True reverses/unnormalizes what transforms.Normalize has done\n",
        "gridofimages(trainload, invert=True)"
      ],
      "metadata": {
        "id": "khJZVK0LSpZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold                    #(out of 10) All other entries below treshold became None\n",
        "    label = label > threshold                    #(out of 10) All other entries are zero already but we still want them to become None\n",
        "\n",
        "    TruePositive = (prob & label).sum(axis=1).float()\n",
        "    TrueNegative = ((~prob) & (~label)).sum(axis=11).float()                    # Just for knowledge - because we do not use this\n",
        "    FalsePositive = (prob & (~label)).sum(axis=1).float()\n",
        "    FalseNegative = ((~prob) & label).sum(axis=1).float()\n",
        "\n",
        "    precision = torch.mean(TruePositive/ (TruePositive+ FalsePositive + 1e-12))\n",
        "    recall = torch.mean(TruePositive/ (TruePositive+ FalseNegative + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)   #harmonic mean 2/(1/precision + 1/recall)\n",
        "    return F2.mean(axis=0)                                         \n",
        "\n",
        "\n",
        "class LossPart(nn.Module):\n",
        "  def trainloss(self, batch):\n",
        "    images, lables = batch\n",
        "    out = self(images)\n",
        "    loss = F.binary_cross_entropy(out, lables)\n",
        "    return loss\n",
        "\n",
        "  def validloss(self, batch):\n",
        "    images, lables = batch\n",
        "    out = self(images)\n",
        "    loss = F.binary_cross_entropy(out, lables)\n",
        "    score = F_score(out, lables)\n",
        "    return {'score': score, 'loss': loss.detach()}\n",
        "\n",
        "  def epochend(self, epochoutputs):\n",
        "    epochlosses = [batch['loss'] for batch in epochoutputs]\n",
        "    epochaverageloss = torch.stack(epochlosses).mean()\n",
        "    epochscores = [batch['score'] for batch in epochoutputs]\n",
        "    epochaveragescore = torch.stack(epochscores).mean()\n",
        "    return {'epochloss' : epochaverageloss.item(), 'epochscore' : epochaveragescore.item()}\n",
        "\n",
        "\n",
        "class ForwardPart(LossPart):                                                    \n",
        "  def __init__(self):\n",
        "    super().__init__()                                                          \n",
        "    self.model = models.resnet34(pretreined=True)\n",
        "    self.lastinput = self.model.fc.in_features                                  # fc is last leyer and in_features input features\n",
        "    self.lastoperation = nn.Linear(self.lastinput, 10)                          # as there are 10 classes to classify\n",
        "\n",
        "  def forward(self, xbatch):\n",
        "    return torch.sigmoid(self.model(xbatch))  # torch.sigmoid is basically same as torch.nn.Sigmoid, first is function, second is class callable like function\n",
        "\n",
        "  def train_lastleyer(self):\n",
        "    for parameter in self.model.parameters():\n",
        "      parameter.require_grad = False\n",
        "    for parameter in self.model.fc.parameters():    # in the pretrained model we took fc is last operation / 'layer' name\n",
        "      parameter.require_grad = True\n",
        "\n",
        "  def train_allleyers(self):\n",
        "    for parameter in self.model.parameters():\n",
        "      parameter.require_grad = True\n",
        "    for parameter in self.model.fc.parameters():   \n",
        "      parameter.require_grad = True\n",
        "\n",
        "\n",
        "model = todevice(ForwardPart(), device)\n",
        "model                                               # Just to see what structure has the model"
      ],
      "metadata": {
        "id": "puG5sJQ4SzVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, valid_or_testload):\n",
        "  model.eval()\n",
        "  epochoutputs = [model.validloss(batch) for batch in valid_or_testload]\n",
        "  epochresult = model.epochend(epochoutputs)\n",
        "  return epochresult\n",
        "\n",
        "def fit(model, trainload, validload, max_lr, epochs, weight_decay=0, clip_grad=None, optim=torch.optim.Adam):\n",
        "  torch.cuda.empty_cache()\n",
        "  history = []\n",
        "  optimizer = optim(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "  scedule = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(trainload))\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    learning_rates = []\n",
        "    training_losses = []\n",
        "\n",
        "    for batch in trainload:\n",
        "      loss = model.trainloss(batch)\n",
        "      training_losses.append(loss)\n",
        "      loss.backward()\n",
        "      if clip_grad:\n",
        "        nn.utils.clip_grad_value_(model.parameters(), clip_grad)                # clips weights from previous time\n",
        "      optimizer.step()                                                          # here will be used weight_decay=weight_decay\n",
        "      optimizer.zero_grad()\n",
        "      learning_rates.append([i['lr'] for i in optimizer.param_groups])\n",
        "      scedule.step()\n",
        "\n",
        "    epochresult = evaluate(model, validload)\n",
        "    epochresult['epoch_trainloss'] = torch.stack(training_losses).mean().item() # creates new key-value pair in dictionary\n",
        "    epochresult['lr'] = learning_rates                                          \n",
        "    history.append(epochresult)\n",
        "  \n",
        "  return history"
      ],
      "metadata": {
        "id": "c3YNc1J1TJwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_lr = 0.01\n",
        "epochs = 8\n",
        "weight_decay = 1e-4\n",
        "clip_grad = 0.1\n",
        "optim = torch.optim.Adam\n",
        "\n",
        "model.train_lastleyer()             #first we train only last / new layer\n",
        "\n",
        "training = []\n",
        "\n",
        "%time\n",
        "training += fit(model, trainload, validload, max_lr, epochs, weight_decay, clip_grad, optim)"
      ],
      "metadata": {
        "id": "mOHTOrg3TUZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_allleyers()             #lastly several times, less epochs or not, we train all layers\n",
        "\n",
        "training += fit(model, trainload, validload, max_lr, epochs, weight_decay, clip_grad, optim)"
      ],
      "metadata": {
        "id": "dSabwqmaTzw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [s['epochscore'] for s in training]\n",
        "validlosses = [loss['epochloss'] for loss in training]\n",
        "trainlosses = [loss.get('epoch_trainloss') for loss in training]\n",
        "learningrates = np.concatenate([lr.get('lr', []) for lr in training])\n",
        "plt.plot(scores, '-mo')\n",
        "plt.plot(validlosses, '-bo')\n",
        "plt.plot(trainlosses, '-co')\n",
        "# plt.plot(learningrates, '-yo')                        #plot separately as scale is completely different and nothing will be seen in same graph, xlabel=batch\n",
        "plt.legend(['score', 'validloss', 'trainloss', 'lrs'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.title('learning Performance');"
      ],
      "metadata": {
        "id": "O2523TKGTWFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, image):                                           # here we put images that are not normalized, so row image from testset not fromtestload!!\n",
        "  image, label = image\n",
        "  uimage = todevice(image.unsqueeze(0), device)\n",
        "  predicted_vectorlabel = model(uimage)\n",
        "  predicted_stringlabel = decode_tostringlabel(predicted_vectorlabel, namelabels=True)\n",
        "  print(f'predicted label is {predicted_stringlabel}, actual label is {label}')\n",
        "  plt.imshow(image.permute(1, 2, 0)) \n",
        "  plt.axis('off');\n",
        "\n",
        "predict(testset[7])"
      ],
      "metadata": {
        "id": "Qf1LO8tlTbSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'microscopy.pth')\n",
        "new_model = ForwardPart()\n",
        "new_model.load_state_dict(torch.load('microscopy.pth')) "
      ],
      "metadata": {
        "id": "nfMVlUxxTlRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
