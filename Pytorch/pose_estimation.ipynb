{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "R E F E R E N C E S:\n",
        "\n",
        "Paper with code: https://lmb.informatik.uni-freiburg.de/projects/freihand/\n",
        "\n",
        "https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-lets-code-it-6c82046d4acf\n",
        "\n",
        "Using Unet architecture for animal pose estimation (DATAHANDLING LIKE COCO): https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/Neuroscience/pose_estimation.ipynb#scrollTo=b-e3OtycfKYq \n",
        "\n",
        "\n",
        "Different pose estimation domain implementations using different architectures: https://github.com/open-mmlab/mmpose \n",
        "\n",
        "\n",
        "https://github.com/topics/colab?l=python&o=asc&s=updated\n",
        "\n",
        "\n",
        "Also interesting implementations: https://colab.research.google.com/github/AI-FLEX-9/SportsFriends/blob/master/Lab/yje/lightopenpose_2d.ipynb#scrollTo=goCOGasgrY6K ; \n",
        "\n",
        "https://github.com/microsoft/human-pose-estimation.pytorch\n"
      ],
      "metadata": {
        "id": "nYwBP2Sz3XEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports { form-width: \"20%\" }\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import json\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K8Q_ZXLjtCOL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download data { form-width: \"20%\" }\n",
        "\n",
        "# Data is from https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html\n",
        "\n",
        "!wget -O data.zip -q https://lmb.informatik.uni-freiburg.de/data/freihand/FreiHAND_pub_v2.zip\n",
        "!unzip -q data.zip \n",
        "# !rm data.zips\n",
        "\n",
        "### ---------- you do not necessarily need this ------------------\n",
        "### !wget -O justjsons.zip -q https://lmb.informatik.uni-freiburg.de/data/freihand/FreiHAND_pub_v2_eval.zip\n",
        "### !unzip -q justjsons.zip \n",
        "### !rm justjsons.zip\n",
        "\n",
        "\n",
        "## ---------in case you want to use coco, but be aware you will need to reformulate dataset class-----------\n",
        "\n",
        "## !wget -O cocoanotation.zip  -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "## !unzip -q cocoanotation.zip \n",
        "## !rm cocoanotation.zip\n",
        "\n",
        "## #let's pretend validation part is whole dataset, because real all data is quite bigs\n",
        "## !cp /content/annotations/person_keypoints_val2017.json  /content/annotations.json\n",
        "\n",
        "## #therefore we also pretend that validation images from coco dataset 2017 is our whole image data\n",
        "## !wget -O cocoimages.zip  -q http://images.cocodataset.org/zips/val2017.zip\n",
        "## !unzip -q cocoimages.zip \n",
        "## !mv /content/val2017/  /content/images/   # just renamed folder  # !mkdir images !cp -r /content/val2017/*.jpg   /content/images/ if I would need copying\n",
        "## !rm cocoimages.zip\n",
        "\n",
        "## shutil.rmtree('annotations') # you may also need to remove: 'val2017', 'sample_data',"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-GMh7_KPDzod"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title calculators { form-width: \"20%\" }\n",
        "\n",
        "N_KEYPOINTS = 21\n",
        "N_IMG_CHANNELS = 3\n",
        "RAW_IMG_SIZE = 224\n",
        "MODEL_IMG_SIZE = 128\n",
        "DATASET_MEANS = [0.3950, 0.4323, 0.2954]\n",
        "DATASET_STDS = [0.1966, 0.1734, 0.1836]\n",
        "MODEL_NEURONS = 16\n",
        "\n",
        "\n",
        "COLORMAP = {\n",
        "    \"thumb\": {\"ids\": [0, 1, 2, 3, 4], \"color\": \"g\"},\n",
        "    \"index\": {\"ids\": [0, 5, 6, 7, 8], \"color\": \"c\"},\n",
        "    \"middle\": {\"ids\": [0, 9, 10, 11, 12], \"color\": \"b\"},\n",
        "    \"ring\": {\"ids\": [0, 13, 14, 15, 16], \"color\": \"m\"},\n",
        "    \"little\": {\"ids\": [0, 17, 18, 19, 20], \"color\": \"r\"},\n",
        "}\n",
        "\n",
        "\n",
        "def projectPoints(xyz, K):\n",
        "    \"\"\"\n",
        "    Projects 3D coordinates into image space.\n",
        "    Function taken from https://github.com/lmb-freiburg/freihand\n",
        "    \"\"\"\n",
        "    xyz = np.array(xyz)\n",
        "    K = np.array(K)\n",
        "    uv = np.matmul(K, xyz.T).T\n",
        "    return uv[:, :2] / uv[:, -1:]\n",
        "\n",
        "\n",
        "def get_norm_params(dataloader):\n",
        "    \"\"\"\n",
        "    Calculates image normalization parameters.\n",
        "    Mean and Std are calculated for each channel separately.\n",
        "\n",
        "    StackOverflow discussion:\n",
        "    https://stackoverflow.com/questions/60101240/finding-mean-and-standard-deviation-across-image-channels-pytorch\n",
        "    \"\"\"\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for i, sample in tqdm(enumerate(dataloader)):\n",
        "        data = sample[\"image_raw\"]\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "    return {\"mean\": mean, \"std\": std}\n",
        "\n",
        "\n",
        "def vector_to_heatmaps(keypoints):\n",
        "    \"\"\"\n",
        "    Creates 2D heatmaps from keypoint locations for a single image\n",
        "    Input: array of size N_KEYPOINTS x 2\n",
        "    Output: array of size N_KEYPOINTS x MODEL_IMG_SIZE x MODEL_IMG_SIZE\n",
        "    \"\"\"\n",
        "    heatmaps = np.zeros([N_KEYPOINTS, MODEL_IMG_SIZE, MODEL_IMG_SIZE])\n",
        "    for k, (x, y) in enumerate(keypoints):\n",
        "        x, y = int(x * MODEL_IMG_SIZE), int(y * MODEL_IMG_SIZE)\n",
        "        if (0 <= x < MODEL_IMG_SIZE) and (0 <= y < MODEL_IMG_SIZE):\n",
        "            heatmaps[k, int(y), int(x)] = 1\n",
        "\n",
        "    heatmaps = blur_heatmaps(heatmaps)\n",
        "    return heatmaps\n",
        "\n",
        "\n",
        "def blur_heatmaps(heatmaps):\n",
        "    \"\"\"Blurs heatmaps using GaussinaBlur of defined size\"\"\"\n",
        "    heatmaps_blurred = heatmaps.copy()\n",
        "    for k in range(len(heatmaps)):\n",
        "        if heatmaps_blurred[k].max() == 1:\n",
        "            heatmaps_blurred[k] = cv2.GaussianBlur(heatmaps[k], (51, 51), 3)\n",
        "            heatmaps_blurred[k] = heatmaps_blurred[k] / heatmaps_blurred[k].max()\n",
        "    return heatmaps_blurred\n",
        "\n",
        "\n",
        "class IoULoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Intersection over Union Loss.\n",
        "    IoU = Area of Overlap / Area of Union\n",
        "    IoU loss is modified to use for heatmaps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(IoULoss, self).__init__()\n",
        "        self.EPSILON = 1e-6\n",
        "\n",
        "    def _op_sum(self, x):\n",
        "        return x.sum(-1).sum(-1)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        inter = self._op_sum(y_true * y_pred)\n",
        "        union = (\n",
        "            self._op_sum(y_true ** 2)\n",
        "            + self._op_sum(y_pred ** 2)\n",
        "            - self._op_sum(y_true * y_pred)\n",
        "        )\n",
        "        iou = (inter + self.EPSILON) / (union + self.EPSILON)\n",
        "        iou = torch.mean(iou)\n",
        "        return 1 - iou\n",
        "\n",
        "\n",
        "def heatmaps_to_coordinates(heatmaps):\n",
        "    \"\"\"\n",
        "    Heatmaps is a numpy array\n",
        "    Its size is: (batch_size, n_keypoints, img_size, img_size)\n",
        "    \"\"\"\n",
        "    batch_size = heatmaps.shape[0]\n",
        "    sums = heatmaps.sum(axis=-1).sum(axis=-1)\n",
        "    sums = np.expand_dims(sums, [2, 3])     #img_size & img_size dimensions expanded\n",
        "    normalized = heatmaps / sums\n",
        "    x_prob = normalized.sum(axis=2)\n",
        "    y_prob = normalized.sum(axis=3)\n",
        "\n",
        "    arr = np.tile(np.float32(np.arange(0, 128)), [batch_size, 21, 1])\n",
        "    x = (arr * x_prob).sum(axis=2)\n",
        "    y = (arr * y_prob).sum(axis=2)\n",
        "    keypoints = np.stack([x, y], axis=-1)    #become cupled as [x,y] coordinates\n",
        "    return keypoints / 128\n",
        "\n",
        "\n",
        "def show_data(dataset, n_samples=12):\n",
        "    \"\"\"\n",
        "    Function to visualize data\n",
        "    Input: torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    n_cols = 4\n",
        "    n_rows = int(np.ceil(n_samples / n_cols))\n",
        "    plt.figure(figsize=[15, n_rows * 4])\n",
        "\n",
        "    ids = np.random.choice(dataset.__len__(), n_samples, replace=False)\n",
        "    for i, id_ in enumerate(ids, 1):\n",
        "        sample = dataset.__getitem__(id_)\n",
        "\n",
        "        image = sample[\"image_raw\"].numpy()\n",
        "        image = np.moveaxis(image, 0, -1)\n",
        "        keypoints = sample[\"keypoints\"].numpy()\n",
        "        keypoints = keypoints * RAW_IMG_SIZE    #coordinates to image scale\n",
        "\n",
        "        plt.subplot(n_rows, n_cols, i)\n",
        "        plt.imshow(image)\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], c=\"k\", alpha=0.5)\n",
        "        for finger, params in COLORMAP.items():\n",
        "            plt.plot(\n",
        "                keypoints[params[\"ids\"], 0],\n",
        "                keypoints[params[\"ids\"], 1],\n",
        "                params[\"color\"],\n",
        "            )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_batch_predictions(batch_data, model):\n",
        "    \"\"\"\n",
        "    Visualizes image, image with actual keypoints and\n",
        "    image with predicted keypoints.\n",
        "    Finger colors are in COLORMAP.\n",
        "\n",
        "    Inputs:\n",
        "    - batch data is batch from dataloader\n",
        "    - model is trained model\n",
        "    \"\"\"\n",
        "    inputs = batch_data[\"image\"] \n",
        "    true_keypoints = batch_data[\"keypoints\"].numpy()\n",
        "    batch_size = true_keypoints.shape[0]\n",
        "    pred_heatmaps = model(inputs)\n",
        "    pred_heatmaps = pred_heatmaps.detach().numpy()\n",
        "    pred_keypoints = heatmaps_to_coordinates(pred_heatmaps)\n",
        "    images = batch_data[\"image_raw\"].numpy()\n",
        "    images = np.moveaxis(images, 1, -1)\n",
        "\n",
        "    plt.figure(figsize=[12, 4 * batch_size])\n",
        "    for i in range(batch_size):\n",
        "        image = images[i]\n",
        "        true_keypoints_img = true_keypoints[i] * RAW_IMG_SIZE\n",
        "        pred_keypoints_img = pred_keypoints[i] * RAW_IMG_SIZE\n",
        "\n",
        "        plt.subplot(batch_size, 3, i * 3 + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(batch_size, 3, i * 3 + 2)\n",
        "        plt.imshow(image)\n",
        "        for finger, params in COLORMAP.items():\n",
        "            plt.plot(\n",
        "                true_keypoints_img[params[\"ids\"], 0],\n",
        "                true_keypoints_img[params[\"ids\"], 1],\n",
        "                params[\"color\"],\n",
        "            )\n",
        "        plt.title(\"True Keypoints\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(batch_size, 3, i * 3 + 3)\n",
        "        plt.imshow(image)\n",
        "        for finger, params in COLORMAP.items():\n",
        "            plt.plot(\n",
        "                pred_keypoints_img[params[\"ids\"], 0],\n",
        "                pred_keypoints_img[params[\"ids\"], 1],\n",
        "                params[\"color\"],\n",
        "            )\n",
        "        plt.title(\"Pred Keypoints\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OyRwF_C5tDXy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data handling { form-width: \"20%\" }\n",
        "\n",
        "class FreiHAND(Dataset):\n",
        "    \"\"\"\n",
        "    Loads FreiHAND dataset. Only training part is used from FreiHAND:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, set_type=\"train\"):\n",
        "        self.device = config[\"device\"]\n",
        "        self.image_dir = os.path.join(config[\"data_dir\"], \"training/rgb\")\n",
        "        self.image_names = np.sort(os.listdir(self.image_dir))\n",
        "\n",
        "        fn_K_matrix = os.path.join(config[\"data_dir\"], \"training_K.json\")\n",
        "        with open(fn_K_matrix, \"r\") as f:\n",
        "            self.K_matrix = np.array(json.load(f))\n",
        "\n",
        "        fn_anno = os.path.join(config[\"data_dir\"], \"training_xyz.json\")\n",
        "        with open(fn_anno, \"r\") as f:\n",
        "            self.anno = np.array(json.load(f))\n",
        "\n",
        "        if set_type == \"train\":\n",
        "            n_start = 0\n",
        "            n_end = 26000\n",
        "        elif set_type == \"val\":\n",
        "            n_start = 26000\n",
        "            n_end = 31000\n",
        "        else:\n",
        "            n_start = 31000\n",
        "            n_end = len(self.anno)\n",
        "            \n",
        "        #n_start = 0\n",
        "        #n_end = 4\n",
        "\n",
        "        self.image_names = self.image_names[n_start:n_end]\n",
        "        self.K_matrix = self.K_matrix[n_start:n_end]\n",
        "        self.anno = self.anno[n_start:n_end]\n",
        "\n",
        "        self.image_raw_transform = transforms.ToTensor()\n",
        "        self.image_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(MODEL_IMG_SIZE),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=DATASET_MEANS, std=DATASET_STDS),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anno)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_names[idx]\n",
        "        image_raw = Image.open(os.path.join(self.image_dir, image_name))\n",
        "        image = self.image_transform(image_raw)\n",
        "        image_raw = self.image_raw_transform(image_raw)\n",
        "        \n",
        "        keypoints = projectPoints(self.anno[idx], self.K_matrix[idx])\n",
        "        keypoints = keypoints / RAW_IMG_SIZE\n",
        "        heatmaps = vector_to_heatmaps(keypoints)\n",
        "        keypoints = torch.from_numpy(keypoints)\n",
        "        heatmaps = torch.from_numpy(np.float32(heatmaps))\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"keypoints\": keypoints,\n",
        "            \"heatmaps\": heatmaps,\n",
        "            \"image_name\": image_name,\n",
        "            \"image_raw\": image_raw,\n",
        "        }\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"data_dir\": \"/content/\",\n",
        "    \"epochs\": 1000,\n",
        "    \"batch_size\": 48,\n",
        "    \"batches_per_epoch\": 50,\n",
        "    \"batches_per_epoch_val\": 20,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}\n",
        "\n",
        "testconfig = {\n",
        "    \"data_dir\": \"/content/\",\n",
        "    \"model_path\": \"/content/\",\n",
        "    \"test_batch_size\": 4,\n",
        "    \"device\": \"cpu\",\n",
        "}\n",
        "\n",
        "train_dataset = FreiHAND(config=config, set_type=\"train\")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=2\n",
        ")\n",
        "\n",
        "val_dataset = FreiHAND(config=config, set_type=\"val\")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_dataset = FreiHAND(config=testconfig, set_type=\"test\")\n",
        "test_dataloader = DataLoader(test_dataset, testconfig[\"test_batch_size\"],\n",
        "    shuffle=True, drop_last=False, num_workers=2,)\n",
        "\n",
        "show_data(train_dataset, n_samples=8)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RZeHIy0jvPMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define the model { form-width: \"20%\" }\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_depth, out_depth):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_depth),\n",
        "            nn.Conv2d(in_depth, out_depth, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(out_depth),\n",
        "            nn.Conv2d(out_depth, out_depth, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class ShallowUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of UNet, slightly modified:\n",
        "    - less downsampling blocks\n",
        "    - less neurons in the layers\n",
        "    - Batch Normalization added\n",
        "    \n",
        "    Link to paper on original UNet:\n",
        "    https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_down1 = ConvBlock(in_channel, MODEL_NEURONS)\n",
        "        self.conv_down2 = ConvBlock(MODEL_NEURONS, MODEL_NEURONS * 2)\n",
        "        self.conv_down3 = ConvBlock(MODEL_NEURONS * 2, MODEL_NEURONS * 4)\n",
        "        self.conv_bottleneck = ConvBlock(MODEL_NEURONS * 4, MODEL_NEURONS * 8)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsamle = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        self.conv_up1 = ConvBlock(\n",
        "            MODEL_NEURONS * 8 + MODEL_NEURONS * 4, MODEL_NEURONS * 4\n",
        "        )\n",
        "        self.conv_up2 = ConvBlock(\n",
        "            MODEL_NEURONS * 4 + MODEL_NEURONS * 2, MODEL_NEURONS * 2\n",
        "        )\n",
        "        self.conv_up3 = ConvBlock(MODEL_NEURONS * 2 + MODEL_NEURONS, MODEL_NEURONS)\n",
        "\n",
        "        self.conv_out = nn.Sequential(\n",
        "            nn.Conv2d(MODEL_NEURONS, out_channel, kernel_size=3, padding=1, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_d1 = self.conv_down1(x)\n",
        "        conv_d2 = self.conv_down2(self.maxpool(conv_d1))\n",
        "        conv_d3 = self.conv_down3(self.maxpool(conv_d2))\n",
        "        conv_b = self.conv_bottleneck(self.maxpool(conv_d3))\n",
        "\n",
        "        conv_u1 = self.conv_up1(torch.cat([self.upsamle(conv_b), conv_d3], dim=1))\n",
        "        conv_u2 = self.conv_up2(torch.cat([self.upsamle(conv_u1), conv_d2], dim=1))\n",
        "        conv_u3 = self.conv_up3(torch.cat([self.upsamle(conv_u2), conv_d1], dim=1))\n",
        "\n",
        "        out = self.conv_out(conv_u3)\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oFtos12LtI04"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prepare for training { form-width: \"20%\" }\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, criterion, optimizer, config, scheduler=None):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.epochs = config[\"epochs\"]\n",
        "        self.batches_per_epoch = config[\"batches_per_epoch\"]\n",
        "        self.batches_per_epoch_val = config[\"batches_per_epoch_val\"]\n",
        "        self.device = config[\"device\"]\n",
        "        self.scheduler = scheduler\n",
        "        self.checkpoint_frequency = 100\n",
        "        self.early_stopping_epochs = 10\n",
        "        self.early_stopping_avg = 10\n",
        "        self.early_stopping_precision = 5\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader):\n",
        "        for epoch in range(self.epochs):\n",
        "            self._epoch_train(train_dataloader)\n",
        "            self._epoch_eval(val_dataloader)\n",
        "            print(\n",
        "                \"Epoch: {}/{}, Train Loss={}, Val Loss={}\".format(\n",
        "                    epoch + 1,\n",
        "                    self.epochs,\n",
        "                    np.round(self.loss[\"train\"][-1], 10),\n",
        "                    np.round(self.loss[\"val\"][-1], 10),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # reducing LR if no improvement\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(self.loss[\"train\"][-1])\n",
        "\n",
        "            # saving model\n",
        "            if (epoch + 1) % self.checkpoint_frequency == 0:\n",
        "                torch.save(\n",
        "                    self.model.state_dict(), \"model_{}\".format(str(epoch + 1).zfill(3))\n",
        "                )\n",
        "\n",
        "            # early stopping\n",
        "            if epoch < self.early_stopping_avg:\n",
        "                min_val_loss = np.round(np.mean(self.loss[\"val\"]), self.early_stopping_precision)\n",
        "                no_decrease_epochs = 0\n",
        "\n",
        "            else:\n",
        "                val_loss = np.round(\n",
        "                    np.mean(self.loss[\"val\"][-self.early_stopping_avg:]), \n",
        "                                    self.early_stopping_precision\n",
        "                )\n",
        "                if val_loss >= min_val_loss:\n",
        "                    no_decrease_epochs += 1\n",
        "                else:\n",
        "                    min_val_loss = val_loss\n",
        "                    no_decrease_epochs = 0\n",
        "                    #print('New min: ', min_val_loss)\n",
        "\n",
        "            if no_decrease_epochs > self.early_stopping_epochs:\n",
        "                print(\"Early Stopping\")\n",
        "                break\n",
        "\n",
        "        torch.save(self.model.state_dict(), \"model_final\")\n",
        "        return self.model\n",
        "\n",
        "    def _epoch_train(self, dataloader):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs = data[\"image\"].to(self.device)\n",
        "            labels = data[\"heatmaps\"].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if i == self.batches_per_epoch:\n",
        "                epoch_loss = np.mean(running_loss)\n",
        "                self.loss[\"train\"].append(epoch_loss)\n",
        "                break\n",
        "\n",
        "    def _epoch_eval(self, dataloader):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(dataloader, 0):\n",
        "                inputs = data[\"image\"].to(self.device)\n",
        "                labels = data[\"heatmaps\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                if i == self.batches_per_epoch_val:\n",
        "                    epoch_loss = np.mean(running_loss)\n",
        "                    self.loss[\"val\"].append(epoch_loss)\n",
        "                    break\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QOLE_xiztTOS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training { form-width: \"20%\" }\n",
        "\n",
        "model = ShallowUNet(N_IMG_CHANNELS, N_KEYPOINTS)\n",
        "model = model.to(config[\"device\"])\n",
        "\n",
        "criterion = IoULoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer, factor=0.5, patience=20, verbose=True, threshold=0.00001\n",
        ")\n",
        "\n",
        "trainer = Trainer(model, criterion, optimizer, config, scheduler)\n",
        "model = trainer.train(train_dataloader, val_dataloader)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oSfRFatatZVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plot training and validation losses\n",
        "\n",
        "plt.plot(trainer.loss[\"train\"], 'b', label=\"train\")\n",
        "plt.plot(trainer.loss[\"val\"], 'c', label=\"val\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hwgrF3AUwvcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './state_dict.pth') "
      ],
      "metadata": {
        "id": "Zy2npW0q3Ad5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title detection { form-width: \"20%\" }\n",
        "\n",
        "model = ShallowUNet(3, 21)\n",
        "model.load_state_dict(\n",
        "    torch.load('./state_dict.pth', map_location=torch.device(config[\"device\"]))\n",
        ")  # you can also use pretrained weights https://pytorch.org/vision/stable/models.html\n",
        "model.eval()\n",
        "\n",
        "accuracy_all = []\n",
        "for data in tqdm(test_dataloader):\n",
        "    inputs = data[\"image\"]\n",
        "    pred_heatmaps = model(inputs)\n",
        "    pred_heatmaps = pred_heatmaps.detach().numpy()\n",
        "    true_keypoints = data[\"keypoints\"].numpy()\n",
        "    pred_keypoints = heatmaps_to_coordinates(pred_heatmaps)\n",
        "\n",
        "    accuracy_keypoint = ((true_keypoints - pred_keypoints) ** 2).sum(axis=2) ** (1 / 2)\n",
        "    accuracy_image = accuracy_keypoint.mean(axis=1)\n",
        "    accuracy_all.extend(list(accuracy_image))\n",
        "\n",
        "error = np.mean(accuracy_all)\n",
        "print(\"Average error per keypoint: {:.1f}% from image size\".format(error * 100))\n",
        "\n",
        "for img_size in [MODEL_IMG_SIZE, RAW_IMG_SIZE]:\n",
        "    error_pixels = error * img_size\n",
        "    image_size = f\"{img_size}x{img_size}\"\n",
        "    print(\n",
        "        \"Average error per keypoint: {:.0f} pixels for image {}\".format(\n",
        "            error_pixels, image_size\n",
        "        )\n",
        "    )\n",
        "\n",
        "for data in test_dataloader:\n",
        "    show_batch_predictions(data, model)\n",
        "    break"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y-H9475tuI8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}