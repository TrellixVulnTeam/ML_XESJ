{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxFyEdAeJxD8"
      },
      "source": [
        "Data :  https://www.manythings.org/anki/ \n",
        "\n",
        "REFERENCE: https://arxiv.org/abs/1409.3215\n",
        "\n",
        "https://github.com/astorfi/sequence-to-sequence-from-scratch \n",
        "\n",
        "https://github.com/pskrunner14/translator\n",
        "\n",
        "https://github.com/spro/practical-pytorch \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48qCoCRNmk5w",
        "outputId": "a1ba3ef8-fcdf-4d80-dca1-e82ca715ffa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n",
            "['Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\\n']\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip\n",
        "!mkdir data\n",
        "!cp -r ./deu.txt ./data/\n",
        "!mv ./data/deu.txt ./data/eng-deu.txt    #rename to express better what actually is in file\n",
        "\n",
        "# !rm deu-eng.zip\n",
        "# import shutil\n",
        "# shutil.rmtree('./ita_eng')\n",
        "\n",
        "#just inspect what is in file. You need this because:\n",
        "#mainly files are too large and take a lot of time to be oppened\n",
        "\n",
        "with open('./data/eng-deu.txt') as f:\n",
        "    lines = [line for line in f]\n",
        "  \n",
        "print(lines[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RxZv3azYJxD6"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import unicodedata\n",
        "from io import open\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjizzMqoJxD-",
        "outputId": "b4426b3c-7001-4104-aee5-1e57ed3df130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 255817 sentence pairs\n",
            "Trimmed to 255817 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 16400\n",
            "deu 36316\n",
            "['it is very kind of you to help me .', 'es ist sehr nett von dir mir zu helfen .']\n"
          ]
        }
      ],
      "source": [
        "#@title helpers and dataset\n",
        "\n",
        "SOS_token = 0  #start of string sos\n",
        "EOS_token = 1  #end of string eos\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, and make all case independent\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "#split the file into lines, and then split lines into pairs\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('./data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in [l.split('\\t')[0], l.split('\\t')[1]]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "###------------------- This part is for simplicity. Do not do it for full scale language translation tasks-----------------\n",
        "\n",
        "# eng_prefixes = (\n",
        "#     \"i am \", \"i m \",\n",
        "#     \"he is\", \"he s \",\n",
        "#     \"she is\", \"she s \",\n",
        "#     \"you are\", \"you re \",\n",
        "#     \"we are\", \"we re \",\n",
        "#     \"they are\", \"they re \"\n",
        "# )\n",
        "\n",
        "\n",
        "# def filterPair(p):\n",
        "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "#         p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "# def filterPairs(pairs):\n",
        "#     return [pair for pair in pairs if filterPair(pair)]\n",
        "###-------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    # pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "# class Dataset():\n",
        "#     \"\"\"dataset object\"\"\"\n",
        "\n",
        "#     def __init__(self, phase, num_embeddings=None, max_input_length=None, transform=None, auto_encoder=False):\n",
        "#         \"\"\"\n",
        "#         The initialization of the dataset object.\n",
        "#         :param phase: train/test.\n",
        "#         :param num_embeddings: The embedding dimentionality.\n",
        "#         :param max_input_length: The maximum enforced length of the sentences.\n",
        "#         :param transform: Post processing if necessary.\n",
        "#         :param auto_encoder: If we are training an autoencoder or not.\n",
        "#         \"\"\"\n",
        "#         if auto_encoder:\n",
        "#             lang_in = 'eng'\n",
        "#             lang_out = 'eng'\n",
        "#         else:\n",
        "#             lang_in = 'eng'\n",
        "#             lang_out = 'deu'\n",
        "#         # Skip and eliminate the sentences with a length larger than max_input_length!\n",
        "#         input_lang, output_lang, pairs = prepareData(lang_in, lang_out, max_input_length, auto_encoder=auto_encoder, reverse=True)\n",
        "#         print(random.choice(pairs))\n",
        "\n",
        "#         # Randomize list\n",
        "#         random.shuffle(pairs)\n",
        "\n",
        "#         if phase == 'train':\n",
        "#             selected_pairs = pairs[0:int(0.8 * len(pairs))]\n",
        "#         else:\n",
        "#             selected_pairs = pairs[int(0.8 * len(pairs)):]\n",
        "\n",
        "#         # Getting the tensors\n",
        "#         selected_pairs_tensors = [tensorsFromPair(selected_pairs[i], input_lang, output_lang, max_input_length)\n",
        "#                      for i in range(len(selected_pairs))]\n",
        "\n",
        "#         self.transform = transform\n",
        "#         self.num_embeddings = num_embeddings\n",
        "#         self.max_input_length = max_input_length\n",
        "#         self.data = selected_pairs_tensors\n",
        "#         self.input_lang = input_lang\n",
        "#         self.output_lang = output_lang\n",
        "\n",
        "#     def langs(self):\n",
        "#         return self.input_lang, self.output_lang\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "\n",
        "#         # A tuple which represent a pair\n",
        "#         pair = self.data[idx]\n",
        "\n",
        "#         # Define the sample dictionary\n",
        "#         sample = {'sentence': pair}\n",
        "\n",
        "#         if self.transform:\n",
        "#             sample = self.transform(sample)\n",
        "\n",
        "#         return sample\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'deu', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "920PXdWGJxEF"
      },
      "outputs": [],
      "source": [
        "#@title model\n",
        "\n",
        "### Encoder Part\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# ### Simple RNN as Decoder \n",
        "# class DecoderRNN(nn.Module):\n",
        "#     def __init__(self, hidden_size, output_size):\n",
        "#         super(DecoderRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "\n",
        "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "#         self.out = nn.Linear(hidden_size, output_size)\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, input, hidden):\n",
        "#         output = self.embedding(input).view(1, 1, -1)\n",
        "#         output = F.relu(output)\n",
        "#         output, hidden = self.gru(output, hidden)\n",
        "#         output = self.softmax(self.out(output[0]))\n",
        "#         return output, hidden\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "### RNN with soft attention as Decoder\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxdIE3DCJxEK"
      },
      "outputs": [],
      "source": [
        "#@title training\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    beta = float('inf')\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if loss<beta:\n",
        "          beta=loss\n",
        "          torch.save(encoder.state_dict(), './encoder.pth')\n",
        "          torch.save(decoder.state_dict(), './decoder.pth')\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(iter, iter / n_iters * 100, print_loss_avg)\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder, attn_decoder, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z5yW_jeTJxEM"
      },
      "outputs": [],
      "source": [
        "#@title evaluation\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Y2KqbhKs8z",
        "outputId": "58b3ae09-b02d-4474-b39f-559da55542f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> the scientists gathered data .\n",
            "= die wissenschaftler sammelten daten .\n",
            "< uberraschungsgeburtstagsfeier freigeschaltet zeitschrift fuhrungsposten menschenmogliche nachbarlich allerbeste kleider allerbeste bevormundende ramsch ramsch ramsch aktentasche sonntagsarbeit sonntagsarbeit allerbeste herausgesprungen freigehalten rustikaler anlangt anlangt ziehen aufzubrechen verbalisieren kleider todesopfer kleider todesopfer hustens allerbeste weideland kleider todesopfer todesopfer hustens niedriger teilte nachbarlich leichteren essentieller spitzentyp wohnstatte scharpe sonntagsarbeit sonntagsarbeit todesopfer nachbarlich allerbeste allerbeste weideland kleider todesopfer heidelbeermarmelade nachbarlich verbalisieren kleider todesopfer heidelbeermarmelade todesopfer nachbarlich storung heidelbeermarmelade sonntagsarbeit zugunfall allerbeste weideland todesopfer kleider todesopfer lotossitz todesopfer essentieller spitzentyp nachbarlich lehrmeister ergreifen fursorglicher zustimmen kompost kompost pickup zuhore volliger weideland piratenkostum betreibst widerspreche pickup zuhore volliger professor uberdurchschnittlicher glucklichste anderthalbmal essentieller essentieller miserablen lehrmeister niedriger\n",
            "\n",
            "> your friend s in the other room .\n",
            "= dein freund ist nebenan .\n",
            "< sozialamt verliererin lotossitz jeden kleider kleider todesopfer lotossitz todesopfer nachbarlich essentieller lehrmeister ergreifen anlangt zuhore anderthalbmal flops anlangt anlangt zugunfall einfuhrung allerbeste denke kleider allerbeste herausgesprungen lebenden verbalisieren todesopfer kleider todesopfer hustens niedriger errotete todesopfer etwaige kommenden miserablen weideland todesopfer nachbarlich storung freigehalten freigehalten durchwuhlte zuruckgelassene zugunfall tierfotograf zugunfall verkneifen kundigten allerbeste allerbeste passah uberdurchschnittlicher glucklichste freigehalten kleider allerbeste todesopfer etwaige kommenden miserablen verbalisieren schwerwiegendes todesopfer sind todesopfer nachbarlich verbalisieren kleider verbalisieren schwerwiegendes todesopfer sonntagsarbeit todesopfer etwaige kommenden lebenden zuhore bedruckend todesopfer am todesopfer hustens fursorglicher fursorglicher storung todesopfer am todesopfer hustens niedriger seilbahn essentieller lehrmeister lotossitz ergreifen fursorglicher seilbahn\n",
            "\n",
            "> you re my hero .\n",
            "= du bist mein held .\n",
            "< verliererin verliererin ramsch ramsch ramsch lausebengel aktentasche allerbeste kleider herausgesprungen gelaute allerbeste weideland kleider allerbeste herausgesprungen zugunfall passah allerbeste bevormundende schneeburg lebenden volliger weideland verspottet ziehen schwerwiegendes unterbreitete niedriger glucklichste anderthalbmal essentieller niedriger anderthalbmal essentieller lehrmeister niedriger leichteren leichteren salzreiche zugunfall biologieunterricht glucklichste futterte allerbeste allerbeste weideland kleider todesopfer regte kleider glucklichste kleider allerbeste todesopfer kleider todesopfer etwaige kommenden miserablen todesopfer nachbarlich geldverdienen verbalisieren kleider todesopfer etwaige anlangt erwurgt todesopfer etwaige kommenden miserablen weideland kleider todesopfer verbalisieren kleider todesopfer regte kleider allerbeste weideland kleider todesopfer kuck staubecken anlangt anderthalbmal anlangt anderthalbmal zugunfall allerbeste weideland todesopfer am freigehalten freigehalten durchwuhlte zuruckgelassene\n",
            "\n",
            "> i won t ask you for help anymore .\n",
            "= ich werde dich nicht mehr um hilfe bitten .\n",
            "< brennenden freud betreibst brauche schwerwiegendes essentieller essentieller spitzentyp rest gezogenen fursorglicher storung zugunfall beginge gelingt anderthalbmal anderthalbmal essentieller essentieller lehrmeister ergreifen lausebengel storung weideland todesopfer am freigehalten durchwuhlte verbalisieren kleider todesopfer kleider verbalisieren schwerwiegendes todesopfer etwaige erwurgt verbalisieren schwerwiegendes ortes ortes worterbuchern todesopfer essentieller kleider todesopfer nachbarlich verbalisieren kleider todesopfer todesopfer hustens achtzehnjahriger leichteren lotossitz aktentasche lotossitz sonntagsarbeit todesopfer essentieller lehrmeister lotossitz ergreifen fursorglicher fursorglicher eiswurfel allerbeste weideland kleider todesopfer todesopfer hustens vorgefertigte indiens schwerwiegendes kuck staubecken anlangt anlangt parteien allerbeste todesopfer kleider todesopfer lotossitz todesopfer kleider lotossitz verbalisieren schwerwiegendes todesopfer blumenbeet indiens schwerwiegendes gehalt schwerwiegendes kompost kompost ernstere fursorglicher\n",
            "\n",
            "> have a wonderful evening .\n",
            "= ich wunsche dir einen sehr schonen abend !\n",
            "< notma anderthalbmal essentieller essentieller lehrmeister ergreifen anlangt anderthalbmal anlangt leichteren zugunfall gehalt zugunfall schokoladenuberzug kundigten allerbeste todesopfer am todesopfer etwaige heidelbeermarmelade lehrmeister engster lausebengel lotossitz ergreifen fursorglicher ramsch ramsch ramsch ramsch ramsch ramsch ramsch dekorierte anlangt anlangt herausgesprungen respektvoll kleider allerbeste awaji verbalisieren schwerwiegendes anlangt anderthalbmal anlangt anlangt zugunfall gelingt einbeziehen anlangt zugunfall gelingt kuck anderthalbmal anderthalbmal aufgeplatzt volliger trockne anlangt anlangt gelingt anderthalbmal anlangt anderthalbmal aufgeplatzt volliger schokoladenuberzug anlangt anlangt zuhore todesopfer fernglas etwaige anlangt anlangt ziehen gelingt sitzungstermins erwurgt allerbeste todesopfer am todesopfer etwaige kommenden erwurgt todesopfer sonntagsarbeit nachbarlich allerbeste verbalisieren kleider todesopfer kleider todesopfer hustens allerbeste zuruckzuzahlen\n",
            "\n",
            "> thank you for your attention .\n",
            "= danke fur ihre aufmerksamkeit .\n",
            "< sozialamt lotossitz trodelmarkt anlangt anlangt zugunfall dichter zugunfall ramsch ramsch ramsch ramsch lausebengel aktentasche sonntagsarbeit sonntagsarbeit todesopfer kleider todesopfer regte kleider todesopfer reformieren lotossitz verbalisieren kleider todesopfer lotossitz verbalisieren kleider todesopfer todesopfer nachbarlich storung weideland todesopfer etwaige anlangt ziehen heidelbeermarmelade allerbeste awaji allerbeste weideland kleider todesopfer heidelbeermarmelade nachbarlich storung heidelbeermarmelade sonntagsarbeit sonntagsarbeit todesopfer sonntagsarbeit todesopfer nachbarlich storung weideland todesopfer kleider lotossitz verbalisieren kleider todesopfer kleider todesopfer fursorglicher etwaige todesopfer heidelbeermarmelade nachbarlich allerbeste heidelbeermarmelade todesopfer nachbarlich wucher todesopfer essentieller lehrmeister lotossitz aktentasche sonntagsarbeit todesopfer kleider todesopfer hustens fursorglicher storung heidelbeermarmelade todesopfer engster lotossitz sonntagsarbeit todesopfer sonntagsarbeit todesopfer etwaige anfragen erwurgt verbalisieren\n",
            "\n",
            "> please be a bit more careful in future .\n",
            "= bitte sei in zukunft etwas vorsichtiger .\n",
            "< verweilen beet engster unterschiede lausebengel botengange ramsch ramsch ramsch ramsch ramsch aktentasche sonntagsarbeit sonntagsarbeit todesopfer kleider allerbeste allerbeste awaji schwerwiegendes gehalt kleider todesopfer fursorglicher todesopfer hustens fursorglicher todesopfer etwaige anfragen heidelbeermarmelade engster auffalligkeit zuhore uberschritte todesopfer nachbarlich storung weideland todesopfer kleider glucklichste todesopfer hustens fursorglicher storung weideland todesopfer kleider allerbeste schwangerschaftswoche ramsch beachten wohlstand kleider allerbeste awaji beachten lebenden schokoladenuberzug sonntagsarbeit kundigten allerbeste todesopfer nachbarlich nachbarlich storung weideland allerbeste weideland piratenkostum gefreiten vorhattest eintrifft allerbeste kleider herausgesprungen zugunfall einfuhrung allerbeste denke kleider allerbeste todesopfer kleider allerbeste awaji beachten freigehalten fragten sind verbalisieren schwerwiegendes schlechtere allerbeste beachten zugunfall gehalt geerbt frohliches\n",
            "\n",
            "> why are they saying that ?\n",
            "= warum sagen sie das ?\n",
            "< verweilen lehrmeister projekts zuhore todesopfer am freigehalten freigehalten durchwuhlte wortern zuruckgelassene zugunfall schokoladenuberzug kuck staubecken anlangt anlangt anderthalbmal anlangt zugunfall einfuhrung allerbeste allerbeste denke kleider todesopfer kleider todesopfer hustens niedriger nachbarlich storung weideland todesopfer kleider lotossitz sonntagsarbeit todesopfer fursorglicher verbalisieren schwerwiegendes todesopfer etwaige leuten todesopfer kleider todesopfer etwaige heidelbeermarmelade todesopfer engster sind verbalisieren schwerwiegendes kompost pickup kleider kraftwerken kleider beweinen einkaufspassage kleider allerbeste weideland todesopfer kleider allerbeste weideland kleider todesopfer lotossitz lotossitz jeden zuhore uberschritte volliger anderthalbmal essentieller geschrieben leichteren lausebengel storung weideland wusch anlangt ziehen gelingt kernenergie jeden zuhore uberschritte volliger storung weideland todesopfer kleider todesopfer lotossitz allerbeste awaji\n",
            "\n",
            "> i wasn t very tired .\n",
            "= ich war nicht besonders mude .\n",
            "< brennenden nektarinen ramsch ramsch ramsch ramsch ramsch ramsch ramsch aktentasche sonntagsarbeit sonntagsarbeit todesopfer nachbarlich kleider allerbeste verbalisieren kleider todesopfer erwurgt todesopfer etwaige verbalisieren schwerwiegendes ortes kompost kompost ernstere zuruckgelassene schokoladenuberzug sonntagsarbeit todesopfer engster regte kleider lausebengel botengange ramsch ramsch ramsch ramsch ramsch lausebengel storung beginge gelingt anderthalbmal anderthalbmal aufgeplatzt volliger anlangt anderthalbmal anlangt zugunfall gelingt einbeziehen anlangt anlangt anderthalbmal zugunfall landesregierung todesopfer lebenden nachbarlich storung weideland wusch allerbeste herausgesprungen respektvoll am kleider todesopfer kleider todesopfer indiens schwerwiegendes kleider verbalisieren schwerwiegendes todesopfer kompost pickup zuhore kleider todesopfer etwaige anfragen verschneit allerbeste allerbeste weideland kleider todesopfer kleider lotossitz verbalisieren schwerwiegendes kleider todesopfer\n",
            "\n",
            "> i spent the whole day at home watching tv .\n",
            "= ich habe den ganzen tag zu hause ferngesehen .\n",
            "< brennenden wetzte versaumnis kundigten allerbeste allerbeste herrscht schwerwiegendes todesopfer fursorglicher etwaige todesopfer todesopfer etwaige anfragen verschneit allerbeste weideland todesopfer kleider lotossitz verbalisieren schwerwiegendes todesopfer etwaige anfragen erwurgt verbalisieren todesopfer kleider todesopfer hustens fursorglicher todesopfer etwaige kommenden todesopfer nachbarlich storung weideland todesopfer vordrangler engster miserablen weideland todesopfer kleider verbalisieren schwerwiegendes kleider todesopfer etwaige erwurgt todesopfer etwaige ortes ortes kompost geldverdienen kleider beachten indiens schwerwiegendes schlechtere allerbeste denke kleider todesopfer allerbeste erwurgt verbalisieren kleider todesopfer etwaige erwurgt todesopfer hustens allerbeste weideland kleider todesopfer hustens fursorglicher storung todesopfer heidelbeermarmelade nachbarlich allerbeste zeitgenosse bevormundende allergie beachten baumelte schwerwiegendes allerbeste allerbeste herausgesprungen zugunfall zugunfall gehalt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "saved_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "saved_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "saved_encoder.state_dict(torch.load('./encoder.pth'))\n",
        "saved_decoder.state_dict(torch.load('./decoder.pth'))\n",
        "\n",
        "evaluateRandomly(saved_encoder, saved_decoder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
