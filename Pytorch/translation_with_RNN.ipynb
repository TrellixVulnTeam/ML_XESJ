{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxFyEdAeJxD8"
      },
      "source": [
        "Data :  https://www.manythings.org/anki/ \n",
        "\n",
        "REFERENCE: https://arxiv.org/abs/1409.3215\n",
        "\n",
        "https://github.com/astorfi/sequence-to-sequence-from-scratch \n",
        "\n",
        "https://github.com/pskrunner14/translator\n",
        "\n",
        "https://github.com/spro/practical-pytorch \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48qCoCRNmk5w",
        "outputId": "a1ba3ef8-fcdf-4d80-dca1-e82ca715ffa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n",
            "['Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\\n']\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip\n",
        "!mkdir data\n",
        "!cp -r ./deu.txt ./data/\n",
        "!mv ./data/deu.txt ./data/eng-deu.txt    #rename to express better what actually is in file\n",
        "\n",
        "# !rm deu-eng.zip\n",
        "# import shutil\n",
        "# shutil.rmtree('./ita_eng')\n",
        "\n",
        "#just inspect what is in file. You need this because:\n",
        "#mainly files are too large and take a lot of time to be oppened\n",
        "\n",
        "with open('./data/eng-deu.txt') as f:\n",
        "    lines = [line for line in f]\n",
        "  \n",
        "print(lines[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RxZv3azYJxD6"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import unicodedata\n",
        "from io import open\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjizzMqoJxD-",
        "outputId": "b4426b3c-7001-4104-aee5-1e57ed3df130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 255817 sentence pairs\n",
            "Trimmed to 255817 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 16400\n",
            "deu 36316\n",
            "['it is very kind of you to help me .', 'es ist sehr nett von dir mir zu helfen .']\n"
          ]
        }
      ],
      "source": [
        "#@title helpers and dataset\n",
        "\n",
        "SOS_token = 0  #start of string sos\n",
        "EOS_token = 1  #end of string eos\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, and make all case independent\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "#split the file into lines, and then split lines into pairs\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('./data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in [l.split('\\t')[0], l.split('\\t')[1]]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "###------------------- This part is for simplicity. Do not do it for full scale language translation tasks-----------------\n",
        "\n",
        "# eng_prefixes = (\n",
        "#     \"i am \", \"i m \",\n",
        "#     \"he is\", \"he s \",\n",
        "#     \"she is\", \"she s \",\n",
        "#     \"you are\", \"you re \",\n",
        "#     \"we are\", \"we re \",\n",
        "#     \"they are\", \"they re \"\n",
        "# )\n",
        "\n",
        "\n",
        "# def filterPair(p):\n",
        "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "#         p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "# def filterPairs(pairs):\n",
        "#     return [pair for pair in pairs if filterPair(pair)]\n",
        "###-------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    # pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "# class Dataset():\n",
        "#     \"\"\"dataset object\"\"\"\n",
        "\n",
        "#     def __init__(self, phase, num_embeddings=None, max_input_length=None, transform=None, auto_encoder=False):\n",
        "#         \"\"\"\n",
        "#         The initialization of the dataset object.\n",
        "#         :param phase: train/test.\n",
        "#         :param num_embeddings: The embedding dimentionality.\n",
        "#         :param max_input_length: The maximum enforced length of the sentences.\n",
        "#         :param transform: Post processing if necessary.\n",
        "#         :param auto_encoder: If we are training an autoencoder or not.\n",
        "#         \"\"\"\n",
        "#         if auto_encoder:\n",
        "#             lang_in = 'eng'\n",
        "#             lang_out = 'eng'\n",
        "#         else:\n",
        "#             lang_in = 'eng'\n",
        "#             lang_out = 'deu'\n",
        "#         # Skip and eliminate the sentences with a length larger than max_input_length!\n",
        "#         input_lang, output_lang, pairs = prepareData(lang_in, lang_out, max_input_length, auto_encoder=auto_encoder, reverse=True)\n",
        "#         print(random.choice(pairs))\n",
        "\n",
        "#         # Randomize list\n",
        "#         random.shuffle(pairs)\n",
        "\n",
        "#         if phase == 'train':\n",
        "#             selected_pairs = pairs[0:int(0.8 * len(pairs))]\n",
        "#         else:\n",
        "#             selected_pairs = pairs[int(0.8 * len(pairs)):]\n",
        "\n",
        "#         # Getting the tensors\n",
        "#         selected_pairs_tensors = [tensorsFromPair(selected_pairs[i], input_lang, output_lang, max_input_length)\n",
        "#                      for i in range(len(selected_pairs))]\n",
        "\n",
        "#         self.transform = transform\n",
        "#         self.num_embeddings = num_embeddings\n",
        "#         self.max_input_length = max_input_length\n",
        "#         self.data = selected_pairs_tensors\n",
        "#         self.input_lang = input_lang\n",
        "#         self.output_lang = output_lang\n",
        "\n",
        "#     def langs(self):\n",
        "#         return self.input_lang, self.output_lang\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "\n",
        "#         # A tuple which represent a pair\n",
        "#         pair = self.data[idx]\n",
        "\n",
        "#         # Define the sample dictionary\n",
        "#         sample = {'sentence': pair}\n",
        "\n",
        "#         if self.transform:\n",
        "#             sample = self.transform(sample)\n",
        "\n",
        "#         return sample\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'deu', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "920PXdWGJxEF"
      },
      "outputs": [],
      "source": [
        "#@title model\n",
        "\n",
        "### Encoder Part\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# ### Simple RNN as Decoder \n",
        "# class DecoderRNN(nn.Module):\n",
        "#     def __init__(self, hidden_size, output_size):\n",
        "#         super(DecoderRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "\n",
        "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "#         self.out = nn.Linear(hidden_size, output_size)\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, input, hidden):\n",
        "#         output = self.embedding(input).view(1, 1, -1)\n",
        "#         output = F.relu(output)\n",
        "#         output, hidden = self.gru(output, hidden)\n",
        "#         output = self.softmax(self.out(output[0]))\n",
        "#         return output, hidden\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "### RNN with soft attention as Decoder\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxdIE3DCJxEK"
      },
      "outputs": [],
      "source": [
        "#@title training\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    beta = float('inf')\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if loss<beta:\n",
        "          beta=loss\n",
        "          torch.save(encoder.state_dict(), './encoder.pth')\n",
        "          torch.save(decoder.state_dict(), './decoder.pth')\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(iter, iter / n_iters * 100, print_loss_avg)\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder, attn_decoder, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z5yW_jeTJxEM"
      },
      "outputs": [],
      "source": [
        "#@title evaluation\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Y2KqbhKs8z",
        "outputId": "58b3ae09-b02d-4474-b39f-559da55542f6"
      },
      "outputs": [],
      "source": [
        "saved_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "saved_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "saved_encoder.state_dict(torch.load('./encoder.pth'))\n",
        "saved_decoder.state_dict(torch.load('./decoder.pth'))\n",
        "\n",
        "evaluateRandomly(saved_encoder, saved_decoder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
