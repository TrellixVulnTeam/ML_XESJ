{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkSLRt3xaLHg"
      },
      "source": [
        "Dataset: \n",
        "\n",
        "you can take any .txt file format, create it yourself or download it from any source, e.g. from project gutenberg https://www.gutenberg.org/\n",
        "\n",
        "I created Rustavely.txt file, which you can download here and use: \n",
        "https://1drv.ms/t/s!AhnVhbVlzYkKgQmnRS7FrC4QrQNM?e=800gvv "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from torch.distributions.categorical import Categorical\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "qwSD6JLzgrTi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJS-2MVIaLHg",
        "outputId": "5d64573a-c06b-4dc7-e66e-b231304c9001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Length: 403097\n",
            "Unique Characters: 68\n"
          ]
        }
      ],
      "source": [
        "with open('Rustaveli.txt', 'r', encoding=\"utf8\") as fl:\n",
        "    text=fl.read()\n",
        "    \n",
        "start_indx = text.find('Shota Rustaveli')\n",
        "end_indx = text.find('The End')\n",
        "\n",
        "text = text[start_indx:end_indx]       # where starts the text and where it ends\n",
        "char_set = set(text)                   # convert into set to get only unique characters\n",
        "print('Total Length:', len(text))\n",
        "print('Unique Characters:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MAVH8-naLHh",
        "outputId": "f76c931f-9083-4283-fb54-827a93d285c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (403097,)\n",
            "Shota Rustaveli      == Encoding ==>  [32 48 55 60 41  2 31 61 59 60 41 62 45 52 49]\n"
          ]
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}   #we give the set the form of character string encoding/ often refered as character dictionary\n",
        "char_array = np.array(chars_sorted)                   \n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)                                     # string encodings for each charackter in text order\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "# print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:30]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtImMGWpaLHi",
        "outputId": "42ed5cde-0c4a-49f1-ef3c-ed0dfa491d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 -> S\n",
            "48 -> h\n",
            "55 -> o\n",
            "60 -> t\n",
            "41 -> a\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVEb0Ng6aLHk",
        "outputId": "d5271d75-a73e-4d01-c84a-6d9bdfa5c180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32 48 55 60 41  2 31 61 59 60 41 62 45 52 49  2  1  1 22 54 60 58 55 44\n",
            " 61 43 60 55 58 65  2 30 61 41 60 58 41 49 54 59]  ->  1\n",
            "'Shota Rustaveli \\n\\nIntroductory Quatrains'  ->  '\\n'\n"
          ]
        }
      ],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size] \n",
        "               for i in range(len(text_encoded)-chunk_size+1)]   # because 10 chunks is not len(10) but len(9)\n",
        "\n",
        "for seq in text_chunks[:1]:                     #let's look at the first chunk\n",
        "    input_seq = seq[:seq_length]                #sequence chunk that we use for prediction of next letter\n",
        "    target = seq[seq_length]                    #next letter that has to be predicted \n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])), # repr() simple way of giving string representation of the value\n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyarVFL0aLHk",
        "outputId": "0d5106d5-3c54-4903-b674-d527308605c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  del sys.path[0]\n"
          ]
        }
      ],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()  #chunk with all elements except last one, except first one\n",
        "        # these are starting sequence and target sequence, which is shifted right by one character     \n",
        "        \n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfrRIHj0aLHl",
        "outputId": "55e0450f-e1fa-450a-8957-1515a155274a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input (x): 'Shota Rustaveli \\n\\nIntroductory Quatrains'\n",
            "Target (y): 'hota Rustaveli \\n\\nIntroductory Quatrains\\n'\n",
            "\n",
            " Input (x): 'hota Rustaveli \\n\\nIntroductory Quatrains\\n'\n",
            "Target (y): 'ota Rustaveli \\n\\nIntroductory Quatrains\\n\\n'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# just inspect if everything works as expeced\n",
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
        "    print('Target (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mF9pIbp9aLHm"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdt8Ujn0aLHm",
        "outputId": "d2c25040-b8e5-47d2-8114-d0c0fb6dab36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(68, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=68, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)   #nn.Embedding just looks up nonzero positions to avoid computations on zero positions\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
        "                           batch_first=True)\n",
        "        self.linear = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.linear(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell                         # out, hidden, cell states are given after each iteration\n",
        "\n",
        "    def init_hidden(self, batch_size):                   #to initialize with batch size\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)        #and get (re-)started hiiden and cell states            \n",
        "\n",
        "###------------------ version with GRU and Variable ----------------------------\n",
        "\n",
        "# from torch.autograd import Variable\n",
        "# class RNN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "#         super(RNN, self).__init__()\n",
        "#         self.input_size = input_size\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.output_size = output_size\n",
        "#         self.n_layers = n_layers\n",
        "        \n",
        "#         self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "#         self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "#         self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "#     def forward(self, input, hidden):\n",
        "#         input = self.encoder(input.view(1, -1))\n",
        "#         output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "#         output = self.decoder(output.view(1, -1))\n",
        "#         return output, hidden\n",
        "\n",
        "#     def init_hidden(self):\n",
        "#         return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
        "\n",
        "### ----------------------------------------------------------------------------\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp2bztxZaLHn"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000 \n",
        "\n",
        "best = float('inf')\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size) # we clean hidden and cell states each epoch but parameters stay\n",
        "    seq_batch, target_batch = next(iter(seq_dl)) # for each epoch we just take next batch/chunk from dataloader\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell) \n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')\n",
        "        if loss<best:\n",
        "          best = loss\n",
        "          torch.save(model.state_dict(), './model.pth')\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model.load_state_dict(torch.load('./model.pth'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhY5DNUfvyko",
        "outputId": "4943cc9f-062b-4af9-bd34-15b339a70888"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(68, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=68, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLAZ_0qFaLHo",
        "outputId": "a7dd1f25-c1cd-4690-c5cb-cc6fe5eadd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peace to world (or as a brother, they will not take myself, advice it is necessary treachery as sun-like one was a hundred (her in his praise was a far a waiting tears!\n",
            " \"The maiden sat shedder water, how have senseless the voiced with you. When night the rose-tintless Tariel, say: 'She forsook would meet it? Spoke now faint?\n",
            "     P’hatman,' Thou went back steel is beloved about. Avt’handil's night, would grieve not Dame in a palanquishing built; you we shoulded here, and at no timble became to battle crepara\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, starting_str, \n",
        "           len_generated_text=500, \n",
        "           temperature=1.0):\n",
        "    #starting string is the string you give as starting sequence for further generation\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)) #just put everithing in second dimention\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
        "    \n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
        "        logits = torch.squeeze(logits, 0)\n",
        "\n",
        "        # scale factor is in the role of temperature patameter (1 over inverse temperature beta) in logsumexp\n",
        "        \n",
        "        ## sampling with raw code, I have to improve this version \n",
        "        dist = logits.view(-1).div(temperature).exp()\n",
        "        last_char = torch.multinomial(dist, 1)[0]        #this is actually not a multinomial necessarily but it works   \n",
        "\n",
        "        ## sampling with Categorical()\n",
        "        # scaled_logits = logits * temperature\n",
        "        # dist = Categorical(logits=scaled_logits)      #we create new distribution for each next character to sample\n",
        "        # last_char = dist.sample()                     #notice, this is done for each next character!\n",
        "\n",
        "        generated_str += str(char_array[last_char])\n",
        "        \n",
        "    return generated_str\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(evaluate(model, starting_str='peace to world'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp2SUA-HaLHq",
        "outputId": "1df7f990-0aed-4a6b-f5bb-7f5d76aafacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peace to world (or a revered) pale--earnest them that look on the partridges; what is the man of the same as was a hundrest of the heart, and who are not well as the sun of the heart of adamant. The king sat down and sought the desire calmed by stone and crystal. P’hridon met me alives the seashore, I shall set as the same and panther or a heap; they play as if the sun in my story of my story?\" said he. What I show my sadness at the healing of heaven of the seventh folk; they become a net of a mountain and re\n"
          ]
        }
      ],
      "source": [
        "## be avare how increase of temperature increases the randomness\n",
        "\n",
        "torch.manual_seed(1)\n",
        "print(evaluate(model, starting_str='peace to world', \n",
        "             temperature=0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY2NimdSaLHq",
        "outputId": "4b67851b-4a8e-4423-bfcc-6e0a243b2d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peace to world (or helpers) with the seashore. When they said: \"What a tenth and said to me: 'What hath a man arose and song, and the moon should they desire the seashore of the seashore is this world and heard of his days. When they say: 'Selds the Kadjis are the sun (Tariel's) armies to see me. When they said to me: 'What a stand the maiden saw the plains; the maiden sat down and stood consumed to meet me. It is better than a moment he saw a madman, I said: 'Stand!' (gong caress (of the Seas) was a gift, th\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(evaluate(model, starting_str='peace to world', \n",
        "             temperature=0.25))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}