{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h4nFvJsdBS9f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "R E F E R E N C E: \n",
        "\n",
        "Deep Learning with PyTorch ELI STEVENS, LUCA ANTIGA, AND THOMAS VIEHMANN\n",
        "\n",
        "Data:\n",
        "\n",
        "https://luna16.grand-challenge.org/Download/"
      ],
      "metadata": {
        "id": "fTbBdUHSCNI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title installs and imports\n",
        "\n",
        "!pip install -q diskcache\n",
        "!pip install -q SimpleITK\n",
        "!pip install -q cassandra-driver\n",
        "\n",
        "# got help from this code:\n",
        "# https://github.com/JonathanSum/pytorch-Deep-Learning_colab/blob/master/p2ch10_explore_data.ipynb\n",
        "from cassandra.cqltypes import BytesType\n",
        "from diskcache import FanoutCache, Disk,core\n",
        "from diskcache.core import io\n",
        "from io import BytesIO\n",
        "from diskcache.core import MODE_BINARY\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch import nn as nn\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import SGD\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import scipy.ndimage.morphology as morphology\n",
        "\n",
        "import torch.cuda\n",
        "import math\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import datetime\n",
        "import hashlib\n",
        "import socket\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim\n",
        "from torch.optim import SGD, Adam"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZQDBEicjZ64B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data download from here: https://luna16.grand-challenge.org/Download/\n",
        "## if __name__ == '__main__' and '__file__' in globals():\n",
        "\n",
        "!wget -O annotations.csv -q https://zenodo.org/record/3723295/files/annotations.csv?download=1\n",
        "!wget -O candidates.csv -q https://zenodo.org/record/3723295/files/candidates.csv?download=1\n",
        "!wget -O subset0.zip -q https://zenodo.org/record/3723295/files/subset0.zip?download=1\n",
        "\n",
        "!wget -O annotations_with_malignancy.csv -q https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/data/part2/luna/annotations_with_malignancy.csv\n",
        "\n",
        "!brew install p7zip   # https://unix.stackexchange.com/questions/115825/extra-bytes-error-when-unzipping-a-file\n",
        "!7za x subset0.zip    # used instead of !unzip subset0.zip\n",
        "# !rm subset0.zip\n",
        "\n",
        "## when you go to the url https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/data/part2/luna/annotations_with_malignancy.csv\n",
        "## you just click button \"raw\" and copy raw data link which is given below\n",
        "##### the same csv file also available here, if first method does not work: https://1drv.ms/u/s!AhnVhbVlzYkKgQg3z27GtN5OTjWG?e=jRRPAM\n",
        "url = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/part2/luna/annotations_with_malignancy.csv'\n",
        "df = pd.read_csv(url, encoding='unicode_escape')\n",
        "df.set_index('seriesuid').to_csv('annotations_with_malignancy.csv') \n",
        "\n",
        "## do not do this, use cuda, otherwize everything becomes incomprehensibly slow\n",
        "## device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "StQH3c7OfhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title functional tools\n",
        "\n",
        "\n",
        "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
        "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
        "\n",
        "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
        "    cri_a = np.array(coord_irc)[::-1]\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
        "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
        "    return XyzTuple(*coords_xyz)\n",
        "\n",
        "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coord_a = np.array(coord_xyz)\n",
        "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
        "    cri_a = np.round(cri_a)\n",
        "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n",
        "\n",
        "\n",
        "def importstr(module_str, from_=None):\n",
        "    \"\"\"\n",
        "    >>> importstr('os')\n",
        "    <module 'os' from '.../os.pyc'>\n",
        "    >>> importstr('math', 'fabs')\n",
        "    <built-in function fabs>\n",
        "    \"\"\"\n",
        "    if from_ is None and ':' in module_str:\n",
        "        module_str, from_ = module_str.rsplit(':')\n",
        "\n",
        "    module = __import__(module_str)\n",
        "    for sub_str in module_str.split('.')[1:]:\n",
        "        module = getattr(module, sub_str)\n",
        "\n",
        "    if from_:\n",
        "        try:\n",
        "            return getattr(module, from_)\n",
        "        except:\n",
        "            raise ImportError('{}.{}'.format(module_str, from_))\n",
        "    return module\n",
        "\n",
        "\n",
        "\n",
        "def prhist(ary, prefix_str=None, **kwargs):\n",
        "    if prefix_str is None:\n",
        "        prefix_str = ''\n",
        "    else:\n",
        "        prefix_str += ' '\n",
        "\n",
        "    count_ary, bins_ary = np.histogram(ary, **kwargs)\n",
        "    for i in range(count_ary.shape[0]):\n",
        "        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n",
        "    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enumerateWithEstimate(\n",
        "        iter,\n",
        "        desc_str,\n",
        "        start_ndx=0,\n",
        "        print_ndx=4,\n",
        "        backoff=None,\n",
        "        iter_len=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    In terms of behavior, `enumerateWithEstimate` is almost identical\n",
        "    to the standard `enumerate` (the differences are things like how\n",
        "    our function returns a generator, while `enumerate` returns a\n",
        "    specialized `<enumerate object at 0x...>`).\n",
        "    \"\"\"\n",
        "    if iter_len is None:\n",
        "        iter_len = len(iter)\n",
        "\n",
        "    if backoff is None:\n",
        "        backoff = 2\n",
        "        while backoff ** 7 < iter_len:\n",
        "            backoff *= 2\n",
        "\n",
        "    assert backoff >= 2\n",
        "    while print_ndx < start_ndx * backoff:\n",
        "        print_ndx *= backoff\n",
        "\n",
        "    print(\"{} ----/{}, starting\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "    ))\n",
        "    start_ts = time.time()\n",
        "    for (current_ndx, item) in enumerate(iter):\n",
        "        yield (current_ndx, item)\n",
        "        if current_ndx == print_ndx:\n",
        "            # ... <1>\n",
        "            duration_sec = ((time.time() - start_ts)\n",
        "                            / (current_ndx - start_ndx + 1)\n",
        "                            * (iter_len-start_ndx)\n",
        "                            )\n",
        "\n",
        "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
        "            done_td = datetime.timedelta(seconds=duration_sec)\n",
        "\n",
        "            print(\"{} {:-4}/{}, done at {}, {}\".format(\n",
        "                desc_str,\n",
        "                current_ndx,\n",
        "                iter_len,\n",
        "                str(done_dt).rsplit('.', 1)[0],\n",
        "                str(done_td).rsplit('.', 1)[0],\n",
        "            ))\n",
        "\n",
        "            print_ndx *= backoff\n",
        "\n",
        "        if current_ndx + 1 == start_ndx:\n",
        "            start_ts = time.time()\n",
        "\n",
        "    print(\"{} ----/{}, done at {}\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "        str(datetime.datetime.now()).rsplit('.', 1)[0],\n",
        "    ))\n",
        "\n",
        "########################################################\n",
        "\n",
        "\n",
        "class GzipDisk(Disk):\n",
        "    def store(self, value, read, key=None):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param value: value to convert\n",
        "        :param bool read: True when value is file-like object\n",
        "        :return: (size, mode, filename, value) tuple for Cache table\n",
        "        \"\"\"\n",
        "        # pylint: disable=unidiomatic-typecheck\n",
        "        if type(value) is BytesType:\n",
        "            if read:\n",
        "                value = value.read()\n",
        "                read = False\n",
        "\n",
        "            str_io = BytesIO()\n",
        "            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n",
        "\n",
        "            for offset in range(0, len(value), 2**30):\n",
        "                gz_file.write(value[offset:offset+2**30])\n",
        "            gz_file.close()\n",
        "\n",
        "            value = str_io.getvalue()\n",
        "\n",
        "        return super(GzipDisk, self).store(value, read)\n",
        "\n",
        "\n",
        "    def fetch(self, mode, filename, value, read):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param int mode: value mode raw, binary, text, or pickle\n",
        "        :param str filename: filename of corresponding value\n",
        "        :param value: database value\n",
        "        :param bool read: when True, return an open file handle\n",
        "        :return: corresponding Python value\n",
        "        \"\"\"\n",
        "        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n",
        "\n",
        "        if mode == MODE_BINARY:\n",
        "            str_io = BytesIO(value)\n",
        "            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n",
        "            read_csio = BytesIO()\n",
        "\n",
        "            while True:\n",
        "                uncompressed_data = gz_file.read(2**30)\n",
        "                if uncompressed_data:\n",
        "                    read_csio.write(uncompressed_data)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            value = read_csio.getvalue()\n",
        "\n",
        "        return value\n",
        "\n",
        "def getCache(scope_str):\n",
        "    return FanoutCache('data-unversioned/cache/' + scope_str,\n",
        "                       disk=GzipDisk,\n",
        "                       shards=64,\n",
        "                       timeout=1,\n",
        "                       size_limit=3e11,\n",
        "                       # disk_min_file_size=2**20,\n",
        "                       )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZwKvesFZbJEC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualization tools\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "def findPositiveSamples(start_ndx=0, limit=10):\n",
        "    ds = LunaDataset(sortby_str='label_and_size')\n",
        "\n",
        "    positiveSample_list = []\n",
        "    for sample_tup in ds.candidateInfo_list:\n",
        "        if sample_tup.isNodule_bool:\n",
        "            print(len(positiveSample_list), sample_tup)\n",
        "            positiveSample_list.append(sample_tup)\n",
        "\n",
        "        if len(positiveSample_list) >= limit:\n",
        "            break\n",
        "\n",
        "    return positiveSample_list\n",
        "\n",
        "def showCandidate(series_uid, batch_ndx=None, **kwargs):\n",
        "    ds = LunaDataset(series_uid=series_uid, **kwargs)\n",
        "    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n",
        "\n",
        "    if batch_ndx is None:\n",
        "        if pos_list:\n",
        "            batch_ndx = pos_list[0]\n",
        "        else:\n",
        "            print(\"Warning: no positive samples found; using first negative sample.\")\n",
        "            batch_ndx = 0\n",
        "\n",
        "    ct = Ct(series_uid)\n",
        "    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n",
        "    ct_a = ct_t[0].numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 50))\n",
        "\n",
        "    group_list = [\n",
        "        [9, 11, 13],\n",
        "        [15, 16, 17],\n",
        "        [19, 21, 23],\n",
        "    ]\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[int(center_irc.index)], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,int(center_irc.row)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,:,int(center_irc.col)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for row, index_list in enumerate(group_list):\n",
        "        for col, index in enumerate(index_list):\n",
        "            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n",
        "            subplot.set_title('slice {}'.format(index), fontsize=30)\n",
        "            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "                label.set_fontsize(20)\n",
        "            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n",
        "\n",
        "\n",
        "    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)\n",
        "\n",
        "\n",
        "#####-------------------------------   Segmentation tasks\n",
        "\n",
        "MaskTuple = namedtuple('MaskTuple', 'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask')\n",
        "\n",
        "class SegmentationMask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_list = nn.ModuleList([\n",
        "            self._make_circle_conv(radius) for radius in range(1, 8)\n",
        "        ])\n",
        "\n",
        "    def _make_circle_conv(self, radius):\n",
        "        diameter = 1 + radius * 2\n",
        "\n",
        "        a = torch.linspace(-1, 1, steps=diameter)**2\n",
        "        b = (a[None] + a[:, None])**0.5\n",
        "\n",
        "        circle_weights = (b <= 1.0).to(torch.float32)\n",
        "\n",
        "        conv = nn.Conv2d(1, 1, kernel_size=diameter, padding=radius, bias=False)\n",
        "        conv.weight.data.fill_(1)\n",
        "        conv.weight.data *= circle_weights / circle_weights.sum()\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "    def erode(self, input_mask, radius, threshold=1):\n",
        "        conv = self.conv_list[radius - 1]\n",
        "        input_float = input_mask.to(torch.float32)\n",
        "        result = conv(input_float)\n",
        "\n",
        "        # log.debug(['erode in ', radius, threshold, input_float.min().item(), input_float.mean().item(), input_float.max().item()])\n",
        "        # log.debug(['erode out', radius, threshold, result.min().item(), result.mean().item(), result.max().item()])\n",
        "\n",
        "        return result >= threshold\n",
        "\n",
        "    def deposit(self, input_mask, radius, threshold=0):\n",
        "        conv = self.conv_list[radius - 1]\n",
        "        input_float = input_mask.to(torch.float32)\n",
        "        result = conv(input_float)\n",
        "\n",
        "        # log.debug(['deposit in ', radius, threshold, input_float.min().item(), input_float.mean().item(), input_float.max().item()])\n",
        "        # log.debug(['deposit out', radius, threshold, result.min().item(), result.mean().item(), result.max().item()])\n",
        "\n",
        "        return result > threshold\n",
        "\n",
        "    def fill_cavity(self, input_mask):\n",
        "        cumsum = input_mask.cumsum(-1)\n",
        "        filled_mask = (cumsum > 0)\n",
        "        filled_mask &= (cumsum < cumsum[..., -1:])\n",
        "        cumsum = input_mask.cumsum(-2)\n",
        "        filled_mask &= (cumsum > 0)\n",
        "        filled_mask &= (cumsum < cumsum[..., -1:, :])\n",
        "\n",
        "        return filled_mask\n",
        "\n",
        "\n",
        "    def forward(self, input_g, raw_pos_g):\n",
        "        gcc_g = input_g + 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # log.info(['gcc_g', gcc_g.min(), gcc_g.mean(), gcc_g.max()])\n",
        "\n",
        "            raw_dense_mask = gcc_g > 0.7\n",
        "            dense_mask = self.deposit(raw_dense_mask, 2)\n",
        "            dense_mask = self.erode(dense_mask, 6)\n",
        "            dense_mask = self.deposit(dense_mask, 4)\n",
        "\n",
        "            body_mask = self.fill_cavity(dense_mask)\n",
        "            air_mask = self.deposit(body_mask & ~dense_mask, 5)\n",
        "            air_mask = self.erode(air_mask, 6)\n",
        "\n",
        "            lung_mask = self.deposit(air_mask, 5)\n",
        "\n",
        "            raw_candidate_mask = gcc_g > 0.4\n",
        "            raw_candidate_mask &= air_mask\n",
        "            candidate_mask = self.erode(raw_candidate_mask, 1)\n",
        "            candidate_mask = self.deposit(candidate_mask, 1)\n",
        "\n",
        "            pos_mask = self.deposit((raw_pos_g > 0.5) & lung_mask, 2)\n",
        "\n",
        "            neg_mask = self.deposit(candidate_mask, 1)\n",
        "            neg_mask &= ~pos_mask\n",
        "            neg_mask &= lung_mask\n",
        "\n",
        "            # label_g = (neg_mask | pos_mask).to(torch.float32)\n",
        "            label_g = (pos_mask).to(torch.float32)\n",
        "            neg_g = neg_mask.to(torch.float32)\n",
        "            pos_g = pos_mask.to(torch.float32)\n",
        "\n",
        "        mask_dict = {\n",
        "            'raw_dense_mask': raw_dense_mask,\n",
        "            'dense_mask': dense_mask,\n",
        "            'body_mask': body_mask,\n",
        "            'air_mask': air_mask,\n",
        "            'raw_candidate_mask': raw_candidate_mask,\n",
        "            'candidate_mask': candidate_mask,\n",
        "            'lung_mask': lung_mask,\n",
        "            'neg_mask': neg_mask,\n",
        "            'pos_mask': pos_mask,\n",
        "        }\n",
        "\n",
        "        return label_g, neg_g, pos_g, lung_mask, mask_dict\n",
        "\n",
        "def build2dLungMask(series_uid, center_ndx):\n",
        "    mask_model = SegmentationMask().to('cuda')\n",
        "    ct = Ct(series_uid)\n",
        "\n",
        "    ct_g = torch.from_numpy(ct.hu_a[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n",
        "    pos_g = torch.from_numpy(ct.positive_mask[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n",
        "    input_g = ct_g / 1000\n",
        "\n",
        "    label_g, neg_g, pos_g, lung_mask, mask_dict = mask_model(input_g, pos_g)\n",
        "    mask_tup = MaskTuple(**mask_dict)\n",
        "\n",
        "    return mask_tup\n",
        "\n",
        "#####------------------------------- for plotting metrics; precision and recall\n",
        "\n",
        "range_a = np.arange(0.01, 1, 0.01)\n",
        "precision_a, recall_a = np.meshgrid(range_a, range_a)\n",
        "\n",
        "f1_score = np.sqrt(2 * precision_a * recall_a / (precision_a + recall_a))\n",
        "\n",
        "def plotScore(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=1, dpi=300, figsize=(7/2, 2.5))\n",
        "\n",
        "    subplts.set_title(title_str + \"(p, r)\")\n",
        "    subplts.contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts.set_xlabel(\"precision\")\n",
        "    subplts.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def plotScores(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=2, dpi=300, figsize=(7, 3.5))\n",
        "\n",
        "    subplts[0].set_title(title_str + \"(p, r)\")\n",
        "    subplts[0].contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts[1].set_title(\"f1(p, r)\")\n",
        "    subplts[1].contourf(f1_score, cmap='gray')\n",
        "\n",
        "    #subplts[2].set_title(\"f1 - \" + title_str)\n",
        "    #subplts[2].contourf(f1_score - other_score, cmap='gray')\n",
        "\n",
        "    for subplt in subplts:\n",
        "        subplt.set_xlabel(\"precision\")\n",
        "        subplt.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# add_score = (precision_a + recall_a) / 2\n",
        "# plotScores(\"avg\", add_score)\n",
        "\n",
        "# min_score = np.min(np.array([precision_a, recall_a]), axis=0)\n",
        "# plotScores(\"min\", min_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# sqrt_score = np.sqrt(precision_a * recall_a)\n",
        "# plotScores(\"sqrt\", sqrt_score)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r_mzaF-ajCvD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for the model used for classification task"
      ],
      "metadata": {
        "id": "h4nFvJsdBS9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xUBxjBMjYxRH"
      },
      "outputs": [],
      "source": [
        "#@title data for classification\n",
        "\n",
        "\n",
        "raw_cache = getCache('classify_raw')\n",
        "\n",
        "CandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, diameter_mm, series_uid, center_xyz')\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    diameter_dict = {}\n",
        "    with open('/content/annotations.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "\n",
        "            diameter_dict.setdefault(series_uid, []).append(\n",
        "                (annotationCenter_xyz, annotationDiameter_mm),\n",
        "            )\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            candidateDiameter_mm = 0.0\n",
        "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
        "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
        "                for i in range(3):\n",
        "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
        "                    if delta_mm > annotationDiameter_mm / 4:\n",
        "                        break\n",
        "                else:\n",
        "                    candidateDiameter_mm = annotationDiameter_mm\n",
        "                    break\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(\n",
        "                isNodule_bool,\n",
        "                candidateDiameter_mm,\n",
        "                series_uid,\n",
        "                candidateCenter_xyz,\n",
        "            ))\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_a.clip(-1000, 1000, ct_a)\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(\n",
        "            center_xyz,\n",
        "            self.origin_xyz,\n",
        "            self.vxSize_xyz,\n",
        "            self.direction_a,\n",
        "        )\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc\n",
        "\n",
        "def getCtAugmentedCandidate(\n",
        "        augmentation_dict,\n",
        "        series_uid, center_xyz, width_irc,\n",
        "        use_cache=True):\n",
        "    if use_cache:\n",
        "        ct_chunk, center_irc = \\\n",
        "            getCtRawCandidate(series_uid, center_xyz, width_irc)\n",
        "    else:\n",
        "        ct = getCt(series_uid)\n",
        "        ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "\n",
        "    ct_t = torch.tensor(ct_chunk).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "    transform_t = torch.eye(4)\n",
        "    # ... <1>\n",
        "\n",
        "    for i in range(3):\n",
        "        if 'flip' in augmentation_dict:\n",
        "            if random.random() > 0.5:\n",
        "                transform_t[i,i] *= -1\n",
        "\n",
        "        if 'offset' in augmentation_dict:\n",
        "            offset_float = augmentation_dict['offset']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,3] = offset_float * random_float\n",
        "\n",
        "        if 'scale' in augmentation_dict:\n",
        "            scale_float = augmentation_dict['scale']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "\n",
        "    if 'rotate' in augmentation_dict:\n",
        "        angle_rad = random.random() * math.pi * 2\n",
        "        s = math.sin(angle_rad)\n",
        "        c = math.cos(angle_rad)\n",
        "\n",
        "        rotation_t = torch.tensor([\n",
        "            [c, -s, 0, 0],\n",
        "            [s, c, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ])\n",
        "\n",
        "        transform_t @= rotation_t\n",
        "\n",
        "    affine_t = F.affine_grid(\n",
        "            transform_t[:3].unsqueeze(0).to(torch.float32),\n",
        "            ct_t.size(),\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "    augmented_chunk = F.grid_sample(\n",
        "            ct_t,\n",
        "            affine_t,\n",
        "            padding_mode='border',\n",
        "            align_corners=False,\n",
        "        ).to('cpu')\n",
        "\n",
        "    if 'noise' in augmentation_dict:\n",
        "        noise_t = torch.randn_like(augmented_chunk)\n",
        "        noise_t *= augmentation_dict['noise']\n",
        "\n",
        "        augmented_chunk += noise_t\n",
        "\n",
        "    return augmented_chunk[0], center_irc\n",
        "\n",
        "\n",
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 sortby_str='random',\n",
        "                 ratio_int=0,\n",
        "                 augmentation_dict=None,\n",
        "                 candidateInfo_list=None,\n",
        "            ):\n",
        "        self.ratio_int = ratio_int\n",
        "        self.augmentation_dict = augmentation_dict\n",
        "\n",
        "        if candidateInfo_list:\n",
        "            self.candidateInfo_list = copy.copy(candidateInfo_list)\n",
        "            self.use_cache = False\n",
        "        else:\n",
        "            self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "            self.use_cache = True\n",
        "\n",
        "        if series_uid:\n",
        "            self.candidateInfo_list = [\n",
        "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
        "            ]\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "        elif val_stride > 0:\n",
        "            del self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "\n",
        "        if sortby_str == 'random':\n",
        "            random.shuffle(self.candidateInfo_list)\n",
        "        elif sortby_str == 'series_uid':\n",
        "            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n",
        "        elif sortby_str == 'label_and_size':\n",
        "            pass\n",
        "        else:\n",
        "            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n",
        "\n",
        "        self.negative_list = [\n",
        "            nt for nt in self.candidateInfo_list if not nt.isNodule_bool\n",
        "        ]\n",
        "        self.pos_list = [\n",
        "            nt for nt in self.candidateInfo_list if nt.isNodule_bool\n",
        "        ]\n",
        "\n",
        "        print(\"{!r}: {} {} samples, {} neg, {} pos, {} ratio\".format(\n",
        "            self,\n",
        "            len(self.candidateInfo_list),\n",
        "            \"validation\" if isValSet_bool else \"training\",\n",
        "            len(self.negative_list),\n",
        "            len(self.pos_list),\n",
        "            '{}:1'.format(self.ratio_int) if self.ratio_int else 'unbalanced'\n",
        "        ))\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        if self.ratio_int:\n",
        "            random.shuffle(self.negative_list)\n",
        "            random.shuffle(self.pos_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.ratio_int:\n",
        "            return 200000\n",
        "        else:\n",
        "            return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        if self.ratio_int:\n",
        "            pos_ndx = ndx // (self.ratio_int + 1)\n",
        "\n",
        "            if ndx % (self.ratio_int + 1):\n",
        "                neg_ndx = ndx - 1 - pos_ndx\n",
        "                neg_ndx %= len(self.negative_list)\n",
        "                candidateInfo_tup = self.negative_list[neg_ndx]\n",
        "            else:\n",
        "                pos_ndx %= len(self.pos_list)\n",
        "                candidateInfo_tup = self.pos_list[pos_ndx]\n",
        "        else:\n",
        "            candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        if self.augmentation_dict:\n",
        "            candidate_t, center_irc = getCtAugmentedCandidate(\n",
        "                self.augmentation_dict,\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "                self.use_cache,\n",
        "            )\n",
        "        elif self.use_cache:\n",
        "            candidate_a, center_irc = getCtRawCandidate(\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "        else:\n",
        "            ct = getCt(candidateInfo_tup.series_uid)\n",
        "            candidate_a, center_irc = ct.getRawCandidate(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "\n",
        "        pos_t = torch.tensor([\n",
        "                not candidateInfo_tup.isNodule_bool,\n",
        "                candidateInfo_tup.isNodule_bool\n",
        "            ],\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        return candidate_t, pos_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)\n",
        "\n",
        "\n",
        "!mkdir ./data-unversioned/classification\n",
        "!mkdir ./data-unversioned/classification/models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title check if you get data in a correct form or if you are getting it at all\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
        "candidateInfo_list[0]\n",
        "\n",
        "positiveSample_list = findPositiveSamples()\n",
        "\n",
        "augmentation_dict = {}\n",
        "augmentation_list = [\n",
        "    ('None', {}),\n",
        "    ('flip', {'flip': True}),\n",
        "    ('offset', {'offset': 0.1}),\n",
        "    ('scale', {'scale': 0.2}),\n",
        "    ('rotate', {'rotate': True}),\n",
        "    ('noise', {'noise': 25.0}),    \n",
        "]\n",
        "ds_list = [\n",
        "    LunaDataset(sortby_str='label_and_size', augmentation_dict=augmentation_dict) \n",
        "    for title_str, augmentation_dict in augmentation_list\n",
        "]\n",
        "\n",
        "all_dict = {}\n",
        "for title_str, augmentation_dict in augmentation_list:\n",
        "    all_dict.update(augmentation_dict)\n",
        "all_ds = LunaDataset(sortby_str='label_and_size', augmentation_dict=all_dict)\n",
        "\n",
        "augmentation_list.extend([('All', augmentation_dict)] * 3)\n",
        "ds_list.extend([all_ds] * 3)\n",
        "\n",
        "##------------------------------------------------- try this in anew cell\n",
        "sample_ndx = 100\n",
        "sample_ndx = 154\n",
        "sample_ndx = 155\n",
        "\n",
        "sample_tup = all_ds[sample_ndx]\n",
        "print(sample_tup[0].shape, sample_tup[1:])\n",
        "\n",
        "fig = plt.figure(figsize=(30, 30))\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "for i, ((title_str, _), ds) in enumerate(zip(augmentation_list, ds_list)):\n",
        "    sample_tup = ds[sample_ndx]\n",
        "    subplot = fig.add_subplot(3, 3, i+1)\n",
        "    subplot.set_title(title_str, fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(sample_tup[0][0][16], clim=clim, cmap='gray')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6kPyvjfkh_L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification model\n",
        "\n",
        "\n",
        "class LunaModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, conv_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
        "\n",
        "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
        "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
        "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
        "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
        "\n",
        "        self.head_linear = nn.Linear(1152, 2)\n",
        "        self.head_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # see also https://github.com/pytorch/pytorch/issues/18182\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {\n",
        "                nn.Linear,\n",
        "                nn.Conv3d,\n",
        "                nn.Conv2d,\n",
        "                nn.ConvTranspose2d,\n",
        "                nn.ConvTranspose3d,\n",
        "            }:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.tail_batchnorm(input_batch)\n",
        "\n",
        "        block_out = self.block1(bn_output)\n",
        "        block_out = self.block2(block_out)\n",
        "        block_out = self.block3(block_out)\n",
        "        block_out = self.block4(block_out)\n",
        "\n",
        "        conv_flat = block_out.view(\n",
        "            block_out.size(0),\n",
        "            -1,\n",
        "        )\n",
        "        linear_output = self.head_linear(conv_flat)\n",
        "\n",
        "        return linear_output, self.head_softmax(linear_output)\n",
        "\n",
        "\n",
        "class LunaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        block_out = self.conv1(input_batch)\n",
        "        block_out = self.relu1(block_out)\n",
        "        block_out = self.conv2(block_out)\n",
        "        block_out = self.relu2(block_out)\n",
        "\n",
        "        return self.maxpool(block_out)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cM3kIGKBsWJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification training (TODO: add model saving here !!!!!)\n",
        "\n",
        "\n",
        "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_LOSS_NDX=2\n",
        "METRICS_SIZE = 3\n",
        "\n",
        "class LunaTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=32,\n",
        "                 num_workers : int=2,\n",
        "                 epochs: int=1,\n",
        "                 balanced = False,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'classification',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.balanced = balanced,\n",
        "        self.augmented = augmented,\n",
        "        self.augment_flip = augment_flip,\n",
        "        self.augment_offset = augment_offset,\n",
        "        self.augment_scale = augment_scale,\n",
        "        self.augment_rotate = augment_rotate,\n",
        "        self.augment_noise = augment_noise,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.totalTrainingSamples_count = 0\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.1\n",
        "        if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                model = nn.DataParallel(model)\n",
        "            model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        # return Adam(self.model.parameters())\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int= bool(self.balanced),    \n",
        "            augmentation_dict=self.augmentation_dict,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('runs', self.tb_prefix, self.time_str)\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-trn_cls-' + self.comment)\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-val_cls-' + self.comment)\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        for epoch_ndx in range(1, int(*self.epochs) + 1):\n",
        "\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                self.epochs,\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                int(*self.batch_size),\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "\n",
        "        if hasattr(self, 'trn_writer'):\n",
        "            self.trn_writer.close()\n",
        "            self.val_writer.close()\n",
        "\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g,\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            valMetrics_g = torch.zeros(\n",
        "                METRICS_SIZE,\n",
        "                len(val_dl.dataset),\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(\n",
        "                    batch_ndx,\n",
        "                    batch_tup,\n",
        "                    val_dl.batch_size,\n",
        "                    valMetrics_g,\n",
        "                )\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "        input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
        "        loss_g = loss_func(\n",
        "            logits_g,\n",
        "            label_g[:,1],\n",
        "        )\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "        return loss_g.mean()\n",
        "\n",
        "\n",
        "    def logMetrics(\n",
        "            self,\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            metrics_t,\n",
        "            classificationThreshold=0.5,\n",
        "    ):\n",
        "        self.initTensorboardWriters()\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
        "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
        "\n",
        "        posLabel_mask = ~negLabel_mask\n",
        "        posPred_mask = ~negPred_mask\n",
        "\n",
        "        neg_count = int(negLabel_mask.sum())\n",
        "        pos_count = int(posLabel_mask.sum())\n",
        "\n",
        "        trueNeg_count = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
        "        truePos_count = pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
        "\n",
        "        falsePos_count = neg_count - neg_correct\n",
        "        falseNeg_count = pos_count - pos_correct\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
        "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
        "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
        "\n",
        "        metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n",
        "        metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n",
        "        metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falsePos_count)\n",
        "        recall    = metrics_dict['pr/recall'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falseNeg_count)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = \\\n",
        "            2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
        "                 + \"{correct/all:-5.1f}% correct, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
        "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_neg',\n",
        "                neg_correct=neg_correct,\n",
        "                neg_count=neg_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
        "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_pos',\n",
        "                pos_correct=pos_correct,\n",
        "                pos_count=pos_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.add_pr_curve(\n",
        "            'pr',\n",
        "            metrics_t[METRICS_LABEL_NDX],\n",
        "            metrics_t[METRICS_PRED_NDX],\n",
        "            self.totalTrainingSamples_count,\n",
        "        )\n",
        "\n",
        "        bins = [x/50.0 for x in range(51)]\n",
        "\n",
        "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
        "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
        "\n",
        "        if negHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_neg',\n",
        "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "        if posHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_pos',\n",
        "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "\n",
        "        # score = 1 \\\n",
        "        #     + metrics_dict['pr/f1_score'] \\\n",
        "        #     - metrics_dict['loss/mal'] * 0.01 \\\n",
        "        #     - metrics_dict['loss/all'] * 0.0001\n",
        "        #\n",
        "        # return score\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             try:\n",
        "    #                 writer.add_histogram(\n",
        "    #                     name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                     param.data.cpu().numpy(),\n",
        "    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                     self.totalTrainingSamples_count,\n",
        "    #                     # bins=bins,\n",
        "    #                 )\n",
        "    #             except Exception as e:\n",
        "    #                 log.error([min_data, max_data])\n",
        "    #                 raise\n",
        "\n",
        "\n",
        "# if __name__ == '__main__' and '__file__' in globals():\n",
        "#     LunaTrainingApp().main()\n",
        "\n",
        "model = LunaTrainingApp()\n",
        "model.main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wULp6AYevPtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for another model for segmentation task"
      ],
      "metadata": {
        "id": "Bxivj3vtBFFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data for segmentation\n",
        "\n",
        "# def cleanCache():\n",
        "#     shutil.rmtree('/content/data-unversioned/cache')\n",
        "#     os.mkdir('/content/data-unversioned/cache')\n",
        "\n",
        "# cleanCache()\n",
        "\n",
        "\n",
        "raw_cache = getCache('segmentation_raw')\n",
        "\n",
        "MaskTuple = namedtuple('MaskTuple', 'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask')\n",
        "\n",
        "CandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, hasAnnotation_bool, isMal_bool, diameter_mm, series_uid, center_xyz')\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/annotations_with_malignancy.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "            isMal_bool = {'False': False, 'True': True}[row[5]]\n",
        "\n",
        "            candidateInfo_list.append(\n",
        "                CandidateInfoTuple(\n",
        "                    True,                #is nodule\n",
        "                    True,                #has annotations \n",
        "                    isMal_bool,\n",
        "                    annotationDiameter_mm,\n",
        "                    series_uid,\n",
        "                    annotationCenter_xyz,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            if not isNodule_bool:\n",
        "                candidateInfo_list.append(\n",
        "                    CandidateInfoTuple(\n",
        "                        False,\n",
        "                        False,\n",
        "                        False,\n",
        "                        0.0,\n",
        "                        series_uid,\n",
        "                        candidateCenter_xyz,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoDict(requireOnDisk_bool=True):\n",
        "    candidateInfo_list = getCandidateInfoList(requireOnDisk_bool)\n",
        "    candidateInfo_dict = {}\n",
        "\n",
        "    for candidateInfo_tup in candidateInfo_list:\n",
        "        candidateInfo_dict.setdefault(candidateInfo_tup.series_uid,\n",
        "                                      []).append(candidateInfo_tup)\n",
        "\n",
        "    return candidateInfo_dict \n",
        "    ## for each uid key: this returnes list of tuples [CandidateInfoTuple(isNodule_bool=True, hasAnnotation_bool=True, \n",
        "    ## isMal_bool=True, diameter_mm=32.27003025, \n",
        "    ### series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.287966244644280690737019247886', \n",
        "    ## center_xyz=(67.82725575, 85.37992457, -109.74672379999998)), CandidateInfoTuple(...),......]\n",
        "\n",
        "\n",
        "class Ct:                           \n",
        "    def __init__(self, series_uid : str):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        self.hu_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "        candidateInfo_list = getCandidateInfoDict()[str(self.series_uid)]\n",
        "\n",
        "        self.positiveInfo_list = [\n",
        "            candidate_tup\n",
        "            for candidate_tup in candidateInfo_list\n",
        "            if candidate_tup.isNodule_bool\n",
        "        ]\n",
        "        self.positive_mask = self.buildAnnotationMask(self.positiveInfo_list)\n",
        "        self.positive_indexes = (self.positive_mask.sum(axis=(1,2))\n",
        "                                 .nonzero()[0].tolist())\n",
        "\n",
        "    def buildAnnotationMask(self, positiveInfo_list, threshold_hu = -700):   \n",
        "        boundingBox_a = np.zeros_like(self.hu_a, dtype=bool)  # instead np.bool: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
        "\n",
        "        for candidateInfo_tup in positiveInfo_list:\n",
        "            center_irc = xyz2irc(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                self.origin_xyz,\n",
        "                self.vxSize_xyz,\n",
        "                self.direction_a,\n",
        "            )\n",
        "            ci = int(center_irc.index)\n",
        "            cr = int(center_irc.row)\n",
        "            cc = int(center_irc.col)\n",
        "\n",
        "            index_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci + index_radius, cr, cc] > threshold_hu and \\\n",
        "                        self.hu_a[ci - index_radius, cr, cc] > threshold_hu:\n",
        "                    index_radius += 1\n",
        "            except IndexError:\n",
        "                index_radius -= 1\n",
        "\n",
        "            row_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci, cr + row_radius, cc] > threshold_hu and \\\n",
        "                        self.hu_a[ci, cr - row_radius, cc] > threshold_hu:\n",
        "                    row_radius += 1\n",
        "            except IndexError:\n",
        "                row_radius -= 1\n",
        "\n",
        "            col_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci, cr, cc + col_radius] > threshold_hu and \\\n",
        "                        self.hu_a[ci, cr, cc - col_radius] > threshold_hu:\n",
        "                    col_radius += 1\n",
        "            except IndexError:\n",
        "                col_radius -= 1\n",
        "\n",
        "            # assert index_radius > 0, repr([candidateInfo_tup.center_xyz, center_irc, self.hu_a[ci, cr, cc]])\n",
        "            # assert row_radius > 0\n",
        "            # assert col_radius > 0\n",
        "\n",
        "            boundingBox_a[\n",
        "                 ci - index_radius: ci + index_radius + 1,\n",
        "                 cr - row_radius: cr + row_radius + 1,\n",
        "                 cc - col_radius: cc + col_radius + 1] = True\n",
        "\n",
        "        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n",
        "\n",
        "        return mask_a\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(center_xyz, self.origin_xyz, self.vxSize_xyz,\n",
        "                             self.direction_a)\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(int(start_ndx), int(end_ndx)))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "        pos_chunk = self.positive_mask[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, pos_chunk, center_irc\n",
        "\n",
        "@functools.lru_cache(1, typed=True)    \n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, pos_chunk, center_irc = ct.getRawCandidate(center_xyz,\n",
        "                                                         width_irc)\n",
        "    ct_chunk.clip(-1000, 1000, ct_chunk)\n",
        "    return ct_chunk, pos_chunk, center_irc\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtSampleSize(series_uid):\n",
        "    ct = Ct(series_uid)\n",
        "    return int(ct.hu_a.shape[0]), ct.positive_indexes\n",
        "\n",
        "\n",
        "class Luna2dSegmentationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 contextSlices_count=3,\n",
        "                 fullCt_bool=False,\n",
        "            ):\n",
        "        self.contextSlices_count = int(contextSlices_count)\n",
        "        self.fullCt_bool = fullCt_bool\n",
        "\n",
        "        if series_uid:\n",
        "            self.series_list = [series_uid]\n",
        "        else:\n",
        "            self.series_list = sorted(getCandidateInfoDict().keys())\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.series_list = self.series_list[::val_stride] #e.g. every 10th item (more precisely key for item)\n",
        "            assert self.series_list\n",
        "        elif val_stride > 0:\n",
        "            del self.series_list[::val_stride]\n",
        "            assert self.series_list\n",
        "\n",
        "        self.sample_list = []\n",
        "        for series_uid in self.series_list:\n",
        "            index_count, positive_indexes = getCtSampleSize(series_uid)\n",
        "\n",
        "            if self.fullCt_bool:\n",
        "                self.sample_list += [(series_uid, slice_ndx)\n",
        "                                     for slice_ndx in range(index_count)]\n",
        "            else:\n",
        "                self.sample_list += [(series_uid, slice_ndx)\n",
        "                                     for slice_ndx in positive_indexes]\n",
        "\n",
        "        self.candidateInfo_list = getCandidateInfoList()\n",
        "\n",
        "        series_set = set(self.series_list)\n",
        "        self.candidateInfo_list = [cit for cit in self.candidateInfo_list\n",
        "                                   if cit.series_uid in series_set]\n",
        "\n",
        "        self.pos_list = [nt for nt in self.candidateInfo_list\n",
        "                            if nt.isNodule_bool]\n",
        "\n",
        "        print(\"{!r}: {} {} series, {} slices, {} nodules\".format(\n",
        "            self,\n",
        "            len(self.series_list),\n",
        "            {None: 'general', True: 'validation', False: 'training'}[isValSet_bool],\n",
        "            len(self.sample_list),\n",
        "            len(self.pos_list),\n",
        "        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
        "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
        "\n",
        "    def getitem_fullSlice(self, series_uid, slice_ndx):\n",
        "        ct = getCt(series_uid)\n",
        "        ct_t = torch.zeros((int(self.contextSlices_count) * 2 + 1, 512, 512))\n",
        "\n",
        "        start_ndx = slice_ndx - int(self.contextSlices_count)\n",
        "        end_ndx = slice_ndx + int(self.contextSlices_count) + 1\n",
        "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
        "            context_ndx = max(context_ndx, 0)\n",
        "            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n",
        "            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_t.clamp_(-1000, 1000)\n",
        "\n",
        "        pos_t = torch.from_numpy(ct.positive_mask[slice_ndx]).unsqueeze(0)\n",
        "\n",
        "        return ct_t, pos_t, ct.series_uid, slice_ndx\n",
        "\n",
        "\n",
        "class TrainingLuna2dSegmentationDataset(Luna2dSegmentationDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.ratio_int = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return 300000       ## len(self.candidateInfo_list)   ?\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        random.shuffle(self.candidateInfo_list)\n",
        "        random.shuffle(self.pos_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n",
        "        return self.getitem_trainingCrop(candidateInfo_tup)\n",
        "\n",
        "    def getitem_trainingCrop(self, candidateInfo_tup):\n",
        "        ct_a, pos_a, center_irc = getCtRawCandidate(\n",
        "            candidateInfo_tup.series_uid,\n",
        "            candidateInfo_tup.center_xyz,\n",
        "            (7, 96, 96),\n",
        "        )\n",
        "        pos_a = pos_a[3:4]\n",
        "\n",
        "        row_offset = random.randrange(0,32)\n",
        "        col_offset = random.randrange(0,32)\n",
        "        ct_t = torch.from_numpy(ct_a[:, row_offset:row_offset+64,\n",
        "                                     col_offset:col_offset+64]).to(torch.float32)\n",
        "        pos_t = torch.from_numpy(pos_a[:, row_offset:row_offset+64,\n",
        "                                       col_offset:col_offset+64]).to(torch.long)\n",
        "\n",
        "        slice_ndx = center_irc.index\n",
        "\n",
        "        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\n",
        "\n",
        "class PrepcacheLunaDataset(Dataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.candidateInfo_list = getCandidateInfoList()\n",
        "        self.pos_list = [nt for nt in self.candidateInfo_list if nt.isNodule_bool]\n",
        "\n",
        "        self.seen_set = set()\n",
        "        self.candidateInfo_list.sort(key=lambda x: x.series_uid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        # candidate_t, pos_t, series_uid, center_t = super().__getitem__(ndx)\n",
        "\n",
        "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "        getCtRawCandidate(candidateInfo_tup.series_uid, candidateInfo_tup.center_xyz, (7, 96, 96))\n",
        "\n",
        "        series_uid = candidateInfo_tup.series_uid\n",
        "        if series_uid not in self.seen_set:\n",
        "            self.seen_set.add(series_uid)\n",
        "\n",
        "            getCtSampleSize(series_uid)\n",
        "            # ct = getCt(series_uid)\n",
        "            # for mask_ndx in ct.positive_indexes:\n",
        "            #     build2dLungMask(series_uid, mask_ndx)\n",
        "\n",
        "        return 0, 1 #candidate_t, pos_t, series_uid, center_t\n",
        "\n",
        "\n",
        "# class TvTrainingLuna2dSegmentationDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, isValSet_bool=False, val_stride=10, contextSlices_count=3):\n",
        "#         assert contextSlices_count == 3\n",
        "#         data = torch.load('./imgs_and_masks.pt')\n",
        "#         suids = list(set(data['suids']))\n",
        "#         trn_mask_suids = torch.arange(len(suids)) % val_stride < (val_stride - 1)\n",
        "#         trn_suids = {s for i, s in zip(trn_mask_suids, suids) if i}\n",
        "#         trn_mask = torch.tensor([(s in trn_suids) for s in data[\"suids\"]])\n",
        "#         if not isValSet_bool:\n",
        "#             self.imgs = data[\"imgs\"][trn_mask]\n",
        "#             self.masks = data[\"masks\"][trn_mask]\n",
        "#             self.suids = [s for s, i in zip(data[\"suids\"], trn_mask) if i]\n",
        "#         else:\n",
        "#             self.imgs = data[\"imgs\"][~trn_mask]\n",
        "#             self.masks = data[\"masks\"][~trn_mask]\n",
        "#             self.suids = [s for s, i in zip(data[\"suids\"], trn_mask) if not i]\n",
        "#         # discard spurious hotspots and clamp bone\n",
        "#         self.imgs.clamp_(-1000, 1000)\n",
        "#         self.imgs /= 1000\n",
        "\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.imgs)\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         oh, ow = torch.randint(0, 32, (2,))\n",
        "#         sl = self.masks.size(1)//2\n",
        "#         return self.imgs[i, :, oh: oh + 64, ow: ow + 64], 1, self.masks[i, sl: sl+1, oh: oh + 64, ow: ow + 64].to(torch.float32), self.suids[i], 9999\n",
        "\n",
        "!mkdir ./data-unversioned/segmentation\n",
        "!mkdir ./data-unversioned/segmentation/models\n",
        "!mkdir segmentation_runs"
      ],
      "metadata": {
        "id": "pdmek9QdBk1D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title check the segmentation data\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=True)\n",
        "item = candidateInfo_list[5]\n",
        "\n",
        "#series_list = sorted(set(t.series_uid for t in candidateInfo_list)) ## if u want to access instances turn into list\n",
        "                                                                    ## list( series_list )   \n",
        "def transparent_cmap(cmap, N=255):\n",
        "    \"Copy colormap and set alpha values\"\n",
        "\n",
        "    mycmap = copy.deepcopy(cmap)\n",
        "    mycmap._init()\n",
        "    mycmap._lut[:,-1] = np.linspace(0, 0.75, N+4)\n",
        "    return mycmap\n",
        "\n",
        "tgray = transparent_cmap(plt.cm.gray)\n",
        "tpurp = transparent_cmap(plt.cm.Purples)\n",
        "tblue = transparent_cmap(plt.cm.Blues)\n",
        "togreen = transparent_cmap(plt.cm.Greens)\n",
        "torange = transparent_cmap(plt.cm.Oranges)\n",
        "tred = transparent_cmap(plt.cm.Reds)\n",
        "\n",
        "clim=(0, 1.3)\n",
        "start_ndx = 3\n",
        "mask_model = SegmentationMask().to('cuda')\n",
        "\n",
        "ct = getCt(item.series_uid)\n",
        "center_irc = xyz2irc(item.center_xyz, ct.origin_xyz, ct.vxSize_xyz, ct.direction_a)\n",
        "print(item, 'center_irc', center_irc)\n",
        "\n",
        "mask_tup = build2dLungMask(ct.series_uid, int(center_irc.index))\n",
        "mask_tup = mask_tup._make(x.cpu().numpy()[0][0] for x in mask_tup)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "slice_a = ((ct.hu_a[int(center_irc.index)] / 1000) + 1) / 2\n",
        "slice_a = slice_a.clip(0, 1)\n",
        "\n",
        "subplot = fig.add_subplot(1, 1, 1)\n",
        "subplot.set_title('mal mask', fontsize=30)\n",
        "for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "    label.set_fontsize(20)\n",
        "plt.imshow(\n",
        "    slice_a + 3 * slice_a * mask_tup.pos_mask, \n",
        "    #clim=(-2000, 2000), \n",
        "    cmap='gray',\n",
        ")\n",
        "#plt.imshow(ct.hu_a[int(center_irc.index)] * mask_tup.pos_mask, clim=(-1000,1000), cmap='gray')"
      ],
      "metadata": {
        "id": "P_z40NwlSjwG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title segmentation model\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
        "                 batch_norm=False, up_mode='upconv'):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
        "                                                padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
        "                                            padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path)-1:\n",
        "                blocks.append(x)\n",
        "                x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i-1])\n",
        "\n",
        "        return self.last(x)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
        "                                         stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class UNetWrapper(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])\n",
        "        self.unet = UNet(**kwargs)\n",
        "        self.final = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        init_set = {\n",
        "            nn.Conv2d,\n",
        "            nn.Conv3d,\n",
        "            nn.ConvTranspose2d,\n",
        "            nn.ConvTranspose3d,\n",
        "            nn.Linear,\n",
        "        }\n",
        "        for m in self.modules():\n",
        "            if type(m) in init_set:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "        # nn.init.constant_(self.unet.last.bias, -4)\n",
        "        # nn.init.constant_(self.unet.last.bias, 4)\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.input_batchnorm(input_batch)\n",
        "        un_output = self.unet(bn_output)\n",
        "        fn_output = self.final(un_output)\n",
        "        return fn_output\n",
        "\n",
        "class SegmentationAugmentation(nn.Module):\n",
        "    def __init__(\n",
        "            self, flip=None, offset=None, scale=None, rotate=None, noise=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flip = flip\n",
        "        self.offset = offset\n",
        "        self.scale = scale\n",
        "        self.rotate = rotate\n",
        "        self.noise = noise\n",
        "\n",
        "    def forward(self, input_g, label_g):\n",
        "        transform_t = self._build2dTransformMatrix()\n",
        "        transform_t = transform_t.expand(input_g.shape[0], -1, -1)\n",
        "        transform_t = transform_t.to(input_g.device, torch.float32)\n",
        "        affine_t = F.affine_grid(transform_t[:,:2],\n",
        "                input_g.size(), align_corners=False)\n",
        "\n",
        "        augmented_input_g = F.grid_sample(input_g,\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "        augmented_label_g = F.grid_sample(label_g.to(torch.float32),\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "\n",
        "        if self.noise:\n",
        "            noise_t = torch.randn_like(augmented_input_g)\n",
        "            noise_t *= self.noise\n",
        "\n",
        "            augmented_input_g += noise_t\n",
        "\n",
        "        return augmented_input_g, augmented_label_g > 0.5\n",
        "\n",
        "    def _build2dTransformMatrix(self):\n",
        "        transform_t = torch.eye(3)\n",
        "\n",
        "        for i in range(2):\n",
        "            if self.flip:\n",
        "                if random.random() > 0.5:\n",
        "                    transform_t[i,i] *= -1\n",
        "\n",
        "            if self.offset:\n",
        "                offset_float = self.offset\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[2,i] = offset_float * random_float\n",
        "\n",
        "            if self.scale:\n",
        "                scale_float = self.scale\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "        if self.rotate:\n",
        "            angle_rad = random.random() * math.pi * 2\n",
        "            s = math.sin(angle_rad)\n",
        "            c = math.cos(angle_rad)\n",
        "\n",
        "            rotation_t = torch.tensor([\n",
        "                [c, -s, 0],\n",
        "                [s, c, 0],\n",
        "                [0, 0, 1]])\n",
        "\n",
        "            transform_t @= rotation_t\n",
        "\n",
        "        return transform_t\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZPEJgYvCqLli"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title segmentation model training\n",
        "\n",
        "\n",
        "# Used for computeClassificationLoss and logMetrics to index into metrics_t/metrics_a\n",
        "# METRICS_LABEL_NDX = 0\n",
        "METRICS_LOSS_NDX = 1\n",
        "# METRICS_FN_LOSS_NDX = 2\n",
        "# METRICS_ALL_LOSS_NDX = 3\n",
        "\n",
        "# METRICS_PTP_NDX = 4\n",
        "# METRICS_PFN_NDX = 5\n",
        "# METRICS_MFP_NDX = 6\n",
        "METRICS_TP_NDX = 7\n",
        "METRICS_FN_NDX = 8\n",
        "METRICS_FP_NDX = 9\n",
        "\n",
        "METRICS_SIZE = 10\n",
        "\n",
        "class SegmentationTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=16,\n",
        "                 num_workers : int=0,  ## if you want more workers first check this: https://stackoverflow.com/questions/71713719/runtimeerror-dataloader-worker-pids-15876-2756-exited-unexpectedly\n",
        "                 epochs: int=1,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'segmentation',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.augmented = augmented,\n",
        "        self.augment_flip = augment_flip,\n",
        "        self.augment_offset = augment_offset,\n",
        "        self.augment_scale = augment_scale,\n",
        "        self.augment_rotate = augment_rotate,\n",
        "        self.augment_noise = augment_noise,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "        self.totalTrainingSamples_count = 0\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.03\n",
        "        if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.segmentation_model, self.augmentation_model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        segmentation_model = UNetWrapper(\n",
        "            in_channels=7,\n",
        "            n_classes=1,\n",
        "            depth=3,\n",
        "            wf=4,\n",
        "            padding=True,\n",
        "            batch_norm=True,\n",
        "            up_mode='upconv',\n",
        "        )\n",
        "\n",
        "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
        "\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                segmentation_model = nn.DataParallel(segmentation_model)\n",
        "                augmentation_model = nn.DataParallel(augmentation_model)\n",
        "            segmentation_model = segmentation_model.to(self.device)\n",
        "            augmentation_model = augmentation_model.to(self.device)\n",
        "\n",
        "        return segmentation_model, augmentation_model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return Adam(self.segmentation_model.parameters())\n",
        "        # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)\n",
        "\n",
        "\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = TrainingLuna2dSegmentationDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            contextSlices_count=3,\n",
        "        )\n",
        "\n",
        "        batch_size = int(*self.batch_size)\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,                  \n",
        "            num_workers=int(*self.num_workers),            ## chenged from tupled int back to int\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = Luna2dSegmentationDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "            contextSlices_count=3,\n",
        "        )\n",
        "\n",
        "        batch_size = int(*self.batch_size)\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('segmentation_runs', str(self.tb_prefix[0]), str(self.time_str)) ## corrected here\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '_trn_seg_')\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '_val_seg_')\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        best_score = 0.0\n",
        "        self.validation_cadence = 5\n",
        "        for epoch_ndx in range(1, int(*self.epochs) + 1):\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                int(*self.epochs),\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                int(*self.batch_size),\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n",
        "                # if validation is wanted\n",
        "                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "                best_score = max(score, best_score)\n",
        "\n",
        "                self.saveModel('seg', epoch_ndx, score == best_score)\n",
        "\n",
        "                self.logImages(epoch_ndx, 'trn', train_dl)\n",
        "                self.logImages(epoch_ndx, 'val', val_dl)\n",
        "\n",
        "        self.trn_writer.close()\n",
        "        self.val_writer.close()\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=self.device)\n",
        "        self.segmentation_model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
        "            loss_var.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += trnMetrics_g.size(1)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=self.device)\n",
        "            self.segmentation_model.eval()\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
        "                         classificationThreshold=0.5):\n",
        "        input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        if self.segmentation_model.training and self.augmentation_dict:\n",
        "            input_g, label_g = self.augmentation_model(input_g, label_g)\n",
        "\n",
        "        prediction_g = self.segmentation_model(input_g)\n",
        "\n",
        "        diceLoss_g = self.diceLoss(prediction_g, label_g)\n",
        "        fnLoss_g = self.diceLoss(prediction_g * label_g, label_g)\n",
        "\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + input_t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictionBool_g = (prediction_g[:, 0:1]\n",
        "                                > classificationThreshold).to(torch.float32)\n",
        "\n",
        "            tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
        "            fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
        "            fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
        "\n",
        "            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
        "            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
        "            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
        "            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
        "\n",
        "        return diceLoss_g.mean() + fnLoss_g.mean() * 8\n",
        "\n",
        "    def diceLoss(self, prediction_g, label_g, epsilon=1):\n",
        "        diceLabel_g = label_g.sum(dim=[1,2,3])\n",
        "        dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
        "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n",
        "\n",
        "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
        "            / (dicePrediction_g + diceLabel_g + epsilon)\n",
        "\n",
        "        return 1 - diceRatio_g\n",
        "\n",
        "\n",
        "    def logImages(self, epoch_ndx, mode_str, dl):\n",
        "        self.segmentation_model.eval()\n",
        "\n",
        "        images = sorted(dl.dataset.series_list)[:12]\n",
        "        for series_ndx, series_uid in enumerate(images):\n",
        "            ct = getCt(series_uid)\n",
        "\n",
        "            for slice_ndx in range(6):\n",
        "                ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n",
        "                sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n",
        "\n",
        "                ct_t, label_t, series_uid, ct_ndx = sample_tup\n",
        "\n",
        "                input_g = ct_t.to(self.device).unsqueeze(0)\n",
        "                label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n",
        "\n",
        "                prediction_g = self.segmentation_model(input_g)[0]\n",
        "                prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n",
        "                label_a = label_g.cpu().numpy()[0][0] > 0.5\n",
        "\n",
        "                ct_t[:-1,:,:] /= 2000\n",
        "                ct_t[:-1,:,:] += 0.5\n",
        "\n",
        "                ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n",
        "\n",
        "                image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "                image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "                image_a[:,:,0] += prediction_a & (1 - label_a)\n",
        "                image_a[:,:,0] += (1 - prediction_a) & label_a\n",
        "                image_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5\n",
        "\n",
        "                image_a[:,:,1] += prediction_a & label_a\n",
        "                image_a *= 0.5\n",
        "                image_a.clip(0, 1, image_a)\n",
        "\n",
        "                writer = getattr(self, mode_str + '_writer')\n",
        "                writer.add_image(\n",
        "                    f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n",
        "                    image_a,\n",
        "                    self.totalTrainingSamples_count,\n",
        "                    dataformats='HWC',\n",
        "                )\n",
        "\n",
        "                if epoch_ndx == 1:\n",
        "                    image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "                    image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "                    # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n",
        "                    image_a[:,:,1] += label_a  # Green\n",
        "                    # image_a[:,:,2] += neg_a  # Blue\n",
        "\n",
        "                    image_a *= 0.5\n",
        "                    image_a[image_a < 0] = 0\n",
        "                    image_a[image_a > 1] = 1\n",
        "                    writer.add_image(\n",
        "                        '{}/{}_label_{}'.format(\n",
        "                            mode_str,\n",
        "                            series_ndx,\n",
        "                            slice_ndx,\n",
        "                        ),\n",
        "                        image_a,\n",
        "                        self.totalTrainingSamples_count,\n",
        "                        dataformats='HWC',\n",
        "                    )\n",
        "                # This flush prevents TB from getting confused about which\n",
        "                # data item belongs where.\n",
        "                writer.flush()\n",
        "\n",
        "    def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        metrics_a = metrics_t.detach().numpy()\n",
        "        sum_a = metrics_a.sum(axis=1)\n",
        "        assert np.isfinite(metrics_a).all()\n",
        "\n",
        "        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
        "\n",
        "        metrics_dict['percent_all/tp'] = \\\n",
        "            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fn'] = \\\n",
        "            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fp'] = \\\n",
        "            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
        "\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
        "        recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
        "            / ((precision + recall) or 1)\n",
        "\n",
        "        print((\"E{} {:8} \"\n",
        "                 + \"{loss/all:.4f} loss, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "                  ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "        print((\"E{} {:8} \"\n",
        "                  + \"{loss/all:.4f} loss, \"\n",
        "                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
        "        ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str + '_all',\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "\n",
        "        self.initTensorboardWriters()\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        prefix_str = 'seg_'\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            writer.add_scalar(prefix_str + key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "        score = metrics_dict['pr/recall']\n",
        "\n",
        "        return score\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             writer.add_histogram(\n",
        "    #                 name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                 param.data.cpu().numpy(),\n",
        "    #                 # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                 self.totalTrainingSamples_count,\n",
        "    #                 # bins=bins,\n",
        "    #             )\n",
        "    #\n",
        "    #             # print name, param.data\n",
        "\n",
        "    def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
        "        file_path = os.path.join(\n",
        "            'data-unversioned',\n",
        "            'segmentation',\n",
        "            'models',\n",
        "            str(self.tb_prefix[0]),\n",
        "            '{}_{}.{}.state'.format(\n",
        "                str(type_str),\n",
        "                str(self.time_str),\n",
        "                int(self.totalTrainingSamples_count),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
        "\n",
        "        model = self.segmentation_model\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model = model.module\n",
        "\n",
        "        ##'sys_argv': sys.argv,\n",
        "        state = {\n",
        "            'time': str(datetime.datetime.now()),\n",
        "            'model_state': model.state_dict(),\n",
        "            'model_name': type(model).__name__,\n",
        "            'optimizer_state' : self.optimizer.state_dict(),\n",
        "            'optimizer_name': type(self.optimizer).__name__,\n",
        "            'epoch': epoch_ndx,\n",
        "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
        "        }\n",
        "        torch.save(state, file_path)\n",
        "\n",
        "        print(\"Saved model params to {}\".format(file_path))\n",
        "\n",
        "        if isBest:\n",
        "            best_path = os.path.join(\n",
        "                'data-unversioned', 'segmentation', 'models',\n",
        "                str(self.tb_prefix[0]),\n",
        "                f'{type_str}_{self.time_str}.best.state')\n",
        "            shutil.copyfile(file_path, best_path)\n",
        "\n",
        "            print(\"Saved model params to {}\".format(best_path))\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            print(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n",
        "\n",
        "\n",
        "segmodel = SegmentationTrainingApp()\n",
        "segmodel.main() "
      ],
      "metadata": {
        "cellView": "form",
        "id": "X_jCb5kDq7nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f93f7da-85f8-4268-8cc6-9009243e6ef4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA; 1 devices.\n",
            "Starting SegmentationTrainingApp, <__main__.SegmentationTrainingApp object at 0x7f645708fd90>\n",
            "<__main__.TrainingLuna2dSegmentationDataset object at 0x7f6462b422d0>: 80 training series, 890 slices, 101 nodules\n",
            "<__main__.Luna2dSegmentationDataset object at 0x7f6462d9d110>: 9 validation series, 81 slices, 11 nodules\n",
            "Epoch 1 of 1, 18750/6 batches of size 16*1\n",
            "E1 Training ----/18750, starting\n",
            "E1 Training    4/18750, done at 2022-09-28 11:00:23, 0:07:02\n",
            "E1 Training   32/18750, done at 2022-09-28 11:00:16, 0:06:55\n",
            "E1 Training  256/18750, done at 2022-09-28 10:59:54, 0:06:33\n",
            "E1 Training 2048/18750, done at 2022-09-28 10:59:49, 0:06:28\n",
            "E1 Training 16384/18750, done at 2022-09-28 10:59:49, 0:06:28\n",
            "E1 Training ----/18750, done at 2022-09-28 10:59:49\n",
            "E1 SegmentationTrainingApp\n",
            "E1 trn      0.3591 loss, 0.3657 precision, 0.9414 recall, 0.5267 f1 score\n",
            "E1 trn_all  0.3591 loss,  94.1% tp,   5.9% fn,     163.3% fp\n",
            "E1 Validation  ----/6, starting\n",
            "E1 Validation     4/6, done at 2022-09-28 10:59:52, 0:00:03\n",
            "E1 Validation  ----/6, done at 2022-09-28 10:59:52\n",
            "E1 SegmentationTrainingApp\n",
            "E1 val      0.9380 loss, 0.0200 precision, 0.6671 recall, 0.0389 f1 score\n",
            "E1 val_all  0.9380 loss,  66.7% tp,  33.3% fn,    3265.2% fp\n",
            "Saved model params to data-unversioned/segmentation/models/segmentation/seg_2022-09-28_10.53.20.300000.state\n",
            "Saved model params to data-unversioned/segmentation/models/segmentation/seg_2022-09-28_10.53.20.best.state\n",
            "SHA1: 3a5e3910f01210f0fe05c7889dd16ff50e0290ef\n"
          ]
        }
      ]
    }
  ]
}