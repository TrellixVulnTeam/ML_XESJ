{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "R E F E R E N C E: \n",
        "\n",
        "Deep Learning with PyTorch ELI STEVENS, LUCA ANTIGA, AND THOMAS VIEHMANN\n",
        "\n",
        "Data:\n",
        "\n",
        "https://luna16.grand-challenge.org/Download/"
      ],
      "metadata": {
        "id": "fTbBdUHSCNI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## data download from here: https://luna16.grand-challenge.org/Download/\n",
        "## if __name__ == '__main__' and '__file__' in globals():\n",
        "\n",
        "!wget -O annotations.csv -q https://zenodo.org/record/3723295/files/annotations.csv?download=1\n",
        "!wget -O candidates.csv -q https://zenodo.org/record/3723295/files/candidates.csv?download=1\n",
        "!wget -O subset0.zip -q https://zenodo.org/record/3723295/files/subset0.zip?download=1\n",
        "\n",
        "!brew install p7zip   # https://unix.stackexchange.com/questions/115825/extra-bytes-error-when-unzipping-a-file\n",
        "!7za x subset0.zip    # used instead of !unzip subset0.zip\n",
        "# !rm subset0.zip"
      ],
      "metadata": {
        "id": "StQH3c7OfhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title installs and imports\n",
        "\n",
        "!pip install -q diskcache\n",
        "!pip install -q SimpleITK\n",
        "!pip install -q cassandra-driver\n",
        "\n",
        "# got help from this code:\n",
        "# https://github.com/JonathanSum/pytorch-Deep-Learning_colab/blob/master/p2ch10_explore_data.ipynb\n",
        "from cassandra.cqltypes import BytesType\n",
        "from diskcache import FanoutCache, Disk,core\n",
        "from diskcache.core import io\n",
        "from io import BytesIO\n",
        "from diskcache.core import MODE_BINARY\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from torch import nn as nn\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import SGD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ZQDBEicjZ64B",
        "outputId": "365e2fca-a900-4beb-e36e-14181176c56d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title functional tools\n",
        "\n",
        "\n",
        "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
        "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
        "\n",
        "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
        "    cri_a = np.array(coord_irc)[::-1]\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
        "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
        "    return XyzTuple(*coords_xyz)\n",
        "\n",
        "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coord_a = np.array(coord_xyz)\n",
        "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
        "    cri_a = np.round(cri_a)\n",
        "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n",
        "\n",
        "\n",
        "def importstr(module_str, from_=None):\n",
        "    \"\"\"\n",
        "    >>> importstr('os')\n",
        "    <module 'os' from '.../os.pyc'>\n",
        "    >>> importstr('math', 'fabs')\n",
        "    <built-in function fabs>\n",
        "    \"\"\"\n",
        "    if from_ is None and ':' in module_str:\n",
        "        module_str, from_ = module_str.rsplit(':')\n",
        "\n",
        "    module = __import__(module_str)\n",
        "    for sub_str in module_str.split('.')[1:]:\n",
        "        module = getattr(module, sub_str)\n",
        "\n",
        "    if from_:\n",
        "        try:\n",
        "            return getattr(module, from_)\n",
        "        except:\n",
        "            raise ImportError('{}.{}'.format(module_str, from_))\n",
        "    return module\n",
        "\n",
        "\n",
        "\n",
        "def prhist(ary, prefix_str=None, **kwargs):\n",
        "    if prefix_str is None:\n",
        "        prefix_str = ''\n",
        "    else:\n",
        "        prefix_str += ' '\n",
        "\n",
        "    count_ary, bins_ary = np.histogram(ary, **kwargs)\n",
        "    for i in range(count_ary.shape[0]):\n",
        "        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n",
        "    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enumerateWithEstimate(\n",
        "        iter,\n",
        "        desc_str,\n",
        "        start_ndx=0,\n",
        "        print_ndx=4,\n",
        "        backoff=None,\n",
        "        iter_len=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    In terms of behavior, `enumerateWithEstimate` is almost identical\n",
        "    to the standard `enumerate` (the differences are things like how\n",
        "    our function returns a generator, while `enumerate` returns a\n",
        "    specialized `<enumerate object at 0x...>`).\n",
        "    \"\"\"\n",
        "    if iter_len is None:\n",
        "        iter_len = len(iter)\n",
        "\n",
        "    if backoff is None:\n",
        "        backoff = 2\n",
        "        while backoff ** 7 < iter_len:\n",
        "            backoff *= 2\n",
        "\n",
        "    assert backoff >= 2\n",
        "    while print_ndx < start_ndx * backoff:\n",
        "        print_ndx *= backoff\n",
        "\n",
        "    print(\"{} ----/{}, starting\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "    ))\n",
        "    start_ts = time.time()\n",
        "    for (current_ndx, item) in enumerate(iter):\n",
        "        yield (current_ndx, item)\n",
        "        if current_ndx == print_ndx:\n",
        "            # ... <1>\n",
        "            duration_sec = ((time.time() - start_ts)\n",
        "                            / (current_ndx - start_ndx + 1)\n",
        "                            * (iter_len-start_ndx)\n",
        "                            )\n",
        "\n",
        "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
        "            done_td = datetime.timedelta(seconds=duration_sec)\n",
        "\n",
        "            print(\"{} {:-4}/{}, done at {}, {}\".format(\n",
        "                desc_str,\n",
        "                current_ndx,\n",
        "                iter_len,\n",
        "                str(done_dt).rsplit('.', 1)[0],\n",
        "                str(done_td).rsplit('.', 1)[0],\n",
        "            ))\n",
        "\n",
        "            print_ndx *= backoff\n",
        "\n",
        "        if current_ndx + 1 == start_ndx:\n",
        "            start_ts = time.time()\n",
        "\n",
        "    print(\"{} ----/{}, done at {}\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "        str(datetime.datetime.now()).rsplit('.', 1)[0],\n",
        "    ))\n",
        "\n",
        "########################################################\n",
        "\n",
        "\n",
        "class GzipDisk(Disk):\n",
        "    def store(self, value, read, key=None):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param value: value to convert\n",
        "        :param bool read: True when value is file-like object\n",
        "        :return: (size, mode, filename, value) tuple for Cache table\n",
        "        \"\"\"\n",
        "        # pylint: disable=unidiomatic-typecheck\n",
        "        if type(value) is BytesType:\n",
        "            if read:\n",
        "                value = value.read()\n",
        "                read = False\n",
        "\n",
        "            str_io = BytesIO()\n",
        "            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n",
        "\n",
        "            for offset in range(0, len(value), 2**30):\n",
        "                gz_file.write(value[offset:offset+2**30])\n",
        "            gz_file.close()\n",
        "\n",
        "            value = str_io.getvalue()\n",
        "\n",
        "        return super(GzipDisk, self).store(value, read)\n",
        "\n",
        "\n",
        "    def fetch(self, mode, filename, value, read):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param int mode: value mode raw, binary, text, or pickle\n",
        "        :param str filename: filename of corresponding value\n",
        "        :param value: database value\n",
        "        :param bool read: when True, return an open file handle\n",
        "        :return: corresponding Python value\n",
        "        \"\"\"\n",
        "        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n",
        "\n",
        "        if mode == MODE_BINARY:\n",
        "            str_io = BytesIO(value)\n",
        "            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n",
        "            read_csio = BytesIO()\n",
        "\n",
        "            while True:\n",
        "                uncompressed_data = gz_file.read(2**30)\n",
        "                if uncompressed_data:\n",
        "                    read_csio.write(uncompressed_data)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            value = read_csio.getvalue()\n",
        "\n",
        "        return value\n",
        "\n",
        "def getCache(scope_str):\n",
        "    return FanoutCache('data-unversioned/cache/' + scope_str,\n",
        "                       disk=GzipDisk,\n",
        "                       shards=64,\n",
        "                       timeout=1,\n",
        "                       size_limit=3e11,\n",
        "                       # disk_min_file_size=2**20,\n",
        "                       )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZwKvesFZbJEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualization tools\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "def findPositiveSamples(start_ndx=0, limit=10):\n",
        "    ds = LunaDataset(sortby_str='label_and_size')\n",
        "\n",
        "    positiveSample_list = []\n",
        "    for sample_tup in ds.candidateInfo_list:\n",
        "        if sample_tup.isNodule_bool:\n",
        "            print(len(positiveSample_list), sample_tup)\n",
        "            positiveSample_list.append(sample_tup)\n",
        "\n",
        "        if len(positiveSample_list) >= limit:\n",
        "            break\n",
        "\n",
        "    return positiveSample_list\n",
        "\n",
        "def showCandidate(series_uid, batch_ndx=None, **kwargs):\n",
        "    ds = LunaDataset(series_uid=series_uid, **kwargs)\n",
        "    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n",
        "\n",
        "    if batch_ndx is None:\n",
        "        if pos_list:\n",
        "            batch_ndx = pos_list[0]\n",
        "        else:\n",
        "            print(\"Warning: no positive samples found; using first negative sample.\")\n",
        "            batch_ndx = 0\n",
        "\n",
        "    ct = Ct(series_uid)\n",
        "    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n",
        "    ct_a = ct_t[0].numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 50))\n",
        "\n",
        "    group_list = [\n",
        "        [9, 11, 13],\n",
        "        [15, 16, 17],\n",
        "        [19, 21, 23],\n",
        "    ]\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[int(center_irc.index)], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,int(center_irc.row)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,:,int(center_irc.col)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for row, index_list in enumerate(group_list):\n",
        "        for col, index in enumerate(index_list):\n",
        "            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n",
        "            subplot.set_title('slice {}'.format(index), fontsize=30)\n",
        "            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "                label.set_fontsize(20)\n",
        "            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n",
        "\n",
        "\n",
        "    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)\n",
        "\n",
        "\n",
        "##### for plotting metrics; precision and recall\n",
        "\n",
        "range_a = np.arange(0.01, 1, 0.01)\n",
        "precision_a, recall_a = np.meshgrid(range_a, range_a)\n",
        "\n",
        "f1_score = np.sqrt(2 * precision_a * recall_a / (precision_a + recall_a))\n",
        "\n",
        "def plotScore(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=1, dpi=300, figsize=(7/2, 2.5))\n",
        "\n",
        "    subplts.set_title(title_str + \"(p, r)\")\n",
        "    subplts.contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts.set_xlabel(\"precision\")\n",
        "    subplts.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def plotScores(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=2, dpi=300, figsize=(7, 3.5))\n",
        "\n",
        "    subplts[0].set_title(title_str + \"(p, r)\")\n",
        "    subplts[0].contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts[1].set_title(\"f1(p, r)\")\n",
        "    subplts[1].contourf(f1_score, cmap='gray')\n",
        "\n",
        "    #subplts[2].set_title(\"f1 - \" + title_str)\n",
        "    #subplts[2].contourf(f1_score - other_score, cmap='gray')\n",
        "\n",
        "    for subplt in subplts:\n",
        "        subplt.set_xlabel(\"precision\")\n",
        "        subplt.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# add_score = (precision_a + recall_a) / 2\n",
        "# plotScores(\"avg\", add_score)\n",
        "\n",
        "# min_score = np.min(np.array([precision_a, recall_a]), axis=0)\n",
        "# plotScores(\"min\", min_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# sqrt_score = np.sqrt(precision_a * recall_a)\n",
        "# plotScores(\"sqrt\", sqrt_score)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r_mzaF-ajCvD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for the model used for classification task"
      ],
      "metadata": {
        "id": "h4nFvJsdBS9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "xUBxjBMjYxRH"
      },
      "outputs": [],
      "source": [
        "#@title data for classification\n",
        "\n",
        "\n",
        "raw_cache = getCache('classify_raw')\n",
        "\n",
        "CandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, diameter_mm, series_uid, center_xyz')\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    diameter_dict = {}\n",
        "    with open('/content/annotations.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "\n",
        "            diameter_dict.setdefault(series_uid, []).append(\n",
        "                (annotationCenter_xyz, annotationDiameter_mm),\n",
        "            )\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            candidateDiameter_mm = 0.0\n",
        "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
        "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
        "                for i in range(3):\n",
        "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
        "                    if delta_mm > annotationDiameter_mm / 4:\n",
        "                        break\n",
        "                else:\n",
        "                    candidateDiameter_mm = annotationDiameter_mm\n",
        "                    break\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(\n",
        "                isNodule_bool,\n",
        "                candidateDiameter_mm,\n",
        "                series_uid,\n",
        "                candidateCenter_xyz,\n",
        "            ))\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_a.clip(-1000, 1000, ct_a)\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(\n",
        "            center_xyz,\n",
        "            self.origin_xyz,\n",
        "            self.vxSize_xyz,\n",
        "            self.direction_a,\n",
        "        )\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc\n",
        "\n",
        "def getCtAugmentedCandidate(\n",
        "        augmentation_dict,\n",
        "        series_uid, center_xyz, width_irc,\n",
        "        use_cache=True):\n",
        "    if use_cache:\n",
        "        ct_chunk, center_irc = \\\n",
        "            getCtRawCandidate(series_uid, center_xyz, width_irc)\n",
        "    else:\n",
        "        ct = getCt(series_uid)\n",
        "        ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "\n",
        "    ct_t = torch.tensor(ct_chunk).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "    transform_t = torch.eye(4)\n",
        "    # ... <1>\n",
        "\n",
        "    for i in range(3):\n",
        "        if 'flip' in augmentation_dict:\n",
        "            if random.random() > 0.5:\n",
        "                transform_t[i,i] *= -1\n",
        "\n",
        "        if 'offset' in augmentation_dict:\n",
        "            offset_float = augmentation_dict['offset']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,3] = offset_float * random_float\n",
        "\n",
        "        if 'scale' in augmentation_dict:\n",
        "            scale_float = augmentation_dict['scale']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "\n",
        "    if 'rotate' in augmentation_dict:\n",
        "        angle_rad = random.random() * math.pi * 2\n",
        "        s = math.sin(angle_rad)\n",
        "        c = math.cos(angle_rad)\n",
        "\n",
        "        rotation_t = torch.tensor([\n",
        "            [c, -s, 0, 0],\n",
        "            [s, c, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ])\n",
        "\n",
        "        transform_t @= rotation_t\n",
        "\n",
        "    affine_t = F.affine_grid(\n",
        "            transform_t[:3].unsqueeze(0).to(torch.float32),\n",
        "            ct_t.size(),\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "    augmented_chunk = F.grid_sample(\n",
        "            ct_t,\n",
        "            affine_t,\n",
        "            padding_mode='border',\n",
        "            align_corners=False,\n",
        "        ).to('cpu')\n",
        "\n",
        "    if 'noise' in augmentation_dict:\n",
        "        noise_t = torch.randn_like(augmented_chunk)\n",
        "        noise_t *= augmentation_dict['noise']\n",
        "\n",
        "        augmented_chunk += noise_t\n",
        "\n",
        "    return augmented_chunk[0], center_irc\n",
        "\n",
        "\n",
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 sortby_str='random',\n",
        "                 ratio_int=0,\n",
        "                 augmentation_dict=None,\n",
        "                 candidateInfo_list=None,\n",
        "            ):\n",
        "        self.ratio_int = ratio_int\n",
        "        self.augmentation_dict = augmentation_dict\n",
        "\n",
        "        if candidateInfo_list:\n",
        "            self.candidateInfo_list = copy.copy(candidateInfo_list)\n",
        "            self.use_cache = False\n",
        "        else:\n",
        "            self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "            self.use_cache = True\n",
        "\n",
        "        if series_uid:\n",
        "            self.candidateInfo_list = [\n",
        "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
        "            ]\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "        elif val_stride > 0:\n",
        "            del self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "\n",
        "        if sortby_str == 'random':\n",
        "            random.shuffle(self.candidateInfo_list)\n",
        "        elif sortby_str == 'series_uid':\n",
        "            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n",
        "        elif sortby_str == 'label_and_size':\n",
        "            pass\n",
        "        else:\n",
        "            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n",
        "\n",
        "        self.negative_list = [\n",
        "            nt for nt in self.candidateInfo_list if not nt.isNodule_bool\n",
        "        ]\n",
        "        self.pos_list = [\n",
        "            nt for nt in self.candidateInfo_list if nt.isNodule_bool\n",
        "        ]\n",
        "\n",
        "        print(\"{!r}: {} {} samples, {} neg, {} pos, {} ratio\".format(\n",
        "            self,\n",
        "            len(self.candidateInfo_list),\n",
        "            \"validation\" if isValSet_bool else \"training\",\n",
        "            len(self.negative_list),\n",
        "            len(self.pos_list),\n",
        "            '{}:1'.format(self.ratio_int) if self.ratio_int else 'unbalanced'\n",
        "        ))\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        if self.ratio_int:\n",
        "            random.shuffle(self.negative_list)\n",
        "            random.shuffle(self.pos_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.ratio_int:\n",
        "            return 200000\n",
        "        else:\n",
        "            return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        if self.ratio_int:\n",
        "            pos_ndx = ndx // (self.ratio_int + 1)\n",
        "\n",
        "            if ndx % (self.ratio_int + 1):\n",
        "                neg_ndx = ndx - 1 - pos_ndx\n",
        "                neg_ndx %= len(self.negative_list)\n",
        "                candidateInfo_tup = self.negative_list[neg_ndx]\n",
        "            else:\n",
        "                pos_ndx %= len(self.pos_list)\n",
        "                candidateInfo_tup = self.pos_list[pos_ndx]\n",
        "        else:\n",
        "            candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        if self.augmentation_dict:\n",
        "            candidate_t, center_irc = getCtAugmentedCandidate(\n",
        "                self.augmentation_dict,\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "                self.use_cache,\n",
        "            )\n",
        "        elif self.use_cache:\n",
        "            candidate_a, center_irc = getCtRawCandidate(\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "        else:\n",
        "            ct = getCt(candidateInfo_tup.series_uid)\n",
        "            candidate_a, center_irc = ct.getRawCandidate(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "\n",
        "        pos_t = torch.tensor([\n",
        "                not candidateInfo_tup.isNodule_bool,\n",
        "                candidateInfo_tup.isNodule_bool\n",
        "            ],\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        return candidate_t, pos_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title check if you get data in a correct form or if you are getting it at all\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
        "candidateInfo_list[0]\n",
        "\n",
        "positiveSample_list = findPositiveSamples()\n",
        "\n",
        "augmentation_dict = {}\n",
        "augmentation_list = [\n",
        "    ('None', {}),\n",
        "    ('flip', {'flip': True}),\n",
        "    ('offset', {'offset': 0.1}),\n",
        "    ('scale', {'scale': 0.2}),\n",
        "    ('rotate', {'rotate': True}),\n",
        "    ('noise', {'noise': 25.0}),    \n",
        "]\n",
        "ds_list = [\n",
        "    LunaDataset(sortby_str='label_and_size', augmentation_dict=augmentation_dict) \n",
        "    for title_str, augmentation_dict in augmentation_list\n",
        "]\n",
        "\n",
        "all_dict = {}\n",
        "for title_str, augmentation_dict in augmentation_list:\n",
        "    all_dict.update(augmentation_dict)\n",
        "all_ds = LunaDataset(sortby_str='label_and_size', augmentation_dict=all_dict)\n",
        "\n",
        "augmentation_list.extend([('All', augmentation_dict)] * 3)\n",
        "ds_list.extend([all_ds] * 3)\n",
        "\n",
        "##------------------------------------------------- try this in anew cell\n",
        "sample_ndx = 100\n",
        "sample_ndx = 154\n",
        "sample_ndx = 155\n",
        "\n",
        "sample_tup = all_ds[sample_ndx]\n",
        "print(sample_tup[0].shape, sample_tup[1:])\n",
        "\n",
        "fig = plt.figure(figsize=(30, 30))\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "for i, ((title_str, _), ds) in enumerate(zip(augmentation_list, ds_list)):\n",
        "    sample_tup = ds[sample_ndx]\n",
        "    subplot = fig.add_subplot(3, 3, i+1)\n",
        "    subplot.set_title(title_str, fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(sample_tup[0][0][16], clim=clim, cmap='gray')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6kPyvjfkh_L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification model\n",
        "\n",
        "\n",
        "class LunaModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, conv_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
        "\n",
        "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
        "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
        "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
        "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
        "\n",
        "        self.head_linear = nn.Linear(1152, 2)\n",
        "        self.head_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # see also https://github.com/pytorch/pytorch/issues/18182\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {\n",
        "                nn.Linear,\n",
        "                nn.Conv3d,\n",
        "                nn.Conv2d,\n",
        "                nn.ConvTranspose2d,\n",
        "                nn.ConvTranspose3d,\n",
        "            }:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.tail_batchnorm(input_batch)\n",
        "\n",
        "        block_out = self.block1(bn_output)\n",
        "        block_out = self.block2(block_out)\n",
        "        block_out = self.block3(block_out)\n",
        "        block_out = self.block4(block_out)\n",
        "\n",
        "        conv_flat = block_out.view(\n",
        "            block_out.size(0),\n",
        "            -1,\n",
        "        )\n",
        "        linear_output = self.head_linear(conv_flat)\n",
        "\n",
        "        return linear_output, self.head_softmax(linear_output)\n",
        "\n",
        "\n",
        "class LunaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        block_out = self.conv1(input_batch)\n",
        "        block_out = self.relu1(block_out)\n",
        "        block_out = self.conv2(block_out)\n",
        "        block_out = self.relu2(block_out)\n",
        "\n",
        "        return self.maxpool(block_out)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cM3kIGKBsWJo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification training\n",
        "\n",
        "\n",
        "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_LOSS_NDX=2\n",
        "METRICS_SIZE = 3\n",
        "\n",
        "class LunaTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=32,\n",
        "                 num_workers : int=2,\n",
        "                 epochs: int=1,\n",
        "                 balanced = False,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'classification',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.balanced = balanced,\n",
        "        self.augmented = augmented,\n",
        "        self.augment_flip = augment_flip,\n",
        "        self.augment_offset = augment_offset,\n",
        "        self.augment_scale = augment_scale,\n",
        "        self.augment_rotate = augment_rotate,\n",
        "        self.augment_noise = augment_noise,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.totalTrainingSamples_count = 0\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.1\n",
        "        if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                model = nn.DataParallel(model)\n",
        "            model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        # return Adam(self.model.parameters())\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int= bool(self.balanced),    \n",
        "            augmentation_dict=self.augmentation_dict,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('runs', self.tb_prefix, self.time_str)\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-trn_cls-' + self.comment)\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-val_cls-' + self.comment)\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        for epoch_ndx in range(1, int(*self.epochs) + 1):\n",
        "\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                self.epochs,\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                int(*self.batch_size),\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "\n",
        "        if hasattr(self, 'trn_writer'):\n",
        "            self.trn_writer.close()\n",
        "            self.val_writer.close()\n",
        "\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g,\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            valMetrics_g = torch.zeros(\n",
        "                METRICS_SIZE,\n",
        "                len(val_dl.dataset),\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(\n",
        "                    batch_ndx,\n",
        "                    batch_tup,\n",
        "                    val_dl.batch_size,\n",
        "                    valMetrics_g,\n",
        "                )\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "        input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
        "        loss_g = loss_func(\n",
        "            logits_g,\n",
        "            label_g[:,1],\n",
        "        )\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "        return loss_g.mean()\n",
        "\n",
        "\n",
        "    def logMetrics(\n",
        "            self,\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            metrics_t,\n",
        "            classificationThreshold=0.5,\n",
        "    ):\n",
        "        self.initTensorboardWriters()\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
        "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
        "\n",
        "        posLabel_mask = ~negLabel_mask\n",
        "        posPred_mask = ~negPred_mask\n",
        "\n",
        "        neg_count = int(negLabel_mask.sum())\n",
        "        pos_count = int(posLabel_mask.sum())\n",
        "\n",
        "        trueNeg_count = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
        "        truePos_count = pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
        "\n",
        "        falsePos_count = neg_count - neg_correct\n",
        "        falseNeg_count = pos_count - pos_correct\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
        "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
        "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
        "\n",
        "        metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n",
        "        metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n",
        "        metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falsePos_count)\n",
        "        recall    = metrics_dict['pr/recall'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falseNeg_count)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = \\\n",
        "            2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
        "                 + \"{correct/all:-5.1f}% correct, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
        "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_neg',\n",
        "                neg_correct=neg_correct,\n",
        "                neg_count=neg_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
        "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_pos',\n",
        "                pos_correct=pos_correct,\n",
        "                pos_count=pos_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.add_pr_curve(\n",
        "            'pr',\n",
        "            metrics_t[METRICS_LABEL_NDX],\n",
        "            metrics_t[METRICS_PRED_NDX],\n",
        "            self.totalTrainingSamples_count,\n",
        "        )\n",
        "\n",
        "        bins = [x/50.0 for x in range(51)]\n",
        "\n",
        "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
        "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
        "\n",
        "        if negHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_neg',\n",
        "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "        if posHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_pos',\n",
        "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "\n",
        "        # score = 1 \\\n",
        "        #     + metrics_dict['pr/f1_score'] \\\n",
        "        #     - metrics_dict['loss/mal'] * 0.01 \\\n",
        "        #     - metrics_dict['loss/all'] * 0.0001\n",
        "        #\n",
        "        # return score\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             try:\n",
        "    #                 writer.add_histogram(\n",
        "    #                     name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                     param.data.cpu().numpy(),\n",
        "    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                     self.totalTrainingSamples_count,\n",
        "    #                     # bins=bins,\n",
        "    #                 )\n",
        "    #             except Exception as e:\n",
        "    #                 log.error([min_data, max_data])\n",
        "    #                 raise\n",
        "\n",
        "\n",
        "# if __name__ == '__main__' and '__file__' in globals():\n",
        "#     LunaTrainingApp().main()\n",
        "\n",
        "model = LunaTrainingApp()\n",
        "model.main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wULp6AYevPtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for another model for segmentation task"
      ],
      "metadata": {
        "id": "Bxivj3vtBFFH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdmek9QdBk1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}