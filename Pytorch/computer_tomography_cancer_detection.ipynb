{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h4nFvJsdBS9f",
        "Bxivj3vtBFFH",
        "NFoMFf3ZvS5R"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Status:  some final touches are still needed to make it work end to end, but the main parts are basically implemented"
      ],
      "metadata": {
        "id": "NtlFiwrjfrMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R E F E R E N C E: \n",
        "\n",
        "Deep Learning with PyTorch ELI STEVENS, LUCA ANTIGA, AND THOMAS VIEHMANN\n",
        "\n",
        "Data:\n",
        "\n",
        "https://luna16.grand-challenge.org/Download/"
      ],
      "metadata": {
        "id": "fTbBdUHSCNI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title installs and imports\n",
        "\n",
        "!pip install -q diskcache\n",
        "!pip install -q SimpleITK\n",
        "!pip install -q cassandra-driver\n",
        "\n",
        "# got help from this code:\n",
        "# https://github.com/JonathanSum/pytorch-Deep-Learning_colab/blob/master/p2ch10_explore_data.ipynb\n",
        "from cassandra.cqltypes import BytesType\n",
        "from diskcache import FanoutCache, Disk,core\n",
        "from diskcache.core import io\n",
        "from io import BytesIO\n",
        "from diskcache.core import MODE_BINARY\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import gzip\n",
        "import datetime\n",
        "%matplotlib inline\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch import nn as nn\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import SGD\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import scipy.ndimage.morphology as morphology\n",
        "\n",
        "import torch.cuda\n",
        "import math\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import datetime\n",
        "import hashlib\n",
        "import socket\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import scipy.ndimage.measurements as measurements\n",
        "import scipy.ndimage.morphology as morphology"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZQDBEicjZ64B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data download from here: https://luna16.grand-challenge.org/Download/\n",
        "## if __name__ == '__main__' and '__file__' in globals():\n",
        "\n",
        "!wget -O annotations.csv -q https://zenodo.org/record/3723295/files/annotations.csv?download=1\n",
        "!wget -O candidates.csv -q https://zenodo.org/record/3723295/files/candidates.csv?download=1\n",
        "!wget -O subset0.zip -q https://zenodo.org/record/3723295/files/subset0.zip?download=1\n",
        "\n",
        "!wget -O annotations_with_malignancy.csv -q https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/data/part2/luna/annotations_with_malignancy.csv\n",
        "\n",
        "!brew install p7zip   # https://unix.stackexchange.com/questions/115825/extra-bytes-error-when-unzipping-a-file\n",
        "!7za x subset0.zip    # used instead of !unzip subset0.zip\n",
        "# !rm subset0.zip\n",
        "\n",
        "## when you go to the url https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/data/part2/luna/annotations_with_malignancy.csv\n",
        "## you just click button \"raw\" and copy raw data link which is given below\n",
        "##### the same csv file also available here, if first method does not work: https://1drv.ms/u/s!AhnVhbVlzYkKgQg3z27GtN5OTjWG?e=jRRPAM\n",
        "url = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/part2/luna/annotations_with_malignancy.csv'\n",
        "df = pd.read_csv(url, encoding='unicode_escape')\n",
        "df.set_index('seriesuid').to_csv('annotations_with_malignancy.csv') \n",
        "\n",
        "## do not do this, use cuda, otherwize everything becomes incomprehensibly slow\n",
        "## device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "StQH3c7OfhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title functional tools\n",
        "\n",
        "\n",
        "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
        "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
        "\n",
        "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
        "    cri_a = np.array(coord_irc)[::-1]\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
        "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
        "    return XyzTuple(*coords_xyz)\n",
        "\n",
        "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coord_a = np.array(coord_xyz)\n",
        "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
        "    cri_a = np.round(cri_a)\n",
        "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n",
        "\n",
        "\n",
        "def importstr(module_str, from_=None):\n",
        "    \"\"\"\n",
        "    >>> importstr('os')\n",
        "    <module 'os' from '.../os.pyc'>\n",
        "    >>> importstr('math', 'fabs')\n",
        "    <built-in function fabs>\n",
        "    \"\"\"\n",
        "    if from_ is None and ':' in module_str:\n",
        "        module_str, from_ = module_str.rsplit(':')\n",
        "\n",
        "    module = __import__(module_str)\n",
        "    for sub_str in module_str.split('.')[1:]:\n",
        "        module = getattr(module, sub_str)\n",
        "\n",
        "    if from_:\n",
        "        try:\n",
        "            return getattr(module, from_)\n",
        "        except:\n",
        "            raise ImportError('{}.{}'.format(module_str, from_))\n",
        "    return module\n",
        "\n",
        "\n",
        "\n",
        "def prhist(ary, prefix_str=None, **kwargs):\n",
        "    if prefix_str is None:\n",
        "        prefix_str = ''\n",
        "    else:\n",
        "        prefix_str += ' '\n",
        "\n",
        "    count_ary, bins_ary = np.histogram(ary, **kwargs)\n",
        "    for i in range(count_ary.shape[0]):\n",
        "        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n",
        "    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enumerateWithEstimate(\n",
        "        iter,\n",
        "        desc_str,\n",
        "        start_ndx=0,\n",
        "        print_ndx=4,\n",
        "        backoff=None,\n",
        "        iter_len=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    In terms of behavior, `enumerateWithEstimate` is almost identical\n",
        "    to the standard `enumerate` (the differences are things like how\n",
        "    our function returns a generator, while `enumerate` returns a\n",
        "    specialized `<enumerate object at 0x...>`).\n",
        "    \"\"\"\n",
        "    if iter_len is None:\n",
        "        iter_len = len(iter)\n",
        "\n",
        "    if backoff is None:\n",
        "        backoff = 2\n",
        "        while backoff ** 7 < iter_len:\n",
        "            backoff *= 2\n",
        "\n",
        "    assert backoff >= 2\n",
        "    while print_ndx < start_ndx * backoff:\n",
        "        print_ndx *= backoff\n",
        "\n",
        "    print(\"{} ----/{}, starting\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "    ))\n",
        "    start_ts = time.time()\n",
        "    for (current_ndx, item) in enumerate(iter):\n",
        "        yield (current_ndx, item)\n",
        "        if current_ndx == print_ndx:\n",
        "            # ... <1>\n",
        "            duration_sec = ((time.time() - start_ts)\n",
        "                            / (current_ndx - start_ndx + 1)\n",
        "                            * (iter_len-start_ndx)\n",
        "                            )\n",
        "\n",
        "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
        "            done_td = datetime.timedelta(seconds=duration_sec)\n",
        "\n",
        "            print(\"{} {:-4}/{}, done at {}, {}\".format(\n",
        "                desc_str,\n",
        "                current_ndx,\n",
        "                iter_len,\n",
        "                str(done_dt).rsplit('.', 1)[0],\n",
        "                str(done_td).rsplit('.', 1)[0],\n",
        "            ))\n",
        "\n",
        "            print_ndx *= backoff\n",
        "\n",
        "        if current_ndx + 1 == start_ndx:\n",
        "            start_ts = time.time()\n",
        "\n",
        "    print(\"{} ----/{}, done at {}\".format(\n",
        "        desc_str,\n",
        "        iter_len,\n",
        "        str(datetime.datetime.now()).rsplit('.', 1)[0],\n",
        "    ))\n",
        "\n",
        "########################################################\n",
        "\n",
        "\n",
        "class GzipDisk(Disk):\n",
        "    def store(self, value, read, key=None):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param value: value to convert\n",
        "        :param bool read: True when value is file-like object\n",
        "        :return: (size, mode, filename, value) tuple for Cache table\n",
        "        \"\"\"\n",
        "        # pylint: disable=unidiomatic-typecheck\n",
        "        if type(value) is BytesType:\n",
        "            if read:\n",
        "                value = value.read()\n",
        "                read = False\n",
        "\n",
        "            str_io = BytesIO()\n",
        "            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n",
        "\n",
        "            for offset in range(0, len(value), 2**30):\n",
        "                gz_file.write(value[offset:offset+2**30])\n",
        "            gz_file.close()\n",
        "\n",
        "            value = str_io.getvalue()\n",
        "\n",
        "        return super(GzipDisk, self).store(value, read)\n",
        "\n",
        "\n",
        "    def fetch(self, mode, filename, value, read):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "        :param int mode: value mode raw, binary, text, or pickle\n",
        "        :param str filename: filename of corresponding value\n",
        "        :param value: database value\n",
        "        :param bool read: when True, return an open file handle\n",
        "        :return: corresponding Python value\n",
        "        \"\"\"\n",
        "        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n",
        "\n",
        "        if mode == MODE_BINARY:\n",
        "            str_io = BytesIO(value)\n",
        "            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n",
        "            read_csio = BytesIO()\n",
        "\n",
        "            while True:\n",
        "                uncompressed_data = gz_file.read(2**30)\n",
        "                if uncompressed_data:\n",
        "                    read_csio.write(uncompressed_data)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            value = read_csio.getvalue()\n",
        "\n",
        "        return value\n",
        "\n",
        "def getCache(scope_str):\n",
        "    return FanoutCache('data-unversioned/cache/' + scope_str,\n",
        "                       disk=GzipDisk,\n",
        "                       shards=64,\n",
        "                       timeout=1,\n",
        "                       size_limit=3e11,\n",
        "                       # disk_min_file_size=2**20,\n",
        "                       )\n",
        "    \n",
        "\n",
        "##### ------------------------ Augmentation------------------------------\n",
        "\n",
        "def augment3d(inp):\n",
        "    transform_t = torch.eye(4, dtype=torch.float32)\n",
        "    for i in range(3):\n",
        "        if True: #'flip' in augmentation_dict:\n",
        "            if random.random() > 0.5:\n",
        "                transform_t[i,i] *= -1\n",
        "        if True: #'offset' in augmentation_dict:\n",
        "            offset_float = 0.1\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[3,i] = offset_float * random_float\n",
        "    if True:\n",
        "        angle_rad = random.random() * np.pi * 2\n",
        "        s = np.sin(angle_rad)\n",
        "        c = np.cos(angle_rad)\n",
        "\n",
        "        rotation_t = torch.tensor([\n",
        "            [c, -s, 0, 0],\n",
        "            [s, c, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        transform_t @= rotation_t\n",
        "    #print(inp.shape, transform_t[:3].unsqueeze(0).expand(inp.size(0), -1, -1).shape)\n",
        "    affine_t = torch.nn.functional.affine_grid(\n",
        "            transform_t[:3].unsqueeze(0).expand(inp.size(0), -1, -1).cuda(),\n",
        "            inp.shape,\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "    augmented_chunk = torch.nn.functional.grid_sample(\n",
        "            inp,\n",
        "            affine_t,\n",
        "            padding_mode='border',\n",
        "            align_corners=False,\n",
        "        )\n",
        "    if False: #'noise' in augmentation_dict:\n",
        "        noise_t = torch.randn_like(augmented_chunk)\n",
        "        noise_t *= augmentation_dict['noise']\n",
        "\n",
        "        augmented_chunk += noise_t\n",
        "    return augmented_chunk\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZwKvesFZbJEC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualization tools\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "def findPositiveSamples(start_ndx=0, limit=10):\n",
        "    ds = LunaDataset(sortby_str='label_and_size')\n",
        "\n",
        "    positiveSample_list = []\n",
        "    for sample_tup in ds.candidateInfo_list:\n",
        "        if sample_tup.isNodule_bool:\n",
        "            print(len(positiveSample_list), sample_tup)\n",
        "            positiveSample_list.append(sample_tup)\n",
        "\n",
        "        if len(positiveSample_list) >= limit:\n",
        "            break\n",
        "\n",
        "    return positiveSample_list\n",
        "\n",
        "def showCandidate(series_uid, batch_ndx=None, **kwargs):\n",
        "    ds = LunaDataset(series_uid=series_uid, **kwargs)\n",
        "    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n",
        "\n",
        "    if batch_ndx is None:\n",
        "        if pos_list:\n",
        "            batch_ndx = pos_list[0]\n",
        "        else:\n",
        "            print(\"Warning: no positive samples found; using first negative sample.\")\n",
        "            batch_ndx = 0\n",
        "\n",
        "    ct = Ct(series_uid)\n",
        "    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n",
        "    ct_a = ct_t[0].numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 50))\n",
        "\n",
        "    group_list = [\n",
        "        [9, 11, 13],\n",
        "        [15, 16, 17],\n",
        "        [19, 21, 23],\n",
        "    ]\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[int(center_irc.index)], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,int(center_irc.row)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,:,int(center_irc.col)], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n",
        "    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n",
        "    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n",
        "    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for row, index_list in enumerate(group_list):\n",
        "        for col, index in enumerate(index_list):\n",
        "            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n",
        "            subplot.set_title('slice {}'.format(index), fontsize=30)\n",
        "            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "                label.set_fontsize(20)\n",
        "            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n",
        "\n",
        "\n",
        "    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)\n",
        "\n",
        "\n",
        "#####-------------------------------   Segmentation tasks\n",
        "\n",
        "MaskTuple = namedtuple('MaskTuple', 'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask')\n",
        "\n",
        "class SegmentationMask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_list = nn.ModuleList([\n",
        "            self._make_circle_conv(radius) for radius in range(1, 8)\n",
        "        ])\n",
        "\n",
        "    def _make_circle_conv(self, radius):\n",
        "        diameter = 1 + radius * 2\n",
        "\n",
        "        a = torch.linspace(-1, 1, steps=diameter)**2\n",
        "        b = (a[None] + a[:, None])**0.5\n",
        "\n",
        "        circle_weights = (b <= 1.0).to(torch.float32)\n",
        "\n",
        "        conv = nn.Conv2d(1, 1, kernel_size=diameter, padding=radius, bias=False)\n",
        "        conv.weight.data.fill_(1)\n",
        "        conv.weight.data *= circle_weights / circle_weights.sum()\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "    def erode(self, input_mask, radius, threshold=1):\n",
        "        conv = self.conv_list[radius - 1]\n",
        "        input_float = input_mask.to(torch.float32)\n",
        "        result = conv(input_float)\n",
        "\n",
        "        # log.debug(['erode in ', radius, threshold, input_float.min().item(), input_float.mean().item(), input_float.max().item()])\n",
        "        # log.debug(['erode out', radius, threshold, result.min().item(), result.mean().item(), result.max().item()])\n",
        "\n",
        "        return result >= threshold\n",
        "\n",
        "    def deposit(self, input_mask, radius, threshold=0):\n",
        "        conv = self.conv_list[radius - 1]\n",
        "        input_float = input_mask.to(torch.float32)\n",
        "        result = conv(input_float)\n",
        "\n",
        "        # log.debug(['deposit in ', radius, threshold, input_float.min().item(), input_float.mean().item(), input_float.max().item()])\n",
        "        # log.debug(['deposit out', radius, threshold, result.min().item(), result.mean().item(), result.max().item()])\n",
        "\n",
        "        return result > threshold\n",
        "\n",
        "    def fill_cavity(self, input_mask):\n",
        "        cumsum = input_mask.cumsum(-1)\n",
        "        filled_mask = (cumsum > 0)\n",
        "        filled_mask &= (cumsum < cumsum[..., -1:])\n",
        "        cumsum = input_mask.cumsum(-2)\n",
        "        filled_mask &= (cumsum > 0)\n",
        "        filled_mask &= (cumsum < cumsum[..., -1:, :])\n",
        "\n",
        "        return filled_mask\n",
        "\n",
        "\n",
        "    def forward(self, input_g, raw_pos_g):\n",
        "        gcc_g = input_g + 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # log.info(['gcc_g', gcc_g.min(), gcc_g.mean(), gcc_g.max()])\n",
        "\n",
        "            raw_dense_mask = gcc_g > 0.7\n",
        "            dense_mask = self.deposit(raw_dense_mask, 2)\n",
        "            dense_mask = self.erode(dense_mask, 6)\n",
        "            dense_mask = self.deposit(dense_mask, 4)\n",
        "\n",
        "            body_mask = self.fill_cavity(dense_mask)\n",
        "            air_mask = self.deposit(body_mask & ~dense_mask, 5)\n",
        "            air_mask = self.erode(air_mask, 6)\n",
        "\n",
        "            lung_mask = self.deposit(air_mask, 5)\n",
        "\n",
        "            raw_candidate_mask = gcc_g > 0.4\n",
        "            raw_candidate_mask &= air_mask\n",
        "            candidate_mask = self.erode(raw_candidate_mask, 1)\n",
        "            candidate_mask = self.deposit(candidate_mask, 1)\n",
        "\n",
        "            pos_mask = self.deposit((raw_pos_g > 0.5) & lung_mask, 2)\n",
        "\n",
        "            neg_mask = self.deposit(candidate_mask, 1)\n",
        "            neg_mask &= ~pos_mask\n",
        "            neg_mask &= lung_mask\n",
        "\n",
        "            # label_g = (neg_mask | pos_mask).to(torch.float32)\n",
        "            label_g = (pos_mask).to(torch.float32)\n",
        "            neg_g = neg_mask.to(torch.float32)\n",
        "            pos_g = pos_mask.to(torch.float32)\n",
        "\n",
        "        mask_dict = {\n",
        "            'raw_dense_mask': raw_dense_mask,\n",
        "            'dense_mask': dense_mask,\n",
        "            'body_mask': body_mask,\n",
        "            'air_mask': air_mask,\n",
        "            'raw_candidate_mask': raw_candidate_mask,\n",
        "            'candidate_mask': candidate_mask,\n",
        "            'lung_mask': lung_mask,\n",
        "            'neg_mask': neg_mask,\n",
        "            'pos_mask': pos_mask,\n",
        "        }\n",
        "\n",
        "        return label_g, neg_g, pos_g, lung_mask, mask_dict\n",
        "\n",
        "def build2dLungMask(series_uid, center_ndx):\n",
        "    mask_model = SegmentationMask().to('cuda')\n",
        "    ct = Ct(series_uid)\n",
        "\n",
        "    ct_g = torch.from_numpy(ct.hu_a[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n",
        "    pos_g = torch.from_numpy(ct.positive_mask[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n",
        "    input_g = ct_g / 1000\n",
        "\n",
        "    label_g, neg_g, pos_g, lung_mask, mask_dict = mask_model(input_g, pos_g)\n",
        "    mask_tup = MaskTuple(**mask_dict)\n",
        "\n",
        "    return mask_tup\n",
        "\n",
        "#####------------------------------- for plotting metrics; precision and recall\n",
        "\n",
        "range_a = np.arange(0.01, 1, 0.01)\n",
        "precision_a, recall_a = np.meshgrid(range_a, range_a)\n",
        "\n",
        "f1_score = np.sqrt(2 * precision_a * recall_a / (precision_a + recall_a))\n",
        "\n",
        "def plotScore(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=1, dpi=300, figsize=(7/2, 2.5))\n",
        "\n",
        "    subplts.set_title(title_str + \"(p, r)\")\n",
        "    subplts.contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts.set_xlabel(\"precision\")\n",
        "    subplts.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def plotScores(title_str, other_score):\n",
        "    fig, subplts = plt.subplots(nrows=1, ncols=2, dpi=300, figsize=(7, 3.5))\n",
        "\n",
        "    subplts[0].set_title(title_str + \"(p, r)\")\n",
        "    subplts[0].contourf(other_score, cmap='gray')\n",
        "\n",
        "    subplts[1].set_title(\"f1(p, r)\")\n",
        "    subplts[1].contourf(f1_score, cmap='gray')\n",
        "\n",
        "    #subplts[2].set_title(\"f1 - \" + title_str)\n",
        "    #subplts[2].contourf(f1_score - other_score, cmap='gray')\n",
        "\n",
        "    for subplt in subplts:\n",
        "        subplt.set_xlabel(\"precision\")\n",
        "        subplt.set_ylabel(\"recall\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# add_score = (precision_a + recall_a) / 2\n",
        "# plotScores(\"avg\", add_score)\n",
        "\n",
        "# min_score = np.min(np.array([precision_a, recall_a]), axis=0)\n",
        "# plotScores(\"min\", min_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# mult_score = precision_a * recall_a\n",
        "# plotScores(\"mult\", mult_score)\n",
        "\n",
        "# sqrt_score = np.sqrt(precision_a * recall_a)\n",
        "# plotScores(\"sqrt\", sqrt_score)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r_mzaF-ajCvD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for the model used for classification task\n",
        "\n",
        "classifies whether candidate nodules are actually nodules or not (both benign or malignant)"
      ],
      "metadata": {
        "id": "h4nFvJsdBS9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "xUBxjBMjYxRH"
      },
      "outputs": [],
      "source": [
        "#@title data for classification\n",
        "\n",
        "\n",
        "raw_cache = getCache('classify_raw')\n",
        "\n",
        "CandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, diameter_mm, series_uid, center_xyz')\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    diameter_dict = {}\n",
        "    with open('/content/annotations.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "\n",
        "            diameter_dict.setdefault(series_uid, []).append(\n",
        "                (annotationCenter_xyz, annotationDiameter_mm),\n",
        "            )\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            candidateDiameter_mm = 0.0\n",
        "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
        "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
        "                for i in range(3):\n",
        "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
        "                    if delta_mm > annotationDiameter_mm / 4:\n",
        "                        break\n",
        "                else:\n",
        "                    candidateDiameter_mm = annotationDiameter_mm\n",
        "                    break\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(\n",
        "                isNodule_bool,\n",
        "                candidateDiameter_mm,\n",
        "                series_uid,\n",
        "                candidateCenter_xyz,\n",
        "            ))\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_a.clip(-1000, 1000, ct_a)\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(\n",
        "            center_xyz,\n",
        "            self.origin_xyz,\n",
        "            self.vxSize_xyz,\n",
        "            self.direction_a,\n",
        "        )\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc\n",
        "\n",
        "def getCtAugmentedCandidate(\n",
        "        augmentation_dict,\n",
        "        series_uid, center_xyz, width_irc,\n",
        "        use_cache=True):\n",
        "    if use_cache:\n",
        "        ct_chunk, center_irc = \\\n",
        "            getCtRawCandidate(series_uid, center_xyz, width_irc)\n",
        "    else:\n",
        "        ct = getCt(series_uid)\n",
        "        ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "\n",
        "    ct_t = torch.tensor(ct_chunk).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "    transform_t = torch.eye(4)\n",
        "    # ... <1>\n",
        "\n",
        "    for i in range(3):\n",
        "        if 'flip' in augmentation_dict:\n",
        "            if random.random() > 0.5:\n",
        "                transform_t[i,i] *= -1\n",
        "\n",
        "        if 'offset' in augmentation_dict:\n",
        "            offset_float = augmentation_dict['offset']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,3] = offset_float * random_float\n",
        "\n",
        "        if 'scale' in augmentation_dict:\n",
        "            scale_float = augmentation_dict['scale']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "\n",
        "    if 'rotate' in augmentation_dict:\n",
        "        angle_rad = random.random() * math.pi * 2\n",
        "        s = math.sin(angle_rad)\n",
        "        c = math.cos(angle_rad)\n",
        "\n",
        "        rotation_t = torch.tensor([\n",
        "            [c, -s, 0, 0],\n",
        "            [s, c, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ])\n",
        "\n",
        "        transform_t @= rotation_t\n",
        "\n",
        "    affine_t = F.affine_grid(\n",
        "            transform_t[:3].unsqueeze(0).to(torch.float32),\n",
        "            ct_t.size(),\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "    augmented_chunk = F.grid_sample(\n",
        "            ct_t,\n",
        "            affine_t,\n",
        "            padding_mode='border',\n",
        "            align_corners=False,\n",
        "        ).to('cpu')\n",
        "\n",
        "    if 'noise' in augmentation_dict:\n",
        "        noise_t = torch.randn_like(augmented_chunk)\n",
        "        noise_t *= augmentation_dict['noise']\n",
        "\n",
        "        augmented_chunk += noise_t\n",
        "\n",
        "    return augmented_chunk[0], center_irc\n",
        "\n",
        "\n",
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 sortby_str='random',\n",
        "                 ratio_int=0,\n",
        "                 augmentation_dict=None,\n",
        "                 candidateInfo_list=None,\n",
        "            ):\n",
        "        self.ratio_int = ratio_int\n",
        "        self.augmentation_dict = augmentation_dict\n",
        "\n",
        "        if candidateInfo_list:\n",
        "            self.candidateInfo_list = copy.copy(candidateInfo_list)\n",
        "            self.use_cache = False\n",
        "        else:\n",
        "            self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "            self.use_cache = True\n",
        "\n",
        "        if series_uid:\n",
        "            self.candidateInfo_list = [\n",
        "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
        "            ]\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "        elif val_stride > 0:\n",
        "            del self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "\n",
        "        if sortby_str == 'random':\n",
        "            random.shuffle(self.candidateInfo_list)\n",
        "        elif sortby_str == 'series_uid':\n",
        "            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n",
        "        elif sortby_str == 'label_and_size':\n",
        "            pass\n",
        "        else:\n",
        "            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n",
        "\n",
        "        self.negative_list = [\n",
        "            nt for nt in self.candidateInfo_list if not nt.isNodule_bool\n",
        "        ]\n",
        "        self.pos_list = [\n",
        "            nt for nt in self.candidateInfo_list if nt.isNodule_bool\n",
        "        ]\n",
        "\n",
        "        print(\"{!r}: {} {} samples, {} neg, {} pos, {} ratio\".format(\n",
        "            self,\n",
        "            len(self.candidateInfo_list),\n",
        "            \"validation\" if isValSet_bool else \"training\",\n",
        "            len(self.negative_list),\n",
        "            len(self.pos_list),\n",
        "            '{}:1'.format(self.ratio_int) if self.ratio_int else 'unbalanced'\n",
        "        ))\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        if self.ratio_int:\n",
        "            random.shuffle(self.negative_list)\n",
        "            random.shuffle(self.pos_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.ratio_int:\n",
        "            return 200000\n",
        "        else:\n",
        "            return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        if self.ratio_int:\n",
        "            pos_ndx = ndx // (self.ratio_int + 1)\n",
        "\n",
        "            if ndx % (self.ratio_int + 1):\n",
        "                neg_ndx = ndx - 1 - pos_ndx\n",
        "                neg_ndx %= len(self.negative_list)\n",
        "                candidateInfo_tup = self.negative_list[neg_ndx]\n",
        "            else:\n",
        "                pos_ndx %= len(self.pos_list)\n",
        "                candidateInfo_tup = self.pos_list[pos_ndx]\n",
        "        else:\n",
        "            candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        if self.augmentation_dict:\n",
        "            candidate_t, center_irc = getCtAugmentedCandidate(\n",
        "                self.augmentation_dict,\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "                self.use_cache,\n",
        "            )\n",
        "        elif self.use_cache:\n",
        "            candidate_a, center_irc = getCtRawCandidate(\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "        else:\n",
        "            ct = getCt(candidateInfo_tup.series_uid)\n",
        "            candidate_a, center_irc = ct.getRawCandidate(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "\n",
        "        pos_t = torch.tensor([\n",
        "                not candidateInfo_tup.isNodule_bool,\n",
        "                candidateInfo_tup.isNodule_bool\n",
        "            ],\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        return candidate_t, pos_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)\n",
        "\n",
        "\n",
        "!mkdir ./data-unversioned/classification\n",
        "!mkdir ./data-unversioned/classification/models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title check if you get data in a correct form or if you are getting it at all\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
        "candidateInfo_list[0]\n",
        "\n",
        "positiveSample_list = findPositiveSamples()\n",
        "\n",
        "augmentation_dict = {}\n",
        "augmentation_list = [\n",
        "    ('None', {}),\n",
        "    ('flip', {'flip': True}),\n",
        "    ('offset', {'offset': 0.1}),\n",
        "    ('scale', {'scale': 0.2}),\n",
        "    ('rotate', {'rotate': True}),\n",
        "    ('noise', {'noise': 25.0}),    \n",
        "]\n",
        "ds_list = [\n",
        "    LunaDataset(sortby_str='label_and_size', augmentation_dict=augmentation_dict) \n",
        "    for title_str, augmentation_dict in augmentation_list\n",
        "]\n",
        "\n",
        "all_dict = {}\n",
        "for title_str, augmentation_dict in augmentation_list:\n",
        "    all_dict.update(augmentation_dict)\n",
        "all_ds = LunaDataset(sortby_str='label_and_size', augmentation_dict=all_dict)\n",
        "\n",
        "augmentation_list.extend([('All', augmentation_dict)] * 3)\n",
        "ds_list.extend([all_ds] * 3)\n",
        "\n",
        "##------------------------------------------------- try this in anew cell\n",
        "sample_ndx = 100\n",
        "sample_ndx = 154\n",
        "sample_ndx = 155\n",
        "\n",
        "sample_tup = all_ds[sample_ndx]\n",
        "print(sample_tup[0].shape, sample_tup[1:])\n",
        "\n",
        "fig = plt.figure(figsize=(30, 30))\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "for i, ((title_str, _), ds) in enumerate(zip(augmentation_list, ds_list)):\n",
        "    sample_tup = ds[sample_ndx]\n",
        "    subplot = fig.add_subplot(3, 3, i+1)\n",
        "    subplot.set_title(title_str, fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(sample_tup[0][0][16], clim=clim, cmap='gray')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6kPyvjfkh_L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification model\n",
        "\n",
        "\n",
        "class LunaModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, conv_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
        "\n",
        "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
        "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
        "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
        "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
        "\n",
        "        self.head_linear = nn.Linear(1152, 2)\n",
        "        self.head_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # see also https://github.com/pytorch/pytorch/issues/18182\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {\n",
        "                nn.Linear,\n",
        "                nn.Conv3d,\n",
        "                nn.Conv2d,\n",
        "                nn.ConvTranspose2d,\n",
        "                nn.ConvTranspose3d,\n",
        "            }:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.tail_batchnorm(input_batch)\n",
        "\n",
        "        block_out = self.block1(bn_output)\n",
        "        block_out = self.block2(block_out)\n",
        "        block_out = self.block3(block_out)\n",
        "        block_out = self.block4(block_out)\n",
        "\n",
        "        conv_flat = block_out.view(\n",
        "            block_out.size(0),\n",
        "            -1,\n",
        "        )\n",
        "        linear_output = self.head_linear(conv_flat)\n",
        "\n",
        "        return linear_output, self.head_softmax(linear_output)\n",
        "\n",
        "\n",
        "class LunaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        block_out = self.conv1(input_batch)\n",
        "        block_out = self.relu1(block_out)\n",
        "        block_out = self.conv2(block_out)\n",
        "        block_out = self.relu2(block_out)\n",
        "\n",
        "        return self.maxpool(block_out)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cM3kIGKBsWJo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title classification training \n",
        "\n",
        "\n",
        "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_LOSS_NDX=2\n",
        "METRICS_SIZE = 3\n",
        "\n",
        "class LunaTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=32,\n",
        "                 num_workers : int=2,\n",
        "                 epochs: int=1,\n",
        "                 balanced = False,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'classification',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.balanced = balanced,\n",
        "        self.augmented = augmented,\n",
        "        self.augment_flip = augment_flip,\n",
        "        self.augment_offset = augment_offset,\n",
        "        self.augment_scale = augment_scale,\n",
        "        self.augment_rotate = augment_rotate,\n",
        "        self.augment_noise = augment_noise,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.totalTrainingSamples_count = 0\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.1\n",
        "        if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                model = nn.DataParallel(model)\n",
        "            model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        # return Adam(self.model.parameters())\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int= bool(self.balanced),    \n",
        "            augmentation_dict=self.augmentation_dict,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=int(*batch_size),\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('runs', self.tb_prefix, self.time_str)\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-trn_cls-' + self.comment)\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-val_cls-' + self.comment)\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        best_score = 0.0\n",
        "\n",
        "        for epoch_ndx in range(1, int(*self.epochs) + 1):\n",
        "\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                int(self.epochs[0]),\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                int(*self.batch_size),\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "            score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "\n",
        "            if epoch_ndx == 1 or epoch_ndx % 5 == 0:\n",
        "                # if validation is wanted\n",
        "                # valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "                # score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "                best_score = max(score, best_score)\n",
        "\n",
        "                self.saveModel('cls', epoch_ndx, score == best_score)\n",
        "\n",
        "\n",
        "        if hasattr(self, 'trn_writer'):\n",
        "            self.trn_writer.close()\n",
        "            self.val_writer.close()\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g,\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            valMetrics_g = torch.zeros(\n",
        "                METRICS_SIZE,\n",
        "                len(val_dl.dataset),\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(\n",
        "                    batch_ndx,\n",
        "                    batch_tup,\n",
        "                    val_dl.batch_size,\n",
        "                    valMetrics_g,\n",
        "                )\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "        input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
        "        loss_g = loss_func(\n",
        "            logits_g,\n",
        "            label_g[:,1],\n",
        "        )\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "        return loss_g.mean()\n",
        "\n",
        "\n",
        "    def logMetrics(\n",
        "            self,\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            metrics_t,\n",
        "            classificationThreshold=0.5,\n",
        "    ):\n",
        "        self.initTensorboardWriters()\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
        "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
        "\n",
        "        posLabel_mask = ~negLabel_mask\n",
        "        posPred_mask = ~negPred_mask\n",
        "\n",
        "        neg_count = int(negLabel_mask.sum())\n",
        "        pos_count = int(posLabel_mask.sum())\n",
        "\n",
        "        trueNeg_count = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
        "        truePos_count = pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
        "\n",
        "        falsePos_count = neg_count - neg_correct\n",
        "        falseNeg_count = pos_count - pos_correct\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
        "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
        "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
        "\n",
        "        metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n",
        "        metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n",
        "        metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falsePos_count)\n",
        "        recall    = metrics_dict['pr/recall'] = \\\n",
        "            truePos_count / np.float32(truePos_count + falseNeg_count)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = \\\n",
        "            2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
        "                 + \"{correct/all:-5.1f}% correct, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
        "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_neg',\n",
        "                neg_correct=neg_correct,\n",
        "                neg_count=neg_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
        "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_pos',\n",
        "                pos_correct=pos_correct,\n",
        "                pos_count=pos_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.add_pr_curve(\n",
        "            'pr',\n",
        "            metrics_t[METRICS_LABEL_NDX],\n",
        "            metrics_t[METRICS_PRED_NDX],\n",
        "            self.totalTrainingSamples_count,\n",
        "        )\n",
        "\n",
        "        bins = [x/50.0 for x in range(51)]\n",
        "\n",
        "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
        "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
        "\n",
        "        if negHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_neg',\n",
        "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "        if posHist_mask.any():\n",
        "            writer.add_histogram(\n",
        "                'is_pos',\n",
        "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
        "                self.totalTrainingSamples_count,\n",
        "                bins=bins,\n",
        "            )\n",
        "\n",
        "        # score = 1 \\\n",
        "        #     + metrics_dict['pr/f1_score'] \\\n",
        "        #     - metrics_dict['loss/mal'] * 0.01 \\\n",
        "        #     - metrics_dict['loss/all'] * 0.0001\n",
        "        #\n",
        "        # return score\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             try:\n",
        "    #                 writer.add_histogram(\n",
        "    #                     name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                     param.data.cpu().numpy(),\n",
        "    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                     self.totalTrainingSamples_count,\n",
        "    #                     # bins=bins,\n",
        "    #                 )\n",
        "    #             except Exception as e:\n",
        "    #                 log.error([min_data, max_data])\n",
        "    #                 raise\n",
        "    \n",
        "    \n",
        "    def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
        "        file_path = os.path.join(\n",
        "            'data-unversioned',\n",
        "            'classification',\n",
        "            'models',\n",
        "            str(self.tb_prefix[0]),\n",
        "            '{}_{}.{}.state'.format(\n",
        "                str(type_str),\n",
        "                str(self.time_str),\n",
        "                int(self.totalTrainingSamples_count),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
        "\n",
        "        model = self.model\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model = model.module\n",
        "\n",
        "        state = {\n",
        "            'time': str(datetime.datetime.now()),\n",
        "            'model_state': model.state_dict(),\n",
        "            'model_name': type(model).__name__,\n",
        "            'optimizer_state' : self.optimizer.state_dict(),\n",
        "            'optimizer_name': type(self.optimizer).__name__,\n",
        "            'epoch': epoch_ndx,\n",
        "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
        "        }\n",
        "        torch.save(state, file_path)\n",
        "\n",
        "        print(\"Saved model params to {}\".format(file_path))\n",
        "\n",
        "        if isBest:\n",
        "            best_path = os.path.join(\n",
        "                'data-unversioned', 'classification', 'models',\n",
        "                str(self.tb_prefix[0]),\n",
        "                f'{type_str}_{self.time_str}.best.state')\n",
        "            shutil.copyfile(file_path, best_path)\n",
        "\n",
        "            print(\"Saved model params to {}\".format(best_path))\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            print(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n",
        "\n",
        "\n",
        "model = LunaTrainingApp()\n",
        "model.main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wULp6AYevPtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts the code for another model for segmentation task\n",
        "\n",
        "gives nodule candidates (some may be benign, some malignant, some may not be nodules at all)"
      ],
      "metadata": {
        "id": "Bxivj3vtBFFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data for segmentation\n",
        "\n",
        "# def cleanCache():\n",
        "#     shutil.rmtree('/content/data-unversioned/cache')\n",
        "#     os.mkdir('/content/data-unversioned/cache')\n",
        "\n",
        "# cleanCache()\n",
        "\n",
        "\n",
        "raw_cache = getCache('segmentation_raw')\n",
        "\n",
        "MaskTuple = namedtuple('MaskTuple', 'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask')\n",
        "\n",
        "CandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, hasAnnotation_bool, isMal_bool, diameter_mm, series_uid, center_xyz')\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/annotations_with_malignancy.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "            isMal_bool = {'False': False, 'True': True}[row[5]]\n",
        "\n",
        "            candidateInfo_list.append(\n",
        "                CandidateInfoTuple(\n",
        "                    True,                #is nodule\n",
        "                    True,                #has annotations \n",
        "                    isMal_bool,\n",
        "                    annotationDiameter_mm,\n",
        "                    series_uid,\n",
        "                    annotationCenter_xyz,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            if not isNodule_bool:\n",
        "                candidateInfo_list.append(\n",
        "                    CandidateInfoTuple(\n",
        "                        False,\n",
        "                        False,\n",
        "                        False,\n",
        "                        0.0,\n",
        "                        series_uid,\n",
        "                        candidateCenter_xyz,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoDict(requireOnDisk_bool=True):\n",
        "    candidateInfo_list = getCandidateInfoList(requireOnDisk_bool)\n",
        "    candidateInfo_dict = {}\n",
        "\n",
        "    for candidateInfo_tup in candidateInfo_list:\n",
        "        candidateInfo_dict.setdefault(candidateInfo_tup.series_uid,\n",
        "                                      []).append(candidateInfo_tup)\n",
        "\n",
        "    return candidateInfo_dict \n",
        "    ## for each uid key: this returnes list of tuples [CandidateInfoTuple(isNodule_bool=True, hasAnnotation_bool=True, \n",
        "    ## isMal_bool=True, diameter_mm=32.27003025, \n",
        "    ### series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.287966244644280690737019247886', \n",
        "    ## center_xyz=(67.82725575, 85.37992457, -109.74672379999998)), CandidateInfoTuple(...),......]\n",
        "\n",
        "\n",
        "class Ct:                           \n",
        "    def __init__(self, series_uid : str):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        self.hu_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "        candidateInfo_list = getCandidateInfoDict()[str(self.series_uid)]\n",
        "\n",
        "        self.positiveInfo_list = [\n",
        "            candidate_tup\n",
        "            for candidate_tup in candidateInfo_list\n",
        "            if candidate_tup.isNodule_bool\n",
        "        ]\n",
        "        self.positive_mask = self.buildAnnotationMask(self.positiveInfo_list)\n",
        "        self.positive_indexes = (self.positive_mask.sum(axis=(1,2))\n",
        "                                 .nonzero()[0].tolist())\n",
        "\n",
        "    def buildAnnotationMask(self, positiveInfo_list, threshold_hu = -700):   \n",
        "        boundingBox_a = np.zeros_like(self.hu_a, dtype=bool)  # instead np.bool: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
        "\n",
        "        for candidateInfo_tup in positiveInfo_list:\n",
        "            center_irc = xyz2irc(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                self.origin_xyz,\n",
        "                self.vxSize_xyz,\n",
        "                self.direction_a,\n",
        "            )\n",
        "            ci = int(center_irc.index)\n",
        "            cr = int(center_irc.row)\n",
        "            cc = int(center_irc.col)\n",
        "\n",
        "            index_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci + index_radius, cr, cc] > threshold_hu and \\\n",
        "                        self.hu_a[ci - index_radius, cr, cc] > threshold_hu:\n",
        "                    index_radius += 1\n",
        "            except IndexError:\n",
        "                index_radius -= 1\n",
        "\n",
        "            row_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci, cr + row_radius, cc] > threshold_hu and \\\n",
        "                        self.hu_a[ci, cr - row_radius, cc] > threshold_hu:\n",
        "                    row_radius += 1\n",
        "            except IndexError:\n",
        "                row_radius -= 1\n",
        "\n",
        "            col_radius = 2\n",
        "            try:\n",
        "                while self.hu_a[ci, cr, cc + col_radius] > threshold_hu and \\\n",
        "                        self.hu_a[ci, cr, cc - col_radius] > threshold_hu:\n",
        "                    col_radius += 1\n",
        "            except IndexError:\n",
        "                col_radius -= 1\n",
        "\n",
        "            # assert index_radius > 0, repr([candidateInfo_tup.center_xyz, center_irc, self.hu_a[ci, cr, cc]])\n",
        "            # assert row_radius > 0\n",
        "            # assert col_radius > 0\n",
        "\n",
        "            boundingBox_a[\n",
        "                 ci - index_radius: ci + index_radius + 1,\n",
        "                 cr - row_radius: cr + row_radius + 1,\n",
        "                 cc - col_radius: cc + col_radius + 1] = True\n",
        "\n",
        "        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n",
        "\n",
        "        return mask_a\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(center_xyz, self.origin_xyz, self.vxSize_xyz,\n",
        "                             self.direction_a)\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(int(start_ndx), int(end_ndx)))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "        pos_chunk = self.positive_mask[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, pos_chunk, center_irc\n",
        "\n",
        "@functools.lru_cache(1, typed=True)    \n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, pos_chunk, center_irc = ct.getRawCandidate(center_xyz,\n",
        "                                                         width_irc)\n",
        "    ct_chunk.clip(-1000, 1000, ct_chunk)\n",
        "    return ct_chunk, pos_chunk, center_irc\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtSampleSize(series_uid):\n",
        "    ct = Ct(series_uid)\n",
        "    return int(ct.hu_a.shape[0]), ct.positive_indexes\n",
        "\n",
        "\n",
        "class Luna2dSegmentationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 contextSlices_count=3,\n",
        "                 fullCt_bool=False,\n",
        "            ):\n",
        "        self.contextSlices_count = int(contextSlices_count)\n",
        "        self.fullCt_bool = fullCt_bool\n",
        "\n",
        "        if series_uid:\n",
        "            self.series_list = [series_uid]\n",
        "        else:\n",
        "            self.series_list = sorted(getCandidateInfoDict().keys())\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.series_list = self.series_list[::val_stride] #e.g. every 10th item (more precisely key for item)\n",
        "            assert self.series_list\n",
        "        elif val_stride > 0:\n",
        "            del self.series_list[::val_stride]\n",
        "            assert self.series_list\n",
        "\n",
        "        self.sample_list = []\n",
        "        for series_uid in self.series_list:\n",
        "            index_count, positive_indexes = getCtSampleSize(series_uid)\n",
        "\n",
        "            if self.fullCt_bool:\n",
        "                self.sample_list += [(series_uid, slice_ndx)\n",
        "                                     for slice_ndx in range(index_count)]\n",
        "            else:\n",
        "                self.sample_list += [(series_uid, slice_ndx)\n",
        "                                     for slice_ndx in positive_indexes]\n",
        "\n",
        "        self.candidateInfo_list = getCandidateInfoList()\n",
        "\n",
        "        series_set = set(self.series_list)\n",
        "        self.candidateInfo_list = [cit for cit in self.candidateInfo_list\n",
        "                                   if cit.series_uid in series_set]\n",
        "\n",
        "        self.pos_list = [nt for nt in self.candidateInfo_list\n",
        "                            if nt.isNodule_bool]\n",
        "\n",
        "        print(\"{!r}: {} {} series, {} slices, {} nodules\".format(\n",
        "            self,\n",
        "            len(self.series_list),\n",
        "            {None: 'general', True: 'validation', False: 'training'}[isValSet_bool],\n",
        "            len(self.sample_list),\n",
        "            len(self.pos_list),\n",
        "        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
        "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
        "\n",
        "    def getitem_fullSlice(self, series_uid, slice_ndx):\n",
        "        ct = getCt(series_uid)\n",
        "        ct_t = torch.zeros((int(self.contextSlices_count) * 2 + 1, 512, 512))\n",
        "\n",
        "        start_ndx = slice_ndx - int(self.contextSlices_count)\n",
        "        end_ndx = slice_ndx + int(self.contextSlices_count) + 1\n",
        "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
        "            context_ndx = max(context_ndx, 0)\n",
        "            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n",
        "            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_t.clamp_(-1000, 1000)\n",
        "\n",
        "        pos_t = torch.from_numpy(ct.positive_mask[slice_ndx]).unsqueeze(0)\n",
        "\n",
        "        return ct_t, pos_t, ct.series_uid, slice_ndx\n",
        "\n",
        "\n",
        "class TrainingLuna2dSegmentationDataset(Luna2dSegmentationDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.ratio_int = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return 300000       ## len(self.candidateInfo_list)   ?\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        random.shuffle(self.candidateInfo_list)\n",
        "        random.shuffle(self.pos_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n",
        "        return self.getitem_trainingCrop(candidateInfo_tup)\n",
        "\n",
        "    def getitem_trainingCrop(self, candidateInfo_tup):\n",
        "        ct_a, pos_a, center_irc = getCtRawCandidate(\n",
        "            candidateInfo_tup.series_uid,\n",
        "            candidateInfo_tup.center_xyz,\n",
        "            (7, 96, 96),\n",
        "        )\n",
        "        pos_a = pos_a[3:4]\n",
        "\n",
        "        row_offset = random.randrange(0,32)\n",
        "        col_offset = random.randrange(0,32)\n",
        "        ct_t = torch.from_numpy(ct_a[:, row_offset:row_offset+64,\n",
        "                                     col_offset:col_offset+64]).to(torch.float32)\n",
        "        pos_t = torch.from_numpy(pos_a[:, row_offset:row_offset+64,\n",
        "                                       col_offset:col_offset+64]).to(torch.long)\n",
        "\n",
        "        slice_ndx = center_irc.index\n",
        "\n",
        "        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\n",
        "\n",
        "class PrepcacheLunaDataset(Dataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.candidateInfo_list = getCandidateInfoList()\n",
        "        self.pos_list = [nt for nt in self.candidateInfo_list if nt.isNodule_bool]\n",
        "\n",
        "        self.seen_set = set()\n",
        "        self.candidateInfo_list.sort(key=lambda x: x.series_uid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        # candidate_t, pos_t, series_uid, center_t = super().__getitem__(ndx)\n",
        "\n",
        "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "        getCtRawCandidate(candidateInfo_tup.series_uid, candidateInfo_tup.center_xyz, (7, 96, 96))\n",
        "\n",
        "        series_uid = candidateInfo_tup.series_uid\n",
        "        if series_uid not in self.seen_set:\n",
        "            self.seen_set.add(series_uid)\n",
        "\n",
        "            getCtSampleSize(series_uid)\n",
        "            # ct = getCt(series_uid)\n",
        "            # for mask_ndx in ct.positive_indexes:\n",
        "            #     build2dLungMask(series_uid, mask_ndx)\n",
        "\n",
        "        return 0, 1 #candidate_t, pos_t, series_uid, center_t\n",
        "\n",
        "\n",
        "# class TvTrainingLuna2dSegmentationDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, isValSet_bool=False, val_stride=10, contextSlices_count=3):\n",
        "#         assert contextSlices_count == 3\n",
        "#         data = torch.load('./imgs_and_masks.pt')\n",
        "#         suids = list(set(data['suids']))\n",
        "#         trn_mask_suids = torch.arange(len(suids)) % val_stride < (val_stride - 1)\n",
        "#         trn_suids = {s for i, s in zip(trn_mask_suids, suids) if i}\n",
        "#         trn_mask = torch.tensor([(s in trn_suids) for s in data[\"suids\"]])\n",
        "#         if not isValSet_bool:\n",
        "#             self.imgs = data[\"imgs\"][trn_mask]\n",
        "#             self.masks = data[\"masks\"][trn_mask]\n",
        "#             self.suids = [s for s, i in zip(data[\"suids\"], trn_mask) if i]\n",
        "#         else:\n",
        "#             self.imgs = data[\"imgs\"][~trn_mask]\n",
        "#             self.masks = data[\"masks\"][~trn_mask]\n",
        "#             self.suids = [s for s, i in zip(data[\"suids\"], trn_mask) if not i]\n",
        "#         # discard spurious hotspots and clamp bone\n",
        "#         self.imgs.clamp_(-1000, 1000)\n",
        "#         self.imgs /= 1000\n",
        "\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.imgs)\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         oh, ow = torch.randint(0, 32, (2,))\n",
        "#         sl = self.masks.size(1)//2\n",
        "#         return self.imgs[i, :, oh: oh + 64, ow: ow + 64], 1, self.masks[i, sl: sl+1, oh: oh + 64, ow: ow + 64].to(torch.float32), self.suids[i], 9999\n",
        "\n",
        "!mkdir ./data-unversioned/segmentation\n",
        "!mkdir ./data-unversioned/segmentation/models\n",
        "!mkdir segmentation_runs"
      ],
      "metadata": {
        "id": "pdmek9QdBk1D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title check the segmentation data\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=True)\n",
        "item = candidateInfo_list[5]\n",
        "\n",
        "#series_list = sorted(set(t.series_uid for t in candidateInfo_list)) ## if u want to access instances turn into list\n",
        "                                                                    ## list( series_list )   \n",
        "def transparent_cmap(cmap, N=255):\n",
        "    \"Copy colormap and set alpha values\"\n",
        "\n",
        "    mycmap = copy.deepcopy(cmap)\n",
        "    mycmap._init()\n",
        "    mycmap._lut[:,-1] = np.linspace(0, 0.75, N+4)\n",
        "    return mycmap\n",
        "\n",
        "tgray = transparent_cmap(plt.cm.gray)\n",
        "tpurp = transparent_cmap(plt.cm.Purples)\n",
        "tblue = transparent_cmap(plt.cm.Blues)\n",
        "togreen = transparent_cmap(plt.cm.Greens)\n",
        "torange = transparent_cmap(plt.cm.Oranges)\n",
        "tred = transparent_cmap(plt.cm.Reds)\n",
        "\n",
        "clim=(0, 1.3)\n",
        "start_ndx = 3\n",
        "mask_model = SegmentationMask().to('cuda')\n",
        "\n",
        "ct = getCt(item.series_uid)\n",
        "center_irc = xyz2irc(item.center_xyz, ct.origin_xyz, ct.vxSize_xyz, ct.direction_a)\n",
        "print(item, 'center_irc', center_irc)\n",
        "\n",
        "mask_tup = build2dLungMask(ct.series_uid, int(center_irc.index))\n",
        "mask_tup = mask_tup._make(x.cpu().numpy()[0][0] for x in mask_tup)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "slice_a = ((ct.hu_a[int(center_irc.index)] / 1000) + 1) / 2\n",
        "slice_a = slice_a.clip(0, 1)\n",
        "\n",
        "subplot = fig.add_subplot(1, 1, 1)\n",
        "subplot.set_title('mal mask', fontsize=30)\n",
        "for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "    label.set_fontsize(20)\n",
        "plt.imshow(\n",
        "    slice_a + 3 * slice_a * mask_tup.pos_mask, \n",
        "    #clim=(-2000, 2000), \n",
        "    cmap='gray',\n",
        ")\n",
        "#plt.imshow(ct.hu_a[int(center_irc.index)] * mask_tup.pos_mask, clim=(-1000,1000), cmap='gray')"
      ],
      "metadata": {
        "id": "P_z40NwlSjwG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title segmentation model\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
        "                 batch_norm=False, up_mode='upconv'):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
        "                                                padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
        "                                            padding, batch_norm))\n",
        "            prev_channels = 2**(wf+i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path)-1:\n",
        "                blocks.append(x)\n",
        "                x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i-1])\n",
        "\n",
        "        return self.last(x)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
        "                               padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        # block.append(nn.LeakyReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
        "                                         stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class UNetWrapper(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])\n",
        "        self.unet = UNet(**kwargs)\n",
        "        self.final = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        init_set = {\n",
        "            nn.Conv2d,\n",
        "            nn.Conv3d,\n",
        "            nn.ConvTranspose2d,\n",
        "            nn.ConvTranspose3d,\n",
        "            nn.Linear,\n",
        "        }\n",
        "        for m in self.modules():\n",
        "            if type(m) in init_set:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "        # nn.init.constant_(self.unet.last.bias, -4)\n",
        "        # nn.init.constant_(self.unet.last.bias, 4)\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.input_batchnorm(input_batch)\n",
        "        un_output = self.unet(bn_output)\n",
        "        fn_output = self.final(un_output)\n",
        "        return fn_output\n",
        "\n",
        "class SegmentationAugmentation(nn.Module):\n",
        "    def __init__(\n",
        "            self, flip=None, offset=None, scale=None, rotate=None, noise=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flip = flip\n",
        "        self.offset = offset\n",
        "        self.scale = scale\n",
        "        self.rotate = rotate\n",
        "        self.noise = noise\n",
        "\n",
        "    def forward(self, input_g, label_g):\n",
        "        transform_t = self._build2dTransformMatrix()\n",
        "        transform_t = transform_t.expand(input_g.shape[0], -1, -1)\n",
        "        transform_t = transform_t.to(input_g.device, torch.float32)\n",
        "        affine_t = F.affine_grid(transform_t[:,:2],\n",
        "                input_g.size(), align_corners=False)\n",
        "\n",
        "        augmented_input_g = F.grid_sample(input_g,\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "        augmented_label_g = F.grid_sample(label_g.to(torch.float32),\n",
        "                affine_t, padding_mode='border',\n",
        "                align_corners=False)\n",
        "\n",
        "        if self.noise:\n",
        "            noise_t = torch.randn_like(augmented_input_g)\n",
        "            noise_t *= self.noise\n",
        "\n",
        "            augmented_input_g += noise_t\n",
        "\n",
        "        return augmented_input_g, augmented_label_g > 0.5\n",
        "\n",
        "    def _build2dTransformMatrix(self):\n",
        "        transform_t = torch.eye(3)\n",
        "\n",
        "        for i in range(2):\n",
        "            if self.flip:\n",
        "                if random.random() > 0.5:\n",
        "                    transform_t[i,i] *= -1\n",
        "\n",
        "            if self.offset:\n",
        "                offset_float = self.offset\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[2,i] = offset_float * random_float\n",
        "\n",
        "            if self.scale:\n",
        "                scale_float = self.scale\n",
        "                random_float = (random.random() * 2 - 1)\n",
        "                transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "        if self.rotate:\n",
        "            angle_rad = random.random() * math.pi * 2\n",
        "            s = math.sin(angle_rad)\n",
        "            c = math.cos(angle_rad)\n",
        "\n",
        "            rotation_t = torch.tensor([\n",
        "                [c, -s, 0],\n",
        "                [s, c, 0],\n",
        "                [0, 0, 1]])\n",
        "\n",
        "            transform_t @= rotation_t\n",
        "\n",
        "        return transform_t\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZPEJgYvCqLli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title segmentation model training\n",
        "\n",
        "\n",
        "# Used for computeClassificationLoss and logMetrics to index into metrics_t/metrics_a\n",
        "# METRICS_LABEL_NDX = 0\n",
        "METRICS_LOSS_NDX = 1\n",
        "# METRICS_FN_LOSS_NDX = 2\n",
        "# METRICS_ALL_LOSS_NDX = 3\n",
        "\n",
        "# METRICS_PTP_NDX = 4\n",
        "# METRICS_PFN_NDX = 5\n",
        "# METRICS_MFP_NDX = 6\n",
        "METRICS_TP_NDX = 7\n",
        "METRICS_FN_NDX = 8\n",
        "METRICS_FP_NDX = 9\n",
        "\n",
        "METRICS_SIZE = 10\n",
        "\n",
        "class SegmentationTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=16,\n",
        "                 num_workers : int=0,  ## if you want more workers first check this: https://stackoverflow.com/questions/71713719/runtimeerror-dataloader-worker-pids-15876-2756-exited-unexpectedly\n",
        "                 epochs: int=1,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'segmentation',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.augmented = augmented,\n",
        "        self.augment_flip = augment_flip,\n",
        "        self.augment_offset = augment_offset,\n",
        "        self.augment_scale = augment_scale,\n",
        "        self.augment_rotate = augment_rotate,\n",
        "        self.augment_noise = augment_noise,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "        self.totalTrainingSamples_count = 0\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.03\n",
        "        if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.segmentation_model, self.augmentation_model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        segmentation_model = UNetWrapper(\n",
        "            in_channels=7,\n",
        "            n_classes=1,\n",
        "            depth=3,\n",
        "            wf=4,\n",
        "            padding=True,\n",
        "            batch_norm=True,\n",
        "            up_mode='upconv',\n",
        "        )\n",
        "\n",
        "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
        "\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                segmentation_model = nn.DataParallel(segmentation_model)\n",
        "                augmentation_model = nn.DataParallel(augmentation_model)\n",
        "            segmentation_model = segmentation_model.to(self.device)\n",
        "            augmentation_model = augmentation_model.to(self.device)\n",
        "\n",
        "        return segmentation_model, augmentation_model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return Adam(self.segmentation_model.parameters())\n",
        "        # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)\n",
        "\n",
        "\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = TrainingLuna2dSegmentationDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            contextSlices_count=3,\n",
        "        )\n",
        "\n",
        "        batch_size = int(*self.batch_size)\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,                  \n",
        "            num_workers=int(*self.num_workers),            ## chenged from tupled int back to int\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = Luna2dSegmentationDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "            contextSlices_count=3,\n",
        "        )\n",
        "\n",
        "        batch_size = int(*self.batch_size)\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=int(*self.num_workers),\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('segmentation_runs', str(self.tb_prefix[0]), str(self.time_str)) ## corrected here\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '_trn_seg_')\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '_val_seg_')\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        best_score = 0.0\n",
        "        self.validation_cadence = 5\n",
        "        for epoch_ndx in range(1, int(*self.epochs) + 1):\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                int(*self.epochs),\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                int(*self.batch_size),\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n",
        "                # if validation is wanted\n",
        "                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "                best_score = max(score, best_score)\n",
        "\n",
        "                self.saveModel('seg', epoch_ndx, score == best_score)\n",
        "\n",
        "                self.logImages(epoch_ndx, 'trn', train_dl)\n",
        "                self.logImages(epoch_ndx, 'val', val_dl)\n",
        "\n",
        "        self.trn_writer.close()\n",
        "        self.val_writer.close()\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=self.device)\n",
        "        self.segmentation_model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
        "            loss_var.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += trnMetrics_g.size(1)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=self.device)\n",
        "            self.segmentation_model.eval()\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
        "                         classificationThreshold=0.5):\n",
        "        input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        if self.segmentation_model.training and self.augmentation_dict:\n",
        "            input_g, label_g = self.augmentation_model(input_g, label_g)\n",
        "\n",
        "        prediction_g = self.segmentation_model(input_g)\n",
        "\n",
        "        diceLoss_g = self.diceLoss(prediction_g, label_g)\n",
        "        fnLoss_g = self.diceLoss(prediction_g * label_g, label_g)\n",
        "\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + input_t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictionBool_g = (prediction_g[:, 0:1]\n",
        "                                > classificationThreshold).to(torch.float32)\n",
        "\n",
        "            tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
        "            fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
        "            fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
        "\n",
        "            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
        "            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
        "            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
        "            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
        "\n",
        "        return diceLoss_g.mean() + fnLoss_g.mean() * 8\n",
        "\n",
        "    def diceLoss(self, prediction_g, label_g, epsilon=1):\n",
        "        diceLabel_g = label_g.sum(dim=[1,2,3])\n",
        "        dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
        "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n",
        "\n",
        "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
        "            / (dicePrediction_g + diceLabel_g + epsilon)\n",
        "\n",
        "        return 1 - diceRatio_g\n",
        "\n",
        "\n",
        "    def logImages(self, epoch_ndx, mode_str, dl):\n",
        "        self.segmentation_model.eval()\n",
        "\n",
        "        images = sorted(dl.dataset.series_list)[:12]\n",
        "        for series_ndx, series_uid in enumerate(images):\n",
        "            ct = getCt(series_uid)\n",
        "\n",
        "            for slice_ndx in range(6):\n",
        "                ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n",
        "                sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n",
        "\n",
        "                ct_t, label_t, series_uid, ct_ndx = sample_tup\n",
        "\n",
        "                input_g = ct_t.to(self.device).unsqueeze(0)\n",
        "                label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n",
        "\n",
        "                prediction_g = self.segmentation_model(input_g)[0]\n",
        "                prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n",
        "                label_a = label_g.cpu().numpy()[0][0] > 0.5\n",
        "\n",
        "                ct_t[:-1,:,:] /= 2000\n",
        "                ct_t[:-1,:,:] += 0.5\n",
        "\n",
        "                ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n",
        "\n",
        "                image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "                image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "                image_a[:,:,0] += prediction_a & (1 - label_a)\n",
        "                image_a[:,:,0] += (1 - prediction_a) & label_a\n",
        "                image_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5\n",
        "\n",
        "                image_a[:,:,1] += prediction_a & label_a\n",
        "                image_a *= 0.5\n",
        "                image_a.clip(0, 1, image_a)\n",
        "\n",
        "                writer = getattr(self, mode_str + '_writer')\n",
        "                writer.add_image(\n",
        "                    f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n",
        "                    image_a,\n",
        "                    self.totalTrainingSamples_count,\n",
        "                    dataformats='HWC',\n",
        "                )\n",
        "\n",
        "                if epoch_ndx == 1:\n",
        "                    image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
        "                    image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n",
        "                    # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n",
        "                    image_a[:,:,1] += label_a  # Green\n",
        "                    # image_a[:,:,2] += neg_a  # Blue\n",
        "\n",
        "                    image_a *= 0.5\n",
        "                    image_a[image_a < 0] = 0\n",
        "                    image_a[image_a > 1] = 1\n",
        "                    writer.add_image(\n",
        "                        '{}/{}_label_{}'.format(\n",
        "                            mode_str,\n",
        "                            series_ndx,\n",
        "                            slice_ndx,\n",
        "                        ),\n",
        "                        image_a,\n",
        "                        self.totalTrainingSamples_count,\n",
        "                        dataformats='HWC',\n",
        "                    )\n",
        "                # This flush prevents TB from getting confused about which\n",
        "                # data item belongs where.\n",
        "                writer.flush()\n",
        "\n",
        "    def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        metrics_a = metrics_t.detach().numpy()\n",
        "        sum_a = metrics_a.sum(axis=1)\n",
        "        assert np.isfinite(metrics_a).all()\n",
        "\n",
        "        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
        "\n",
        "        metrics_dict['percent_all/tp'] = \\\n",
        "            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fn'] = \\\n",
        "            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
        "        metrics_dict['percent_all/fp'] = \\\n",
        "            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
        "\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
        "        recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n",
        "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
        "            / ((precision + recall) or 1)\n",
        "\n",
        "        print((\"E{} {:8} \"\n",
        "                 + \"{loss/all:.4f} loss, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score\"\n",
        "                  ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "        print((\"E{} {:8} \"\n",
        "                  + \"{loss/all:.4f} loss, \"\n",
        "                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
        "        ).format(\n",
        "            epoch_ndx,\n",
        "            mode_str + '_all',\n",
        "            **metrics_dict,\n",
        "        ))\n",
        "\n",
        "        self.initTensorboardWriters()\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        prefix_str = 'seg_'\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            writer.add_scalar(prefix_str + key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "        score = metrics_dict['pr/recall']\n",
        "\n",
        "        return score\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             writer.add_histogram(\n",
        "    #                 name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                 param.data.cpu().numpy(),\n",
        "    #                 # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                 self.totalTrainingSamples_count,\n",
        "    #                 # bins=bins,\n",
        "    #             )\n",
        "    #\n",
        "    #             # print name, param.data\n",
        "\n",
        "    def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
        "        file_path = os.path.join(\n",
        "            'data-unversioned',\n",
        "            'segmentation',\n",
        "            'models',\n",
        "            str(self.tb_prefix[0]),\n",
        "            '{}_{}.{}.state'.format(\n",
        "                str(type_str),\n",
        "                str(self.time_str),\n",
        "                int(self.totalTrainingSamples_count),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
        "\n",
        "        model = self.segmentation_model\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model = model.module\n",
        "\n",
        "        ##'sys_argv': sys.argv,\n",
        "        state = {\n",
        "            'time': str(datetime.datetime.now()),\n",
        "            'model_state': model.state_dict(),\n",
        "            'model_name': type(model).__name__,\n",
        "            'optimizer_state' : self.optimizer.state_dict(),\n",
        "            'optimizer_name': type(self.optimizer).__name__,\n",
        "            'epoch': epoch_ndx,\n",
        "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
        "        }\n",
        "        torch.save(state, file_path)\n",
        "\n",
        "        print(\"Saved model params to {}\".format(file_path))\n",
        "\n",
        "        if isBest:\n",
        "            best_path = os.path.join(\n",
        "                'data-unversioned', 'segmentation', 'models',\n",
        "                str(self.tb_prefix[0]),\n",
        "                f'{type_str}_{self.time_str}.best.state')\n",
        "            shutil.copyfile(file_path, best_path)\n",
        "\n",
        "            print(\"Saved model params to {}\".format(best_path))\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            print(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n",
        "\n",
        "\n",
        "segmodel = SegmentationTrainingApp()\n",
        "segmodel.main() "
      ],
      "metadata": {
        "cellView": "form",
        "id": "X_jCb5kDq7nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here starts end-to-end cancer detection\n",
        "\n",
        "here we combine segmentation model for nodule proposals, classification model for selecting which of those nodules look like real nodules and retraining of same classification model for classifying malignant nodules (out of all realistic nodules, malignant and benign)"
      ],
      "metadata": {
        "id": "NFoMFf3ZvS5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dataset for malignancy detection\n",
        "\n",
        "# def cleanCache():\n",
        "#     shutil.rmtree('/content/data-unversioned/cache')\n",
        "#     os.mkdir('/content/data-unversioned/cache')\n",
        "\n",
        "# cleanCache()\n",
        "\n",
        "raw_cache = getCache('end-to-end')\n",
        "\n",
        "CandidateInfoTuple = namedtuple(\n",
        "    'CandidateInfoTuple',\n",
        "    'isNodule_bool, hasAnnotation_bool, isMal_bool, diameter_mm, series_uid, center_xyz',\n",
        ")\n",
        "MaskTuple = namedtuple(\n",
        "    'MaskTuple',\n",
        "    'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask',\n",
        ")\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    candidateInfo_list = []\n",
        "    with open('/content/annotations_with_malignancy.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "            isMal_bool = {'False': False, 'True': True}[row[5]]\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(True, True, isMal_bool, annotationDiameter_mm, series_uid, annotationCenter_xyz))\n",
        "\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            if not isNodule_bool:\n",
        "                candidateInfo_list.append(CandidateInfoTuple(\n",
        "                    False,\n",
        "                    False,\n",
        "                    False,\n",
        "                    0.0,\n",
        "                    series_uid,\n",
        "                    candidateCenter_xyz,\n",
        "                ))\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoDict(requireOnDisk_bool=True):\n",
        "    candidateInfo_list = getCandidateInfoList(requireOnDisk_bool)\n",
        "    candidateInfo_dict = {}\n",
        "\n",
        "    for candidateInfo_tup in candidateInfo_list:\n",
        "        candidateInfo_dict.setdefault(candidateInfo_tup.series_uid, []).append(candidateInfo_tup)\n",
        "\n",
        "    return candidateInfo_dict\n",
        "\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_a.clip(-1000, 1000, ct_a)\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(center_xyz, self.origin_xyz, self.vxSize_xyz, self.direction_a)\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtSampleSize(series_uid):\n",
        "    ct = Ct(series_uid, buildMasks_bool=False)\n",
        "    return len(ct.negative_indexes)\n",
        "\n",
        "def getCtAugmentedCandidate(\n",
        "        augmentation_dict,\n",
        "        series_uid, center_xyz, width_irc,\n",
        "        use_cache=True):\n",
        "    if use_cache:\n",
        "        ct_chunk, center_irc = getCtRawCandidate(series_uid, center_xyz, width_irc)\n",
        "    else:\n",
        "        ct = getCt(series_uid)\n",
        "        ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "\n",
        "    ct_t = torch.tensor(ct_chunk).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "    transform_t = torch.eye(4)\n",
        "    # ... <1>\n",
        "\n",
        "    for i in range(3):\n",
        "        if 'flip' in augmentation_dict:\n",
        "            if random.random() > 0.5:\n",
        "                transform_t[i,i] *= -1\n",
        "\n",
        "        if 'offset' in augmentation_dict:\n",
        "            offset_float = augmentation_dict['offset']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i, 3] = offset_float * random_float\n",
        "\n",
        "        if 'scale' in augmentation_dict:\n",
        "            scale_float = augmentation_dict['scale']\n",
        "            random_float = (random.random() * 2 - 1)\n",
        "            transform_t[i,i] *= 1.0 + scale_float * random_float\n",
        "\n",
        "\n",
        "    if 'rotate' in augmentation_dict:\n",
        "        angle_rad = random.random() * math.pi * 2\n",
        "        s = math.sin(angle_rad)\n",
        "        c = math.cos(angle_rad)\n",
        "\n",
        "        rotation_t = torch.tensor([\n",
        "            [c, -s, 0, 0],\n",
        "            [s, c, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ])\n",
        "\n",
        "        transform_t @= rotation_t\n",
        "\n",
        "    affine_t = F.affine_grid(\n",
        "            transform_t[:3].unsqueeze(0).to(torch.float32),\n",
        "            ct_t.size(),\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "    augmented_chunk = F.grid_sample(\n",
        "            ct_t,\n",
        "            affine_t,\n",
        "            padding_mode='border',\n",
        "            align_corners=False,\n",
        "        ).to('cpu')\n",
        "\n",
        "    if 'noise' in augmentation_dict:\n",
        "        noise_t = torch.randn_like(augmented_chunk)\n",
        "        noise_t *= augmentation_dict['noise']\n",
        "\n",
        "        augmented_chunk += noise_t\n",
        "\n",
        "    return augmented_chunk[0], center_irc\n",
        "\n",
        "\n",
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "                 sortby_str='random',\n",
        "                 ratio_int=0,\n",
        "                 augmentation_dict=None,\n",
        "                 candidateInfo_list=None,\n",
        "            ):\n",
        "        self.ratio_int = ratio_int\n",
        "        self.augmentation_dict = augmentation_dict\n",
        "\n",
        "        if candidateInfo_list:\n",
        "            self.candidateInfo_list = copy.copy(candidateInfo_list)\n",
        "            self.use_cache = False\n",
        "        else:\n",
        "            self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "            self.use_cache = True\n",
        "\n",
        "        if series_uid:\n",
        "            self.series_list = [series_uid]\n",
        "        else:\n",
        "            self.series_list = sorted(set(candidateInfo_tup.series_uid for candidateInfo_tup in self.candidateInfo_list))\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.series_list = self.series_list[::val_stride]\n",
        "            assert self.series_list\n",
        "        elif val_stride > 0:\n",
        "            del self.series_list[::val_stride]\n",
        "            assert self.series_list\n",
        "\n",
        "        series_set = set(self.series_list)\n",
        "        self.candidateInfo_list = [x for x in self.candidateInfo_list if x.series_uid in series_set]\n",
        "\n",
        "        if sortby_str == 'random':\n",
        "            random.shuffle(self.candidateInfo_list)\n",
        "        elif sortby_str == 'series_uid':\n",
        "            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n",
        "        elif sortby_str == 'label_and_size':\n",
        "            pass\n",
        "        else:\n",
        "            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n",
        "\n",
        "        self.neg_list = \\\n",
        "            [nt for nt in self.candidateInfo_list if not nt.isNodule_bool]\n",
        "        self.pos_list = \\\n",
        "            [nt for nt in self.candidateInfo_list if nt.isNodule_bool]\n",
        "        self.ben_list = \\\n",
        "            [nt for nt in self.pos_list if not nt.isMal_bool]\n",
        "        self.mal_list = \\\n",
        "            [nt for nt in self.pos_list if nt.isMal_bool]\n",
        "\n",
        "        print(\"{!r}: {} {} samples, {} neg, {} pos, {} ratio\".format(\n",
        "            self,\n",
        "            len(self.candidateInfo_list),\n",
        "            \"validation\" if isValSet_bool else \"training\",\n",
        "            len(self.neg_list),\n",
        "            len(self.pos_list),\n",
        "            '{}:1'.format(self.ratio_int) if self.ratio_int else 'unbalanced'\n",
        "        ))\n",
        "\n",
        "    def shuffleSamples(self):\n",
        "        if self.ratio_int:\n",
        "            random.shuffle(self.candidateInfo_list)\n",
        "            random.shuffle(self.neg_list)\n",
        "            random.shuffle(self.pos_list)\n",
        "            random.shuffle(self.ben_list)\n",
        "            random.shuffle(self.mal_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.ratio_int:\n",
        "            return 50000\n",
        "        else:\n",
        "            return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        if self.ratio_int:\n",
        "            pos_ndx = ndx // (self.ratio_int + 1)\n",
        "\n",
        "            if ndx % (self.ratio_int + 1):\n",
        "                neg_ndx = ndx - 1 - pos_ndx\n",
        "                neg_ndx %= len(self.neg_list)\n",
        "                candidateInfo_tup = self.neg_list[neg_ndx]\n",
        "            else:\n",
        "                pos_ndx %= len(self.pos_list)\n",
        "                candidateInfo_tup = self.pos_list[pos_ndx]\n",
        "        else:\n",
        "            candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "\n",
        "        return self.sampleFromCandidateInfo_tup(\n",
        "            candidateInfo_tup, candidateInfo_tup.isNodule_bool\n",
        "        )\n",
        "\n",
        "    def sampleFromCandidateInfo_tup(self, candidateInfo_tup, label_bool):\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        if self.augmentation_dict:\n",
        "            candidate_t, center_irc = getCtAugmentedCandidate(\n",
        "                self.augmentation_dict,\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "                self.use_cache,\n",
        "            )\n",
        "        elif self.use_cache:\n",
        "            candidate_a, center_irc = getCtRawCandidate(\n",
        "                candidateInfo_tup.series_uid,\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "        else:\n",
        "            ct = getCt(candidateInfo_tup.series_uid)\n",
        "            candidate_a, center_irc = ct.getRawCandidate(\n",
        "                candidateInfo_tup.center_xyz,\n",
        "                width_irc,\n",
        "            )\n",
        "            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
        "            candidate_t = candidate_t.unsqueeze(0)\n",
        "\n",
        "        label_t = torch.tensor([False, False], dtype=torch.long)\n",
        "\n",
        "        if not label_bool:\n",
        "            label_t[0] = True\n",
        "            index_t = 0\n",
        "        else:\n",
        "            label_t[1] = True\n",
        "            index_t = 1\n",
        "\n",
        "        return candidate_t, label_t, index_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)\n",
        "\n",
        "\n",
        "class MalignantLunaDataset(LunaDataset):\n",
        "    def __len__(self):\n",
        "        if self.ratio_int:\n",
        "            return 100000\n",
        "        else:\n",
        "            return len(self.ben_list + self.mal_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        if self.ratio_int:\n",
        "            if ndx % 2 != 0:\n",
        "                candidateInfo_tup = self.mal_list[(ndx // 2) % len(self.mal_list)]\n",
        "            elif ndx % 4 == 0:\n",
        "                candidateInfo_tup = self.ben_list[(ndx // 4) % len(self.ben_list)]\n",
        "            else:\n",
        "                candidateInfo_tup = self.neg_list[(ndx // 4) % len(self.neg_list)]\n",
        "        else:\n",
        "            if ndx >= len(self.ben_list):\n",
        "                candidateInfo_tup = self.mal_list[ndx - len(self.ben_list)]\n",
        "            else:\n",
        "                candidateInfo_tup = self.ben_list[ndx]\n",
        "\n",
        "        return self.sampleFromCandidateInfo_tup(\n",
        "            candidateInfo_tup, candidateInfo_tup.isMal_bool\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6udgywvVwgxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for analysing nodules : segmentation - modifications - nodule presense classification - (if model retrained) malignancy classification\n",
        "\n",
        "\n",
        "def print_confusion(label, confusions, do_mal):\n",
        "    row_labels = ['Non-Nodules', 'Benign', 'Malignant']\n",
        "\n",
        "    if do_mal:\n",
        "        col_labels = ['', 'Complete Miss', 'Filtered Out', 'Pred. Benign', 'Pred. Malignant']\n",
        "    else:\n",
        "        col_labels = ['', 'Complete Miss', 'Filtered Out', 'Pred. Nodule']\n",
        "        confusions[:, -2] += confusions[:, -1]\n",
        "        confusions = confusions[:, :-1]\n",
        "    cell_width = 16\n",
        "    f = '{:>' + str(cell_width) + '}'\n",
        "    print(label)\n",
        "    print(' | '.join([f.format(s) for s in col_labels]))\n",
        "    for i, (l, r) in enumerate(zip(row_labels, confusions)):\n",
        "        r = [l] + list(r)\n",
        "        if i == 0:\n",
        "            r[1] = ''\n",
        "        print(' | '.join([f.format(i) for i in r]))\n",
        "\n",
        "def match_and_score(detections, truth, threshold=0.5, threshold_mal=0.5):\n",
        "    # Returns 3x4 confusion matrix for:\n",
        "    # Rows: Truth: Non-Nodules, Benign, Malignant\n",
        "    # Cols: Not Detected, Detected by Seg, Detected as Benign, Detected as Malignant\n",
        "    # If one true nodule matches multiple detections, the \"highest\" detection is considered\n",
        "    # If one detection matches several true nodule annotations, it counts for all of them\n",
        "    true_nodules = [c for c in truth if c.isNodule_bool]\n",
        "    truth_diams = np.array([c.diameter_mm for c in true_nodules])\n",
        "    truth_xyz = np.array([c.center_xyz for c in true_nodules])\n",
        "\n",
        "    detected_xyz = np.array([n[2] for n in detections])\n",
        "    # detection classes will contain\n",
        "    # 1 -> detected by seg but filtered by cls\n",
        "    # 2 -> detected as benign nodule (or nodule if no malignancy model is used)\n",
        "    # 3 -> detected as malignant nodule (if applicable)\n",
        "    detected_classes = np.array([1 if d[0] < threshold\n",
        "                                 else (2 if d[1] < threshold\n",
        "                                       else 3) for d in detections])\n",
        "\n",
        "    confusion = np.zeros((3, 4), dtype=np.int)\n",
        "    if len(detected_xyz) == 0:\n",
        "        for tn in true_nodules:\n",
        "            confusion[2 if tn.isMal_bool else 1, 0] += 1\n",
        "    elif len(truth_xyz) == 0:\n",
        "        for dc in detected_classes:\n",
        "            confusion[0, dc] += 1\n",
        "    else:\n",
        "        normalized_dists = np.linalg.norm(truth_xyz[:, None] - detected_xyz[None], ord=2, axis=-1) / truth_diams[:, None]\n",
        "        matches = (normalized_dists < 0.7)\n",
        "        unmatched_detections = np.ones(len(detections), dtype=np.bool)\n",
        "        matched_true_nodules = np.zeros(len(true_nodules), dtype=np.int)\n",
        "        for i_tn, i_detection in zip(*matches.nonzero()):\n",
        "            matched_true_nodules[i_tn] = max(matched_true_nodules[i_tn], detected_classes[i_detection])\n",
        "            unmatched_detections[i_detection] = False\n",
        "\n",
        "        for ud, dc in zip(unmatched_detections, detected_classes):\n",
        "            if ud:\n",
        "                confusion[0, dc] += 1\n",
        "        for tn, dc in zip(true_nodules, matched_true_nodules):\n",
        "            confusion[2 if tn.isMal_bool else 1, dc] += 1\n",
        "    return confusion\n",
        "\n",
        "class NoduleAnalysisApp:\n",
        " def __init__(self,\n",
        "                 batch_size : int=4,\n",
        "                 num_workers : int=0,  ## if you want more workers first check this: https://stackoverflow.com/questions/71713719/runtimeerror-dataloader-worker-pids-15876-2756-exited-unexpectedly\n",
        "                 run_validation = False,\n",
        "                 include_train = False,\n",
        "                 segmentation_path = './data-unversioned/segmentation/models/segmentation/*.best.state',\n",
        "                 classification_path = './data-unversioned/classification/models/classification/*.best.state',\n",
        "                 malignancy_path = None,\n",
        "                 series_uid = None,\n",
        "                 augmented = False,\n",
        "                 augment_flip = False,\n",
        "                 augment_offset = False,\n",
        "                 augment_scale = False,\n",
        "                 augment_rotate = False,\n",
        "                 augment_noise = False,\n",
        "                 tb_prefix = 'end-to-end',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.run_validation = run_validation,\n",
        "        self.include_train = include_train,\n",
        "        self.segmentation_path = segmentation_path,\n",
        "        self.classification_path = classification_path,\n",
        "        self.malignancy_path = malignancy_path,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        self.series_uid = series_uid\n",
        "       \n",
        "        # self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
        "\n",
        "        if not (bool(self.series_uid) ^ self.run_validation):\n",
        "            raise Exception(\"One and only one of series_uid and --run_validation should be given\")\n",
        "\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        if not self.segmentation_path:\n",
        "            self.segmentation_path = self.initModelPath('seg')\n",
        "\n",
        "        if not self.classification_path:\n",
        "            self.classification_path = self.initModelPath('cls')\n",
        "\n",
        "        self.seg_model, self.cls_model, self.malignancy_model = self.initModels()\n",
        "\n",
        "        def initModelPath(self, type_str):\n",
        "            local_path = os.path.join(\n",
        "                'data-unversioned',\n",
        "                'part2',\n",
        "                'models',\n",
        "                'p2ch13',#self.tb_prefix,\n",
        "                type_str + '_{}_{}.{}.state'.format('*', '*', 'best'),\n",
        "            )\n",
        "\n",
        "            file_list = glob.glob(local_path)\n",
        "            if not file_list:\n",
        "                pretrained_path = os.path.join(\n",
        "                    'data',\n",
        "                    'part2',\n",
        "                    'models',\n",
        "                    type_str + '_{}_{}.{}.state'.format('*', '*', '*'),\n",
        "                )\n",
        "                file_list = glob.glob(pretrained_path)\n",
        "            else:\n",
        "                pretrained_path = None\n",
        "\n",
        "            file_list.sort()\n",
        "\n",
        "            try:\n",
        "                return file_list[-1]\n",
        "            except IndexError:\n",
        "                print([local_path, pretrained_path, file_list])\n",
        "                raise\n",
        "\n",
        "        def initModels(self):\n",
        "            print(self.segmentation_path)\n",
        "            seg_dict = torch.load(self.segmentation_path)\n",
        "\n",
        "            seg_model = UNetWrapper(\n",
        "                in_channels=7,\n",
        "                n_classes=1,\n",
        "                depth=3,\n",
        "                wf=4,\n",
        "                padding=True,\n",
        "                batch_norm=True,\n",
        "                up_mode='upconv',\n",
        "            )\n",
        "\n",
        "            seg_model.load_state_dict(seg_dict['model_state'])\n",
        "            seg_model.eval()\n",
        "\n",
        "            print(self.classification_path)\n",
        "            cls_dict = torch.load(self.classification_path)\n",
        "\n",
        "            model_cls = getattr(LunaModel, self.cls_model)\n",
        "            cls_model = model_cls()\n",
        "            cls_model.load_state_dict(cls_dict['model_state'])\n",
        "            cls_model.eval()\n",
        "\n",
        "            if self.use_cuda:\n",
        "                if torch.cuda.device_count() > 1:\n",
        "                    seg_model = nn.DataParallel(seg_model)\n",
        "                    cls_model = nn.DataParallel(cls_model)\n",
        "\n",
        "                seg_model.to(self.device)\n",
        "                cls_model.to(self.device)\n",
        "\n",
        "            if self.malignancy_path:\n",
        "                model_cls = getattr(LunaModel, self.malignancy_model)\n",
        "                malignancy_model = model_cls()\n",
        "                malignancy_dict = torch.load(self.malignancy_path)\n",
        "                malignancy_model.load_state_dict(malignancy_dict['model_state'])\n",
        "                malignancy_model.eval()\n",
        "                if self.use_cuda:\n",
        "                    malignancy_model.to(self.device)\n",
        "            else:\n",
        "                malignancy_model = None\n",
        "            return seg_model, cls_model, malignancy_model\n",
        "\n",
        "\n",
        "        def initSegmentationDl(self, series_uid):\n",
        "            seg_ds = Luna2dSegmentationDataset(\n",
        "                    contextSlices_count=3,\n",
        "                    series_uid=series_uid,\n",
        "                    fullCt_bool=True,\n",
        "                )\n",
        "            seg_dl = DataLoader(\n",
        "                seg_ds,\n",
        "                batch_size=self.batch_size * (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "                num_workers=self.num_workers,\n",
        "                pin_memory=self.use_cuda,\n",
        "            )\n",
        "\n",
        "            return seg_dl\n",
        "\n",
        "        def initClassificationDl(self, candidateInfo_list):\n",
        "            cls_ds = LunaDataset(\n",
        "                    sortby_str='series_uid',\n",
        "                    candidateInfo_list=candidateInfo_list,\n",
        "                )\n",
        "            cls_dl = DataLoader(\n",
        "                cls_ds,\n",
        "                batch_size=self.batch_size * (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "                num_workers=self.num_workers,\n",
        "                pin_memory=self.use_cuda,\n",
        "            )\n",
        "\n",
        "            return cls_dl\n",
        "\n",
        "\n",
        "        def main(self):\n",
        "            print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "            val_ds = LunaDataset(\n",
        "                val_stride=10,\n",
        "                isValSet_bool=True,\n",
        "            )\n",
        "            val_set = set(\n",
        "                candidateInfo_tup.series_uid\n",
        "                for candidateInfo_tup in val_ds.candidateInfo_list\n",
        "            )\n",
        "            positive_set = set(\n",
        "                candidateInfo_tup.series_uid\n",
        "                for candidateInfo_tup in getCandidateInfoList()\n",
        "                if candidateInfo_tup.isNodule_bool\n",
        "            )\n",
        "\n",
        "            if self.series_uid:\n",
        "                series_set = set(self.series_uid.split(','))\n",
        "            else:\n",
        "                series_set = set(\n",
        "                    candidateInfo_tup.series_uid\n",
        "                    for candidateInfo_tup in getCandidateInfoList()\n",
        "                )\n",
        "\n",
        "            if self.include_train:\n",
        "                train_list = sorted(series_set - val_set)\n",
        "            else:\n",
        "                train_list = []\n",
        "            val_list = sorted(series_set & val_set)\n",
        "\n",
        "\n",
        "            candidateInfo_dict = getCandidateInfoDict()\n",
        "            series_iter = enumerateWithEstimate(\n",
        "                val_list + train_list,\n",
        "                \"Series\",\n",
        "            )\n",
        "            all_confusion = np.zeros((3, 4), dtype=np.int)\n",
        "            for _, series_uid in series_iter:\n",
        "                ct = getCt(series_uid)\n",
        "                mask_a = self.segmentCt(ct, series_uid)\n",
        "\n",
        "                candidateInfo_list = self.groupSegmentationOutput(\n",
        "                    series_uid, ct, mask_a)\n",
        "                classifications_list = self.classifyCandidates(\n",
        "                    ct, candidateInfo_list)\n",
        "\n",
        "                if not self.run_validation:\n",
        "                    print(f\"found nodule candidates in {series_uid}:\")\n",
        "                    for prob, prob_mal, center_xyz, center_irc in classifications_list:\n",
        "                        if prob > 0.5:\n",
        "                            s = f\"nodule prob {prob:.3f}, \"\n",
        "                            if self.malignancy_model:\n",
        "                                s += f\"malignancy prob {prob_mal:.3f}, \"\n",
        "                            s += f\"center xyz {center_xyz}\"\n",
        "                            print(s)\n",
        "\n",
        "                if series_uid in candidateInfo_dict:\n",
        "                    one_confusion = match_and_score(\n",
        "                        classifications_list, candidateInfo_dict[series_uid]\n",
        "                    )\n",
        "                    all_confusion += one_confusion\n",
        "                    print_confusion(\n",
        "                        series_uid, one_confusion, self.malignancy_model is not None\n",
        "                    )\n",
        "\n",
        "            print_confusion(\n",
        "                \"Total\", all_confusion, self.malignancy_model is not None\n",
        "            )\n",
        "\n",
        "\n",
        "        def classifyCandidates(self, ct, candidateInfo_list):\n",
        "            cls_dl = self.initClassificationDl(candidateInfo_list)\n",
        "            classifications_list = []\n",
        "            for batch_ndx, batch_tup in enumerate(cls_dl):\n",
        "                input_t, _, _, series_list, center_list = batch_tup\n",
        "\n",
        "                input_g = input_t.to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    _, probability_nodule_g = self.cls_model(input_g)\n",
        "                    if self.malignancy_model is not None:\n",
        "                        _, probability_mal_g = self.malignancy_model(input_g)\n",
        "                    else:\n",
        "                        probability_mal_g = torch.zeros_like(probability_nodule_g)\n",
        "\n",
        "                zip_iter = zip(center_list,\n",
        "                    probability_nodule_g[:,1].tolist(),\n",
        "                    probability_mal_g[:,1].tolist())\n",
        "                for center_irc, prob_nodule, prob_mal in zip_iter:\n",
        "                    center_xyz = irc2xyz(center_irc,\n",
        "                        direction_a=ct.direction_a,\n",
        "                        origin_xyz=ct.origin_xyz,\n",
        "                        vxSize_xyz=ct.vxSize_xyz,\n",
        "                    )\n",
        "                    cls_tup = (prob_nodule, prob_mal, center_xyz, center_irc)\n",
        "                    classifications_list.append(cls_tup)\n",
        "            return classifications_list\n",
        "\n",
        "        def segmentCt(self, ct, series_uid):\n",
        "            with torch.no_grad():\n",
        "                output_a = np.zeros_like(ct.hu_a, dtype=np.float32)\n",
        "                seg_dl = self.initSegmentationDl(series_uid)  #  <3>\n",
        "                for input_t, _, _, slice_ndx_list in seg_dl:\n",
        "\n",
        "                    input_g = input_t.to(self.device)\n",
        "                    prediction_g = self.seg_model(input_g)\n",
        "\n",
        "                    for i, slice_ndx in enumerate(slice_ndx_list):\n",
        "                        output_a[slice_ndx] = prediction_g[i].cpu().numpy()\n",
        "\n",
        "                mask_a = output_a > 0.5\n",
        "                mask_a = morphology.binary_erosion(mask_a, iterations=1)\n",
        "\n",
        "            return mask_a\n",
        "\n",
        "        def groupSegmentationOutput(self, series_uid,  ct, clean_a):\n",
        "            candidateLabel_a, candidate_count = measurements.label(clean_a)\n",
        "            centerIrc_list = measurements.center_of_mass(\n",
        "                ct.hu_a.clip(-1000, 1000) + 1001,\n",
        "                labels=candidateLabel_a,\n",
        "                index=np.arange(1, candidate_count+1),\n",
        "            )\n",
        "\n",
        "            candidateInfo_list = []\n",
        "            for i, center_irc in enumerate(centerIrc_list):\n",
        "                center_xyz = irc2xyz(\n",
        "                    center_irc,\n",
        "                    ct.origin_xyz,\n",
        "                    ct.vxSize_xyz,\n",
        "                    ct.direction_a,\n",
        "                )\n",
        "                assert np.all(np.isfinite(center_irc)), repr(['irc', center_irc, i, candidate_count])\n",
        "                assert np.all(np.isfinite(center_xyz)), repr(['xyz', center_xyz])\n",
        "                candidateInfo_tup = \\\n",
        "                    CandidateInfoTuple(False, False, False, 0.0, series_uid, center_xyz)\n",
        "                candidateInfo_list.append(candidateInfo_tup)\n",
        "\n",
        "            return candidateInfo_list\n",
        "\n",
        "        def logResults(self, mode_str, filtered_list, series2diagnosis_dict, positive_set):\n",
        "            count_dict = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}\n",
        "            for series_uid in filtered_list:\n",
        "                probablity_float, center_irc = series2diagnosis_dict.get(series_uid, (0.0, None))\n",
        "                if center_irc is not None:\n",
        "                    center_irc = tuple(int(x.item()) for x in center_irc)\n",
        "                positive_bool = series_uid in positive_set\n",
        "                prediction_bool = probablity_float > 0.5\n",
        "                correct_bool = positive_bool == prediction_bool\n",
        "\n",
        "                if positive_bool and prediction_bool:\n",
        "                    count_dict['tp'] += 1\n",
        "                if not positive_bool and not prediction_bool:\n",
        "                    count_dict['tn'] += 1\n",
        "                if not positive_bool and prediction_bool:\n",
        "                    count_dict['fp'] += 1\n",
        "                if positive_bool and not prediction_bool:\n",
        "                    count_dict['fn'] += 1\n",
        "\n",
        "\n",
        "                print(\"{} {} Label:{!r:5} Pred:{!r:5} Correct?:{!r:5} Value:{:.4f} {}\".format(\n",
        "                    mode_str,\n",
        "                    series_uid,\n",
        "                    positive_bool,\n",
        "                    prediction_bool,\n",
        "                    correct_bool,\n",
        "                    probablity_float,\n",
        "                    center_irc,\n",
        "                ))\n",
        "\n",
        "            total_count = sum(count_dict.values())\n",
        "            percent_dict = {k: v / (total_count or 1) * 100 for k, v in count_dict.items()}\n",
        "\n",
        "            precision = percent_dict['p'] = count_dict['tp'] / ((count_dict['tp'] + count_dict['fp']) or 1)\n",
        "            recall    = percent_dict['r'] = count_dict['tp'] / ((count_dict['tp'] + count_dict['fn']) or 1)\n",
        "            percent_dict['f1'] = 2 * (precision * recall) / ((precision + recall) or 1)\n",
        "\n",
        "            print(mode_str + \" tp:{tp:.1f}%, tn:{tn:.1f}%, fp:{fp:.1f}%, fn:{fn:.1f}%\".format(\n",
        "                **percent_dict,\n",
        "            ))\n",
        "            print(mode_str + \" precision:{p:.3f}, recall:{r:.3f}, F1:{f1:.3f}\".format(\n",
        "                **percent_dict,\n",
        "            ))\n",
        "\n",
        "\n",
        "nodule_analysis = NoduleAnalysisApp()\n",
        "nodule_analysis.main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wD7DjQhg4CVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title finetunning - retraining the classification model for detection on malignant nodules\n",
        "\n",
        "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_PRED_P_NDX=2\n",
        "METRICS_LOSS_NDX=3\n",
        "METRICS_SIZE = 4\n",
        "\n",
        "class ClassificationTrainingApp:\n",
        "    def __init__(self,\n",
        "                 batch_size : int=24,\n",
        "                 num_workers : int=2,\n",
        "                 epochs: int=1,\n",
        "                 dataset = LunaDataset,\n",
        "                 model = LunaModel,\n",
        "                 malignant = False,\n",
        "                 finetune =  '',\n",
        "                 finetune_depth : int= 1,\n",
        "                 tb_prefix = 'finetune',\n",
        "                 ):\n",
        "\n",
        "        self.batch_size = batch_size,\n",
        "        self.num_workers = num_workers,\n",
        "        self.epochs = epochs,\n",
        "        self.dataset = dataset,\n",
        "        self.model = model,\n",
        "        self.malignant = malignant,\n",
        "        self.finetune = finetune,\n",
        "        self.finetune_depth = finetune_depth,\n",
        "        self.tb_prefix = tb_prefix,\n",
        "        \n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.totalTrainingSamples_count = 0\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if True:\n",
        "        # if self.augmented or self.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        # if self.augmented or self.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.1\n",
        "        # if self.augmented or self.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        # if self.augmented or self.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        # if self.augmented or self.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        model_cls = getattr(LunaModel, self.model)\n",
        "        model = model_cls()\n",
        "\n",
        "        if self.finetune:\n",
        "            d = torch.load(self.finetune, map_location='cpu')\n",
        "            model_blocks = [\n",
        "                n for n, subm in model.named_children()\n",
        "                if len(list(subm.parameters())) > 0\n",
        "            ]\n",
        "            finetune_blocks = model_blocks[-self.finetune_depth:]\n",
        "            print(f\"finetuning from {self.finetune}, blocks {' '.join(finetune_blocks)}\")\n",
        "            model.load_state_dict(\n",
        "                {\n",
        "                    k: v for k,v in d['model_state'].items()\n",
        "                    if k.split('.')[0] not in model_blocks[-1]\n",
        "                },\n",
        "                strict=False,\n",
        "            )\n",
        "            for n, p in model.named_parameters():\n",
        "                if n.split('.')[0] not in finetune_blocks:\n",
        "                    p.requires_grad_(False)\n",
        "        if self.use_cuda:\n",
        "            print(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                model = nn.DataParallel(model)\n",
        "            model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        lr = 0.003 if self.finetune else 0.001\n",
        "        return SGD(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        #return Adam(self.model.parameters(), lr=3e-4)\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        ds_cls = getattr(LunaDataset, self.dataset)\n",
        "\n",
        "        train_ds = ds_cls(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int=1,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        ds_cls = getattr(LunaDataset, self.dataset)\n",
        "\n",
        "        val_ds = ds_cls(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "        )\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('runs', self.tb_prefix,\n",
        "                                   self.time_str)\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-trn_cls-')\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-val_cls-')\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Starting {}, {}\".format(type(self).__name__, self))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()\n",
        "\n",
        "        best_score = 0.0\n",
        "        validation_cadence = 5 if not self.finetune else 1\n",
        "        for epoch_ndx in range(1, self.epochs + 1):\n",
        "\n",
        "            print(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
        "                epoch_ndx,\n",
        "                self.epochs,\n",
        "                len(train_dl),\n",
        "                len(val_dl),\n",
        "                self.batch_size,\n",
        "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
        "            ))\n",
        "\n",
        "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "            if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
        "                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "                best_score = max(score, best_score)\n",
        "\n",
        "                # TODO: this 'cls' will need to change for the malignant classifier\n",
        "                self.saveModel('cls', epoch_ndx, score == best_score)\n",
        "\n",
        "\n",
        "        if hasattr(self, 'trn_writer'):\n",
        "            self.trn_writer.close()\n",
        "            self.val_writer.close()\n",
        "\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g,\n",
        "                augment=True\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "    def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            valMetrics_g = torch.zeros(\n",
        "                METRICS_SIZE,\n",
        "                len(val_dl.dataset),\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(\n",
        "                    batch_ndx,\n",
        "                    batch_tup,\n",
        "                    val_dl.batch_size,\n",
        "                    valMetrics_g,\n",
        "                    augment=False\n",
        "                )\n",
        "\n",
        "        return valMetrics_g.to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
        "                         augment=True):\n",
        "        input_t, label_t, index_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "        index_g = index_t.to(self.device, non_blocking=True)\n",
        "\n",
        "\n",
        "        if augment:\n",
        "            input_g = augment3d(input_g)\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "        loss_g = nn.functional.cross_entropy(logits_g, label_g[:, 1],\n",
        "                                             reduction=\"none\")\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        _, predLabel_g = torch.max(probability_g, dim=1, keepdim=False,\n",
        "                                   out=None)\n",
        "\n",
        "        # print(index_g)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = index_g\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predLabel_g\n",
        "        # metrics_g[METRICS_PRED_N_NDX, start_ndx:end_ndx] = probability_g[:,0]\n",
        "        metrics_g[METRICS_PRED_P_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "        # metrics_g[METRICS_PRED_M_NDX, start_ndx:end_ndx] = probability_g[:,2]\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "        return loss_g.mean()\n",
        "\n",
        "\n",
        "    def logMetrics(\n",
        "            self,\n",
        "            epoch_ndx,\n",
        "            mode_str,\n",
        "            metrics_t,\n",
        "            classificationThreshold=0.5,\n",
        "    ):\n",
        "        self.initTensorboardWriters()\n",
        "        print(\"E{} {}\".format(\n",
        "            epoch_ndx,\n",
        "            type(self).__name__,\n",
        "        ))\n",
        "\n",
        "        if self.dataset == 'MalignantLunaDataset':\n",
        "            pos = 'mal'\n",
        "            neg = 'ben'\n",
        "        else:\n",
        "            pos = 'pos'\n",
        "            neg = 'neg'\n",
        "\n",
        "\n",
        "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] == 0\n",
        "        negPred_mask = metrics_t[METRICS_PRED_NDX] == 0\n",
        "\n",
        "        posLabel_mask = ~negLabel_mask\n",
        "        posPred_mask = ~negPred_mask\n",
        "\n",
        "        # benLabel_mask = metrics_t[METRICS_LABEL_NDX] == 1\n",
        "        # benPred_mask = metrics_t[METRICS_PRED_NDX] == 1\n",
        "        #\n",
        "        # malLabel_mask = metrics_t[METRICS_LABEL_NDX] == 2\n",
        "        # malPred_mask = metrics_t[METRICS_PRED_NDX] == 2\n",
        "\n",
        "        # benLabel_mask = ~malLabel_mask & posLabel_mask\n",
        "        # benPred_mask = ~malPred_mask & posLabel_mask\n",
        "\n",
        "        neg_count = int(negLabel_mask.sum())\n",
        "        pos_count = int(posLabel_mask.sum())\n",
        "        # ben_count = int(benLabel_mask.sum())\n",
        "        # mal_count = int(malLabel_mask.sum())\n",
        "\n",
        "        neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
        "        pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
        "        # ben_correct = int((benLabel_mask & benPred_mask).sum())\n",
        "        # mal_correct = int((malLabel_mask & malPred_mask).sum())\n",
        "\n",
        "        trueNeg_count = neg_correct\n",
        "        truePos_count = pos_correct\n",
        "\n",
        "        falsePos_count = neg_count - neg_correct\n",
        "        falseNeg_count = pos_count - pos_correct\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
        "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
        "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
        "        # metrics_dict['loss/ben'] = metrics_t[METRICS_LOSS_NDX, benLabel_mask].mean()\n",
        "        # metrics_dict['loss/mal'] = metrics_t[METRICS_LOSS_NDX, malLabel_mask].mean()\n",
        "\n",
        "        metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n",
        "        metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n",
        "        metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n",
        "        # metrics_dict['correct/ben'] = (ben_correct) / ben_count * 100\n",
        "        # metrics_dict['correct/mal'] = (mal_correct) / mal_count * 100\n",
        "\n",
        "        precision = metrics_dict['pr/precision'] = \\\n",
        "            truePos_count / np.float64(truePos_count + falsePos_count)\n",
        "        recall    = metrics_dict['pr/recall'] = \\\n",
        "            truePos_count / np.float64(truePos_count + falseNeg_count)\n",
        "\n",
        "        metrics_dict['pr/f1_score'] = \\\n",
        "            2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        threshold = torch.linspace(1, 0)\n",
        "        tpr = (metrics_t[None, METRICS_PRED_P_NDX, posLabel_mask] >= threshold[:, None]).sum(1).float() / pos_count\n",
        "        fpr = (metrics_t[None, METRICS_PRED_P_NDX, negLabel_mask] >= threshold[:, None]).sum(1).float() / neg_count\n",
        "        fp_diff = fpr[1:]-fpr[:-1]\n",
        "        tp_avg  = (tpr[1:]+tpr[:-1])/2\n",
        "        auc = (fp_diff * tp_avg).sum()\n",
        "        metrics_dict['auc'] = auc\n",
        "\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
        "                 + \"{correct/all:-5.1f}% correct, \"\n",
        "                 + \"{pr/precision:.4f} precision, \"\n",
        "                 + \"{pr/recall:.4f} recall, \"\n",
        "                 + \"{pr/f1_score:.4f} f1 score, \"\n",
        "                 + \"{auc:.4f} auc\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
        "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_' + neg,\n",
        "                neg_correct=neg_correct,\n",
        "                neg_count=neg_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
        "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_' + pos,\n",
        "                pos_correct=pos_correct,\n",
        "                pos_count=pos_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "        # print(\n",
        "        #     (\"E{} {:8} {loss/ben:.4f} loss, \"\n",
        "        #          + \"{correct/ben:-5.1f}% correct ({ben_correct:} of {ben_count:})\"\n",
        "        #     ).format(\n",
        "        #         epoch_ndx,\n",
        "        #         mode_str + '_ben',\n",
        "        #         ben_correct=ben_correct,\n",
        "        #         ben_count=ben_count,\n",
        "        #         **metrics_dict,\n",
        "        #     )\n",
        "        # )\n",
        "        # print(\n",
        "        #     (\"E{} {:8} {loss/mal:.4f} loss, \"\n",
        "        #          + \"{correct/mal:-5.1f}% correct ({mal_correct:} of {mal_count:})\"\n",
        "        #     ).format(\n",
        "        #         epoch_ndx,\n",
        "        #         mode_str + '_mal',\n",
        "        #         mal_correct=mal_correct,\n",
        "        #         mal_count=mal_count,\n",
        "        #         **metrics_dict,\n",
        "        #     )\n",
        "        # )\n",
        "        writer = getattr(self, mode_str + '_writer')\n",
        "\n",
        "        for key, value in metrics_dict.items():\n",
        "            key = key.replace('pos', pos)\n",
        "            key = key.replace('neg', neg)\n",
        "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
        "\n",
        "        fig = plt.figure()\n",
        "        plt.plot(fpr, tpr)\n",
        "        writer.add_figure('roc', fig, self.totalTrainingSamples_count)\n",
        "\n",
        "        writer.add_scalar('auc', auc, self.totalTrainingSamples_count)\n",
        "# # tag::logMetrics_writer_prcurve[]\n",
        "#        writer.add_pr_curve(\n",
        "#            'pr',\n",
        "#            metrics_t[METRICS_LABEL_NDX],\n",
        "#            metrics_t[METRICS_PRED_P_NDX],\n",
        "#            self.totalTrainingSamples_count,\n",
        "#        )\n",
        "# # end::logMetrics_writer_prcurve[]\n",
        "\n",
        "        bins = np.linspace(0, 1)\n",
        "\n",
        "        writer.add_histogram(\n",
        "            'label_neg',\n",
        "            metrics_t[METRICS_PRED_P_NDX, negLabel_mask],\n",
        "            self.totalTrainingSamples_count,\n",
        "            bins=bins\n",
        "        )\n",
        "        writer.add_histogram(\n",
        "            'label_pos',\n",
        "            metrics_t[METRICS_PRED_P_NDX, posLabel_mask],\n",
        "            self.totalTrainingSamples_count,\n",
        "            bins=bins\n",
        "        )\n",
        "\n",
        "        if not self.malignant:\n",
        "            score = metrics_dict['pr/f1_score']\n",
        "        else:\n",
        "            score = metrics_dict['auc']\n",
        "\n",
        "        return score\n",
        "\n",
        "    def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
        "        file_path = os.path.join(\n",
        "            'data-unversioned',\n",
        "            'part2',\n",
        "            'models',\n",
        "            self.tb_prefix,\n",
        "            '{}_{}.{}.state'.format(\n",
        "                type_str,\n",
        "                self.time_str,\n",
        "                \n",
        "                self.totalTrainingSamples_count,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
        "\n",
        "        model = self.model\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model = model.module\n",
        "\n",
        "        state = {\n",
        "            'model_state': model.state_dict(),\n",
        "            'model_name': type(model).__name__,\n",
        "            'optimizer_state' : self.optimizer.state_dict(),\n",
        "            'optimizer_name': type(self.optimizer).__name__,\n",
        "            'epoch': epoch_ndx,\n",
        "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
        "        }\n",
        "        torch.save(state, file_path)\n",
        "\n",
        "        print(\"Saved model params to {}\".format(file_path))\n",
        "\n",
        "        if isBest:\n",
        "            best_path = os.path.join(\n",
        "                'data-unversioned',\n",
        "                'part2',\n",
        "                'models',\n",
        "                self.tb_prefix,\n",
        "                '{}_{}.{}.state'.format(\n",
        "                    type_str,\n",
        "                    self.time_str,\n",
        "                    'best',\n",
        "                )\n",
        "            )\n",
        "            shutil.copyfile(file_path, best_path)\n",
        "\n",
        "            print(\"Saved model params to {}\".format(best_path))\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            print(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n",
        "\n",
        "    # def logModelMetrics(self, model):\n",
        "    #     writer = getattr(self, 'trn_writer')\n",
        "    #\n",
        "    #     model = getattr(model, 'module', model)\n",
        "    #\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         if param.requires_grad:\n",
        "    #             min_data = float(param.data.min())\n",
        "    #             max_data = float(param.data.max())\n",
        "    #             max_extent = max(abs(min_data), abs(max_data))\n",
        "    #\n",
        "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
        "    #\n",
        "    #             try:\n",
        "    #                 writer.add_histogram(\n",
        "    #                     name.rsplit('.', 1)[-1] + '/' + name,\n",
        "    #                     param.data.cpu().numpy(),\n",
        "    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
        "    #                     self.totalTrainingSamples_count,\n",
        "    #                     # bins=bins,\n",
        "    #                 )\n",
        "    #             except Exception as e:\n",
        "    #                 log.error([min_data, max_data])\n",
        "    #                 raise\n",
        "\n",
        "\n",
        "finetuning = ClassificationTrainingApp()\n",
        "finetuning.main()"
      ],
      "metadata": {
        "id": "tk7k9zs1jnMN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title malignant and benign evaluation\n",
        "\n",
        "ds = MalignantLunaDataset(val_stride=10, isValSet_bool=True)  # <1>\n",
        "nodules = ds.ben_list + ds.mal_list\n",
        "is_mal = torch.tensor([n.isMal_bool for n in nodules])  # <2>\n",
        "diam  = torch.tensor([n.diameter_mm for n in nodules])\n",
        "num_mal = is_mal.sum()  # <3>\n",
        "num_ben = len(is_mal) - num_mal\n",
        "\n",
        "threshold = torch.linspace(diam.max(), diam.min())\n",
        "predictions = (diam[None] >= threshold[:, None])  # <1>\n",
        "tp_diam = (predictions & is_mal[None]).sum(1).float() / num_mal  # <2>\n",
        "fp_diam = (predictions & ~is_mal[None]).sum(1).float() / num_ben\n",
        "\n",
        "fp_diam_diff =  fp_diam[1:] - fp_diam[:-1]\n",
        "tp_diam_avg  = (tp_diam[1:] + tp_diam[:-1])/2\n",
        "auc_diam = (fp_diam_diff * tp_diam_avg).sum()\n",
        "\n",
        "fp_fill = torch.ones((fp_diam.shape[0] + 1,))\n",
        "fp_fill[:-1] = fp_diam\n",
        "\n",
        "tp_fill = torch.zeros((tp_diam.shape[0] + 1,))\n",
        "tp_fill[:-1] = tp_diam\n",
        "\n",
        "print(threshold)\n",
        "print(fp_diam)\n",
        "print(tp_diam)\n",
        "\n",
        "for i in range(threshold.shape[0]):\n",
        "    print(i, threshold[i], fp_diam[i], tp_diam[i])\n",
        "\n",
        "plt.figure(figsize=(7,5), dpi=1200)\n",
        "for i in [62, 88]:\n",
        "    plt.scatter(fp_diam[i], tp_diam[i], color='red')\n",
        "    print(f'diam: {round(threshold[i].item(), 2)}, x: {round(fp_diam[i].item(), 2)}, y: {round(tp_diam[i].item(), 2)}')\n",
        "plt.fill(fp_fill, tp_fill, facecolor='#0077bb', alpha=0.25)\n",
        "plt.plot(fp_diam, tp_diam, label=f'diameter baseline, AUC={auc_diam:.3f}')\n",
        "plt.title(f'ROC diameter baseline, AUC={auc_diam:.3f}')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.xlabel('false positive rate')\n",
        "plt.savefig('roc_diameter_baseline.png')\n",
        "\n",
        "##-------------------- new cell ------------------------\n",
        "\n",
        "model = LunaModel()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sd = torch.load('./data-unversioned/classification/models/classification/*.best.state')\n",
        "model.load_state_dict(sd['model_state'])\n",
        "model.to(device)\n",
        "model.eval();\n",
        "\n",
        "ds = MalignantLunaDataset(val_stride=10, isValSet_bool=True)\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size=32, num_workers=4)\n",
        "\n",
        "preds = []\n",
        "truth = []\n",
        "for inp, label, _, _, _ in dl:\n",
        "    inp = inp.to(device)    \n",
        "    truth += (label[:,1]>0).tolist()\n",
        "    with torch.no_grad():\n",
        "        _, p = model(inp)\n",
        "        preds += p[:, 1].tolist()\n",
        "truth = torch.tensor(truth)\n",
        "preds = torch.tensor(preds)\n",
        "\n",
        "num_mal = truth.sum()\n",
        "num_ben = len(truth) - num_mal\n",
        "threshold = torch.linspace(1, 0)\n",
        "tp_finetune = ((preds[None] >= threshold[:, None]) & truth[None]).sum(1).float() / num_mal\n",
        "fp_finetune = ((preds[None] >= threshold[:, None]) & ~truth[None]).sum(1).float() / num_ben\n",
        "fp_finetune_diff = fp_finetune[1:]-fp_finetune[:-1]\n",
        "tp_finetune_avg  = (tp_finetune[1:]+tp_finetune[:-1])/2\n",
        "auc_finetune = (fp_finetune_diff * tp_finetune_avg).sum()\n",
        "\n",
        "plt.figure(figsize=(7,5), dpi=300)\n",
        "plt.fill(fp_fill, tp_fill, facecolor='#0077bb', alpha=0.25)\n",
        "plt.plot(fp_diam, tp_diam, label=f'diameter baseline, AUC={auc_diam:.3f}')\n",
        "plt.plot(fp_finetune, tp_finetune, label=f'1 layer fine-tuned, AUC={auc_finetune:.3f}')\n",
        "plt.legend()\n",
        "plt.savefig('roc_finetune.png')\n",
        "\n",
        "##------------------------ new cell ---------------------------\n",
        "\n",
        "if 1:\n",
        "    fn = './data-unversioned/classification/models/classification/*.best.state'\n",
        "    model = LunaModel()\n",
        "    sd = torch.load(fn, map_location='cpu')['model_state']\n",
        "    model.load_state_dict(sd)\n",
        "    model.to(device)\n",
        "    model.eval();\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "truth = []\n",
        "for inp, label, _, _, _ in dl:\n",
        "    inp = inp.to(device)    \n",
        "    truth += (label[:,1]>0).tolist()\n",
        "    with torch.no_grad():\n",
        "        _, p = model(inp)\n",
        "        preds += p[:, 1].tolist()\n",
        "truth = torch.tensor(truth)\n",
        "preds = torch.tensor(preds)\n",
        "\n",
        "num_mal = truth.sum()\n",
        "num_ben = len(truth) - num_mal\n",
        "threshold = torch.linspace(1, 0)\n",
        "tp = ((preds[None] >= threshold[:, None]) & truth[None]).sum(1).float() / num_mal\n",
        "fp = ((preds[None] >= threshold[:, None]) & ~truth[None]).sum(1).float() / num_ben\n",
        "\n",
        "fp_diff = fp[1:]-fp[:-1]\n",
        "tp_avg  = (tp[1:]+tp[:-1])/2\n",
        "auc_modified = (fp_diff * tp_avg).sum()\n",
        "\n",
        "plt.figure(figsize=(7,5), dpi=300)\n",
        "plt.fill(fp_fill, tp_fill, facecolor='#0077bb', alpha=0.25)\n",
        "plt.plot(fp_diam, tp_diam, label=f'diameter baseline, AUC={auc_diam:.3f}')\n",
        "plt.plot(fp_finetune, tp_finetune, label=f'1 layer fine-tuned, AUC={auc_finetune:.3f}')\n",
        "plt.plot(fp, tp, label=f'2 layers fine-tuned, AUC={auc_modified:.3f}')\n",
        "plt.legend()\n",
        "plt.savefig('roc_modified.png')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Uvnh2wOPhVbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}