{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmB-SwhGflKC"
      },
      "source": [
        "\n",
        "R E F E R E N C E\n",
        "\n",
        "CenterNet detects the center of the objects and finds everithing else (size or offsets of the object) based on regression (between ground truth and predicted heatmaps-created via gaussian).\n",
        "But downside is also caused by this feature. Downside is that we cannot really make very accurate instance segmentations.\n",
        "\n",
        "Paper : https://arxiv.org/pdf/1904.07850.pdf\n",
        "\n",
        "Code base : https://github.com/xingyizhou/CenterNet\n",
        "\n",
        "Data : https://www.kaggle.com/competitions/pku-autonomous-driving/data\n",
        "\n",
        "Competitions : https://www.kaggle.com/code/hocop1/centernet-baseline/notebook#3D-Visualization\n",
        "\n",
        "Consize explanation here : https://www.youtube.com/watch?v=pDqoJNJfEZo\n",
        "One correction in this video is that centernet directly finds center of\n",
        "3D object rather to find first the center of 2D and then go to 3D.\n",
        "\n",
        "Authors explaining it : https://www.youtube.com/watch?v=9vM6E6zoA84 \n",
        "\n",
        "Untill now we learned yolo and centernet. Also read cascade R-CNN, that\n",
        "improves consists of a sequence of detectors trained end-to-end with increasing IoU thresholds, to be sequentially more selective against close false positives. The output of a previous stage detector is forwarded to a later stage detector, and the detection results will be improved stage by stage.\n",
        "But as u know inference time is problem with the R-CNN: \n",
        "\n",
        "https://arxiv.org/abs/1712.00726 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "dZlkdlmoSVbI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\medas\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "WARNING: You are using pip version 21.2.4; however, version 22.2.2 is available.\n",
            "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Collecting torch\n",
            "  Downloading torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
            "Using legacy 'setup.py install' for efficientnet-pytorch, since package 'wheel' is not installed.\n",
            "Installing collected packages: typing-extensions, torch, efficientnet-pytorch\n",
            "    Running setup.py install for efficientnet-pytorch: started\n",
            "    Running setup.py install for efficientnet-pytorch: finished with status 'done'\n",
            "Successfully installed efficientnet-pytorch-0.7.1 torch-1.12.1 typing-extensions-4.3.0\n"
          ]
        }
      ],
      "source": [
        "#@title installs and imports\n",
        "\n",
        "!pip install efficientnet-pytorch\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "# read this: https://pytorch.org/hub/nvidia_deeplearningexamples_efficientnet/\n",
        "# and this: https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n",
        "# quick eplanation here : https://www.youtube.com/watch?v=XiLy_2clr6s\n",
        "# also read the paper: https://arxiv.org/abs/1905.11946  \n",
        "\n",
        "!pip install livelossplot --quiet\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "from tqdm import tqdm#_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "import os\n",
        "from scipy.optimize import minimize\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from math import sin, cos\n",
        "import gc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v_3Yhig2W7Uq"
      },
      "outputs": [],
      "source": [
        "#@title downloads\n",
        "\n",
        "#please note: you have to get your kaggle.json contining API key and upload it in colab files section\n",
        "# here is step by step explanation how to do that: \n",
        "# https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "!pip install -q kaggle  \n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/   # you may not need this copying the json file to newly created directory if you uploaded it already in that directory, obviously\n",
        "!kaggle competitions download -c pku-autonomous-driving\n",
        "\n",
        "!unzip /content/pku-autonomous-driving.zip\n",
        "# !rm /content/pku-autonomous-driving.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1BFpg8l9JfV"
      },
      "source": [
        "# Data handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDIN8A7mVcUd"
      },
      "outputs": [],
      "source": [
        "PATH = './'\n",
        "os.listdir(PATH)\n",
        "\n",
        "train = pd.read_csv(PATH + 'train.csv')\n",
        "test = pd.read_csv(PATH + 'sample_submission.csv')\n",
        "\n",
        "# From camera.zip\n",
        "camera_matrix = np.array([[2304.5479, 0,  1686.2379],\n",
        "                          [0, 2305.8757, 1354.9849],\n",
        "                          [0, 0, 1]], dtype=np.float32)\n",
        "camera_matrix_inv = np.linalg.inv(camera_matrix)\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q_A1XzYIVsub"
      },
      "outputs": [],
      "source": [
        "#@title helper functions\n",
        "\n",
        "def imread(path, fast_mode=False):\n",
        "    img = cv2.imread(path)\n",
        "    if not fast_mode and img is not None and len(img.shape) == 3:\n",
        "        img = np.array(img[:, :, ::-1])\n",
        "    return img\n",
        "\n",
        "def rotate(x, angle):\n",
        "    x = x + angle\n",
        "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
        "    return x\n",
        "\n",
        "def str2coords(s, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n",
        "    '''\n",
        "    Input:\n",
        "        s: PredictionString (e.g. from train dataframe)\n",
        "        names: array of what to extract from the string\n",
        "    Output:\n",
        "        list of dicts with keys from `names`\n",
        "    '''\n",
        "    coords = []\n",
        "    for l in np.array(s.split()).reshape([-1, 7]):\n",
        "        coords.append(dict(zip(names, l.astype('float'))))\n",
        "        if 'id' in coords[-1]:\n",
        "            coords[-1]['id'] = int(coords[-1]['id'])\n",
        "    return coords\n",
        "\n",
        "def coords2str(coords, names=['yaw', 'pitch', 'roll', 'x', 'y', 'z', 'confidence']):\n",
        "    s = []\n",
        "    for c in coords:\n",
        "        for n in names:\n",
        "            s.append(str(c.get(n, 0)))\n",
        "    return ' '.join(s)\n",
        "    \n",
        "\n",
        "###----------------function for 2D visualizations start here--------------------\n",
        "\n",
        "def get_img_coords(s):\n",
        "    '''\n",
        "    Input is a PredictionString (e.g. from train dataframe)\n",
        "    Output is two arrays:\n",
        "        xs: x coordinates in the image (row)\n",
        "        ys: y coordinates in the image (column)\n",
        "    '''\n",
        "    coords = str2coords(s)\n",
        "    xs = [c['x'] for c in coords]\n",
        "    ys = [c['y'] for c in coords]\n",
        "    zs = [c['z'] for c in coords]\n",
        "    P = np.array(list(zip(xs, ys, zs))).T\n",
        "    img_p = np.dot(camera_matrix, P).T\n",
        "    img_p[:, 0] /= img_p[:, 2]\n",
        "    img_p[:, 1] /= img_p[:, 2]\n",
        "    img_xs = img_p[:, 0]\n",
        "    img_ys = img_p[:, 1]\n",
        "    img_zs = img_p[:, 2] # z = Distance from the camera\n",
        "    return img_xs, img_ys\n",
        "\n",
        "\n",
        "###--------------- functions for 3D visualizations stert here-------------------\n",
        "\n",
        "def draw_line(image, points):\n",
        "    color = (255, 0, 0)\n",
        "    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_points(image, points):\n",
        "    for (p_x, p_y, p_z) in points:\n",
        "        cv2.circle(image, (p_x, p_y), int(1000 / p_z), (0, 255, 0), -1)\n",
        "#         if p_x > image.shape[1] or p_y > image.shape[0]:\n",
        "#             print('Point', p_x, p_y, 'is out of image with shape', image.shape)\n",
        "    return image\n",
        "\n",
        "# convert euler angle to rotation matrix\n",
        "def euler_to_Rot(yaw, pitch, roll):\n",
        "    Y = np.array([[cos(yaw), 0, sin(yaw)],\n",
        "                  [0, 1, 0],\n",
        "                  [-sin(yaw), 0, cos(yaw)]])\n",
        "    P = np.array([[1, 0, 0],\n",
        "                  [0, cos(pitch), -sin(pitch)],\n",
        "                  [0, sin(pitch), cos(pitch)]])\n",
        "    R = np.array([[cos(roll), -sin(roll), 0],\n",
        "                  [sin(roll), cos(roll), 0],\n",
        "                  [0, 0, 1]])\n",
        "    return np.dot(Y, np.dot(P, R))\n",
        "\n",
        "def visualize(img, coords):\n",
        "    # You will also need functions from the previous cells\n",
        "    x_l = 1.02\n",
        "    y_l = 0.80\n",
        "    z_l = 2.31\n",
        "    \n",
        "    img = img.copy()\n",
        "    for point in coords:\n",
        "        # Get values\n",
        "        x, y, z = point['x'], point['y'], point['z']\n",
        "        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n",
        "        # Math\n",
        "        Rt = np.eye(4)\n",
        "        t = np.array([x, y, z])\n",
        "        Rt[:3, 3] = t\n",
        "        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n",
        "        Rt = Rt[:3, :]\n",
        "        P = np.array([[x_l, -y_l, -z_l, 1],\n",
        "                      [x_l, -y_l, z_l, 1],\n",
        "                      [-x_l, -y_l, z_l, 1],\n",
        "                      [-x_l, -y_l, -z_l, 1],\n",
        "                      [0, 0, 0, 1]]).T\n",
        "        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n",
        "        img_cor_points = img_cor_points.T\n",
        "        img_cor_points[:, 0] /= img_cor_points[:, 2]\n",
        "        img_cor_points[:, 1] /= img_cor_points[:, 2]\n",
        "        img_cor_points = img_cor_points.astype(int)\n",
        "        # Drawing\n",
        "        img = draw_line(img, img_cor_points)\n",
        "        img = draw_points(img, img_cor_points[-1:])\n",
        "    \n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATOBQxdbVrtz"
      },
      "outputs": [],
      "source": [
        "### check if we get images\n",
        "\n",
        "img = imread(PATH + 'train_images/ID_0a8994081' + '.jpg') \n",
        "IMG_SHAPE = img.shape\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(img);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WECA2R-yWKTt"
      },
      "outputs": [],
      "source": [
        "### check pose information and fit linear regression line to get more insights\n",
        "\n",
        "points_df = pd.DataFrame()\n",
        "for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n",
        "    arr = []\n",
        "    for ps in train['PredictionString']:\n",
        "        coords = str2coords(ps)\n",
        "        arr += [c[col] for c in coords]\n",
        "    points_df[col] = arr\n",
        "\n",
        "\n",
        "zy_slope = LinearRegression()\n",
        "X = points_df[['z']]\n",
        "y = points_df['y']\n",
        "zy_slope.fit(X, y)\n",
        "print('MAE without x:', mean_absolute_error(y, zy_slope.predict(X)))\n",
        "\n",
        "# Will use this model later\n",
        "xzy_slope = LinearRegression()\n",
        "X = points_df[['x', 'z']]\n",
        "y = points_df['y']\n",
        "xzy_slope.fit(X, y)\n",
        "\n",
        "print('MAE with x:', mean_absolute_error(y, xzy_slope.predict(X)))\n",
        "print('\\ndy/dx = {:.3f}\\ndy/dz = {:.3f}'.format(*xzy_slope.coef_))\n",
        "\n",
        "print('len(points_df)', len(points_df))\n",
        "points_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "plt.xlim(0,500)\n",
        "plt.ylim(0,100)\n",
        "plt.scatter(points_df['z'], points_df['y'], label='Real points')\n",
        "X_line = np.linspace(0,500, 10)\n",
        "plt.plot(X_line, zy_slope.predict(X_line.reshape(-1, 1)), color='orange', label='Regression')\n",
        "plt.legend()\n",
        "plt.xlabel('z coordinate')\n",
        "plt.ylabel('y coordinate');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MwD0XlFqZVss"
      },
      "outputs": [],
      "source": [
        "#@title from 3D info to 2D image masks (prepare for regression) and image preprocessing\n",
        "\n",
        "\n",
        "IMG_WIDTH = 1024\n",
        "IMG_HEIGHT = IMG_WIDTH // 16 * 5\n",
        "MODEL_SCALE = 8\n",
        "\n",
        "def _regr_preprocess(regr_dict, flip=False):\n",
        "    if flip:\n",
        "        for k in ['x', 'pitch', 'roll']:\n",
        "            regr_dict[k] = -regr_dict[k]\n",
        "    for name in ['x', 'y', 'z']:\n",
        "        regr_dict[name] = regr_dict[name] / 100\n",
        "    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n",
        "    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n",
        "    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n",
        "    regr_dict.pop('pitch') # gives poped element with key 'pich'\n",
        "    regr_dict.pop('id')\n",
        "    return regr_dict\n",
        "\n",
        "def _regr_back(regr_dict):\n",
        "    for name in ['x', 'y', 'z']:\n",
        "        regr_dict[name] = regr_dict[name] * 100\n",
        "    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n",
        "    \n",
        "    pitch_sin = regr_dict['pitch_sin'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
        "    pitch_cos = regr_dict['pitch_cos'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
        "    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n",
        "    return regr_dict\n",
        "\n",
        "\n",
        "def get_mask_and_regr(img, labels, flip=False):\n",
        "    mask = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE], dtype='float32')\n",
        "    regr_names = ['x', 'y', 'z', 'yaw', 'pitch', 'roll']\n",
        "    regr = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE, 7], dtype='float32')\n",
        "    coords = str2coords(labels)\n",
        "    xs, ys = get_img_coords(labels)\n",
        "    for x, y, regr_dict in zip(xs, ys, coords):\n",
        "        x, y = y, x\n",
        "        x = (x - img.shape[0] // 2) * IMG_HEIGHT / (img.shape[0] // 2) / MODEL_SCALE\n",
        "        x = np.round(x).astype('int')\n",
        "        y = (y + img.shape[1] // 6) * IMG_WIDTH / (img.shape[1] * 4/3) / MODEL_SCALE\n",
        "        y = np.round(y).astype('int')\n",
        "        if x >= 0 and x < IMG_HEIGHT // MODEL_SCALE and y >= 0 and y < IMG_WIDTH // MODEL_SCALE:\n",
        "            mask[x, y] = 1\n",
        "            regr_dict = _regr_preprocess(regr_dict, flip)\n",
        "            regr[x, y] = [regr_dict[n] for n in sorted(regr_dict)]\n",
        "    if flip:\n",
        "        mask = np.array(mask[:,::-1])\n",
        "        regr = np.array(regr[:,::-1])\n",
        "    return mask, regr\n",
        "\n",
        "def preprocess_image(img, flip=False):\n",
        "    img = img[img.shape[0] // 2:]\n",
        "    bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n",
        "    bg = bg[:, :img.shape[1] // 6]\n",
        "    img = np.concatenate([bg, img, bg], 1)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    if flip:\n",
        "        img = img[:,::-1]\n",
        "    return (img / 255).astype('float32')\n",
        "\n",
        "\n",
        "## check if image preprocessing works\n",
        "\n",
        "img0 = imread(PATH + 'train_images/' + train['ImageId'][6] + '.jpg')\n",
        "img = preprocess_image(img0)\n",
        "\n",
        "mask, regr = get_mask_and_regr(img0, train['PredictionString'][6])\n",
        "\n",
        "print('img.shape', img.shape, 'std:', np.std(img))\n",
        "print('mask.shape', mask.shape, 'std:', np.std(mask))\n",
        "print('regr.shape', regr.shape, 'std:', np.std(regr))\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Processed image')\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Detection Mask')\n",
        "plt.imshow(mask)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Yaw values')\n",
        "plt.imshow(regr[:,:,-2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J2CcgpS5ZjvE"
      },
      "outputs": [],
      "source": [
        "#@title functions for converting from 2D to 3D \n",
        "\n",
        "### functions to convert back from 2d map to 3d coordinates and angles\n",
        "\n",
        "DISTANCE_THRESH_CLEAR = 2\n",
        "\n",
        "def convert_3d_to_2d(x, y, z, fx = 2304.5479, fy = 2305.8757, cx = 1686.2379, cy = 1354.9849):\n",
        "    # from https://www.kaggle.com/theshockwaverider/eda-visualization-baseline\n",
        "    return x * fx / z + cx, y * fy / z + cy\n",
        "\n",
        "def optimize_xy(r, c, x0, y0, z0, flipped=False):\n",
        "    def distance_fn(xyz):\n",
        "        x, y, z = xyz\n",
        "        xx = -x if flipped else x\n",
        "        slope_err = (xzy_slope.predict([[xx,z]])[0] - y)**2\n",
        "        x, y = convert_3d_to_2d(x, y, z)\n",
        "        y, x = x, y\n",
        "        x = (x - IMG_SHAPE[0] // 2) * IMG_HEIGHT / (IMG_SHAPE[0] // 2) / MODEL_SCALE\n",
        "        y = (y + IMG_SHAPE[1] // 6) * IMG_WIDTH / (IMG_SHAPE[1] * 4 / 3) / MODEL_SCALE\n",
        "        return max(0.2, (x-r)**2 + (y-c)**2) + max(0.4, slope_err)\n",
        "    \n",
        "    res = minimize(distance_fn, [x0, y0, z0], method='Powell')\n",
        "    x_new, y_new, z_new = res.x\n",
        "    return x_new, y_new, z_new\n",
        "\n",
        "def clear_duplicates(coords):\n",
        "    for c1 in coords:\n",
        "        xyz1 = np.array([c1['x'], c1['y'], c1['z']])\n",
        "        for c2 in coords:\n",
        "            xyz2 = np.array([c2['x'], c2['y'], c2['z']])\n",
        "            distance = np.sqrt(((xyz1 - xyz2)**2).sum())\n",
        "            if distance < DISTANCE_THRESH_CLEAR:\n",
        "                if c1['confidence'] < c2['confidence']:\n",
        "                    c1['confidence'] = -1\n",
        "    return [c for c in coords if c['confidence'] > 0]\n",
        "\n",
        "def extract_coords(prediction, flipped=False):\n",
        "    logits = prediction[0]\n",
        "    regr_output = prediction[1:]\n",
        "    points = np.argwhere(logits > 0)\n",
        "    col_names = sorted(['x', 'y', 'z', 'yaw', 'pitch_sin', 'pitch_cos', 'roll'])\n",
        "    coords = []\n",
        "    for r, c in points:\n",
        "        regr_dict = dict(zip(col_names, regr_output[:, r, c]))\n",
        "        coords.append(_regr_back(regr_dict))\n",
        "        coords[-1]['confidence'] = 1 / (1 + np.exp(-logits[r, c]))\n",
        "        coords[-1]['x'], coords[-1]['y'], coords[-1]['z'] = \\\n",
        "                optimize_xy(r, c,\n",
        "                            coords[-1]['x'],\n",
        "                            coords[-1]['y'],\n",
        "                            coords[-1]['z'], flipped)\n",
        "    coords = clear_duplicates(coords)\n",
        "    return coords\n",
        "\n",
        "\n",
        "##  to check if all works\n",
        "\n",
        "def visualize_2dto3d(start=0, end=2, step=1):\n",
        "  for idx in range(start, end, step):\n",
        "      fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
        "      \n",
        "      for ax_i in range(2):\n",
        "          img0 = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n",
        "          if ax_i == 1:\n",
        "              img0 = img0[:,::-1]\n",
        "          img = preprocess_image(img0, ax_i==1)\n",
        "          mask, regr = get_mask_and_regr(img0, train['PredictionString'][idx], ax_i==1)\n",
        "          regr = np.rollaxis(regr, 2, 0)\n",
        "          coords = extract_coords(np.concatenate([mask[None], regr], 0), ax_i==1)\n",
        "          \n",
        "          axes[ax_i].set_title('Flip = {}'.format(ax_i==1))\n",
        "          axes[ax_i].imshow(visualize(img0, coords))\n",
        "      plt.show()\n",
        "\n",
        "visualize_2dto3d()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XY0nmXemZ-Tb"
      },
      "outputs": [],
      "source": [
        "#@title data handling\n",
        "\n",
        "class CarDataset(Dataset):\n",
        "    \"\"\"Car dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, root_dir, training=True, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # Get image name\n",
        "        idx, labels = self.df.values[idx]\n",
        "        img_name = self.root_dir.format(idx)\n",
        "        \n",
        "        # Augmentation\n",
        "        flip = False\n",
        "        if self.training:\n",
        "            flip = np.random.randint(10) == 1\n",
        "        \n",
        "        # Read image\n",
        "        img0 = imread(img_name, True)\n",
        "        img = preprocess_image(img0, flip=flip)\n",
        "        img = np.rollaxis(img, 2, 0)\n",
        "        \n",
        "        # Get mask and regression maps\n",
        "        mask, regr = get_mask_and_regr(img0, labels, flip=flip)\n",
        "        regr = np.rollaxis(regr, 2, 0)\n",
        "        \n",
        "        return [img, mask, regr]\n",
        "\n",
        "\n",
        "train_images_dir = PATH + 'train_images/{}.jpg'\n",
        "test_images_dir = PATH + 'test_images/{}.jpg'\n",
        "\n",
        "df_train, df_dev = train_test_split(train, test_size=0.01, random_state=42)\n",
        "df_test = test\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = CarDataset(df_train, train_images_dir, training=True)\n",
        "dev_dataset = CarDataset(df_dev, train_images_dir, training=False)\n",
        "test_dataset = CarDataset(df_test, test_images_dir, training=False)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "# Create data generators - they will produce batches\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "## check if all works\n",
        "\n",
        "img, mask, regr = train_dataset[0]\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(np.rollaxis(img, 0, 3))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(mask)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(regr[-2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vbIyYKJZSVl"
      },
      "outputs": [],
      "source": [
        "n_rows = 10\n",
        "\n",
        "for idx in range(6, n_rows):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
        "    img = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n",
        "    axes[0].imshow(img)\n",
        "    img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n",
        "    axes[1].imshow(img_vis)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVzyHfDOYUVW"
      },
      "outputs": [],
      "source": [
        "xs, ys = [], []\n",
        "\n",
        "for ps in train['PredictionString']:\n",
        "    x, y = get_img_coords(ps)\n",
        "    xs += list(x)\n",
        "    ys += list(y)\n",
        "\n",
        "plt.figure(figsize=(18,18))\n",
        "plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2121] + '.jpg'), alpha=0.3)\n",
        "plt.scatter(xs, ys, color='red', s=10, alpha=0.2);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s60IvQm9V6N"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "nF5thYrhapMa"
      },
      "outputs": [],
      "source": [
        "#@title model\n",
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        \n",
        "        # for padding issues, see \n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        \n",
        "        if x2 is not None:\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "        else:\n",
        "            x = x1\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "def get_mesh(batch_size, shape_x, shape_y):\n",
        "    mg_x, mg_y = np.meshgrid(np.linspace(0, 1, shape_y), np.linspace(0, 1, shape_x))\n",
        "    mg_x = np.tile(mg_x[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
        "    mg_y = np.tile(mg_y[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
        "    mesh = torch.cat([torch.tensor(mg_x).to(device), torch.tensor(mg_y).to(device)], 1)\n",
        "    return mesh\n",
        "\n",
        "class MyUNet(nn.Module):\n",
        "    '''Mixture of previous classes'''\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyUNet, self).__init__()\n",
        "        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        \n",
        "        self.conv0 = double_conv(5, 64)\n",
        "        self.conv1 = double_conv(64, 128)\n",
        "        self.conv2 = double_conv(128, 512)\n",
        "        self.conv3 = double_conv(512, 1024)\n",
        "        \n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.up1 = up(1282 + 1024, 512)\n",
        "        self.up2 = up(512 + 512, 256)\n",
        "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        mesh1 = get_mesh(batch_size, x.shape[2], x.shape[3])\n",
        "        x0 = torch.cat([x, mesh1], 1)\n",
        "        x1 = self.mp(self.conv0(x0))\n",
        "        x2 = self.mp(self.conv1(x1))\n",
        "        x3 = self.mp(self.conv2(x2))\n",
        "        x4 = self.mp(self.conv3(x3))\n",
        "        \n",
        "        x_center = x[:, :, :, IMG_WIDTH // 8: -IMG_WIDTH // 8]\n",
        "        feats = self.base_model.extract_features(x_center)\n",
        "        bg = torch.zeros([feats.shape[0], feats.shape[1], feats.shape[2], feats.shape[3] // 8]).to(device)\n",
        "        feats = torch.cat([bg, feats, bg], 3)\n",
        "        \n",
        "        # Add positional info\n",
        "        mesh2 = get_mesh(batch_size, feats.shape[2], feats.shape[3])\n",
        "        feats = torch.cat([feats, mesh2], 1)\n",
        "        \n",
        "        x = self.up1(feats, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.outc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ2NFCvi9ajm"
      },
      "source": [
        "# Training and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LErOB0fycIAA"
      },
      "outputs": [],
      "source": [
        "#@title training\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "model = MyUNet(8).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=max(n_epochs, 10) * len(train_loader) // 3, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "def criterion(prediction, mask, regr, size_average=True):\n",
        "    # Binary mask loss\n",
        "    pred_mask = torch.sigmoid(prediction[:, 0])\n",
        "    # mask_loss = mask * (1 - pred_mask)**2 * torch.log(pred_mask + 1e-12) + (1 - mask) * pred_mask**2 * torch.log(1 - pred_mask + 1e-12)\n",
        "    mask_loss = mask * torch.log(pred_mask + 1e-12) + (1 - mask) * torch.log(1 - pred_mask + 1e-12)\n",
        "    mask_loss = -mask_loss.mean(0).sum()\n",
        "    \n",
        "    # Regression L1 loss\n",
        "    pred_regr = prediction[:, 1:]\n",
        "    regr_loss = (torch.abs(pred_regr - regr).sum(1) * mask).sum(1).sum(1) / mask.sum(1).sum(1)\n",
        "    regr_loss = regr_loss.mean(0)\n",
        "    \n",
        "    # Sum\n",
        "    loss = mask_loss + regr_loss\n",
        "    if not size_average:\n",
        "        loss *= prediction.shape[0]\n",
        "    return loss\n",
        "\n",
        "def train_model(epoch, history=None, losses=None, train_loader=train_loader):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (img_batch, mask_batch, regr_batch) in enumerate(tqdm(train_loader)):\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "        regr_batch = regr_batch.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(img_batch)\n",
        "        loss = criterion(output, mask_batch, regr_batch)\n",
        "        if history is not None:\n",
        "            history.loc[epoch + batch_idx / len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n",
        "        losses['loss'] += loss.data.cpu().numpy()\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        exp_lr_scheduler.step()\n",
        "\n",
        "    losses['loss'] = losses['loss']/len(train_loader)\n",
        "    print('Train Epoch: {} \\tLR: {:.6f}\\tLoss: {:.6f}'.format(\n",
        "        epoch,\n",
        "        optimizer.state_dict()['param_groups'][0]['lr'],\n",
        "        loss.data))\n",
        "    \n",
        "    return history, losses['loss']\n",
        "\n",
        "def evaluate_model(epoch, history=None, losses=None, dev_loader=dev_loader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img_batch, mask_batch, regr_batch in dev_loader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            mask_batch = mask_batch.to(device)\n",
        "            regr_batch = regr_batch.to(device)\n",
        "\n",
        "            output = model(img_batch)\n",
        "            loss += criterion(output, mask_batch, regr_batch, size_average=False).data\n",
        "\n",
        "    loss /= len(dev_loader.dataset)\n",
        "\n",
        "    if history is not None:\n",
        "        history.loc[epoch, 'dev_loss'] = loss.cpu().numpy()\n",
        "\n",
        "    losses['val_loss'] = loss.cpu().numpy()\n",
        "\n",
        "    print('Dev loss: {:.4f}'.format(loss))\n",
        "\n",
        "    return history, losses['val_loss']\n",
        "\n",
        "\n",
        "def training(n_epochs=n_epochs, train_loader=train_loader, eval_loader=dev_loader):\n",
        "  \n",
        "  liveloss = PlotLosses()\n",
        "  history = pd.DataFrame()\n",
        "  best = float('inf')\n",
        "  losses = {'loss': None,\n",
        "          'val_loss': None}\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "      losses['loss'] = 0.0\n",
        "      losses['val_loss'] = 0.0\n",
        "      \n",
        "      history, losses['loss'] = train_model(epoch, history, losses, train_loader)\n",
        "      \n",
        "      history, losses['val_loss'] = evaluate_model(epoch, history, losses, eval_loader)\n",
        "      if losses['val_loss'] < best:\n",
        "        best = losses['val_loss']\n",
        "        torch.save(model.state_dict(), os.path.join(PATH, 'best_model.pt'))\n",
        "      \n",
        "      liveloss.update(losses)\n",
        "      liveloss.send()\n",
        "  \n",
        "  return history\n",
        "\n",
        "### This is intended for quick testing putposes only as it takes only single bach from both dataloaders\n",
        "## next(iter(train_loader))[0].shape --> torch.Size([4, 3, 320, 1024]) # subset_indices=[0]\n",
        "## got help from here: https://stackoverflow.com/questions/53570732/get-single-random-example-from-pytorch-dataloader\n",
        "# train_subset = torch.utils.data.Subset(train_dataset, [0]) \n",
        "# trainloader_subset = torch.utils.data.DataLoader(train_subset, batch_size=1, num_workers=0, shuffle=False)\n",
        "\n",
        "# valid_subset = torch.utils.data.Subset(dev_dataset, [0]) \n",
        "# validloader_subset = torch.utils.data.DataLoader(valid_subset, batch_size=1, num_workers=0, shuffle=False)\n",
        "\n",
        "# history = training(3, trainloader_subset, validloader_subset)\n",
        "\n",
        "### This part is for real training\n",
        "%time     # otherwise use %%time \n",
        "history = training(n_epochs, train_loader, dev_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hiw2b0EEcnI_"
      },
      "outputs": [],
      "source": [
        "##torch.save(model.state_dict(), './model.pth')\n",
        "history['train_loss'].iloc[100:].plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HazPb-S8ctnf"
      },
      "outputs": [],
      "source": [
        "series = history.dropna()['dev_loss']\n",
        "plt.scatter(series.index, series);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfdt5kGYczT2"
      },
      "outputs": [],
      "source": [
        "img, mask, regr = dev_dataset[0]\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Input image')\n",
        "plt.imshow(np.rollaxis(img, 0, 3))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Ground truth mask')\n",
        "plt.imshow(mask)\n",
        "plt.show()\n",
        "\n",
        "output = model(torch.tensor(img[None]).to(device))\n",
        "logits = output[0,0].data.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Model predictions')\n",
        "plt.imshow(logits)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.title('Model predictions thresholded')\n",
        "plt.imshow(logits > 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6U7q71rD9QL"
      },
      "outputs": [],
      "source": [
        "model = MyUNet(8).to(device)\n",
        "model.load_state_dict(torch.load('./best_model.pt')) \n",
        "# use map_location ='cpu' when gpu is not available like this:\n",
        "# model.load_state_dict(torch.load('./best_model.pt',map_location ='cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kRIalj0lc0hO"
      },
      "outputs": [],
      "source": [
        "#@title prediction\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()            \n",
        "\n",
        "for idx in range(2):\n",
        "    img, mask, regr = dev_dataset[idx]\n",
        "    \n",
        "    output = model(torch.tensor(img[None]).to(device)).data.cpu().numpy() \n",
        "    coords_pred = extract_coords(output[0])\n",
        "    coords_true = extract_coords(np.concatenate([mask[None], regr], 0))\n",
        "    \n",
        "    img = imread(train_images_dir.format(df_dev['ImageId'].iloc[idx]))\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(30,30))\n",
        "    axes[0].set_title('Ground truth');\n",
        "    axes[0].imshow(visualize(img, coords_true))\n",
        "    axes[1].set_title('Prediction')\n",
        "    axes[1].imshow(visualize(img, coords_pred))\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "r1BFpg8l9JfV",
        "Ga0wa5aafd47",
        "0s60IvQm9V6N",
        "gJ2NFCvi9ajm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
