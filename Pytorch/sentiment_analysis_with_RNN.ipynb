{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvBFSQJ0TZJ4"
      },
      "source": [
        "# Binary sentiment analysis starts here\n",
        "\n",
        "Data : http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq6zhok8AseD",
        "outputId": "55128a00-6d07-4a2c-d6e8-6247b1f144f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 36.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 42.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 46.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 48.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 48.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 49.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 430 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 604 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 860 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 952 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 51.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 51.2 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('1.12.1+cu113', '0.13.1')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q torchtext\n",
        "# !pip install -q livelossplot \n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "# from livelossplot import PlotLosses\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch. __version__ , torchtext. __version__ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UxZc2Mq9MRkS"
      },
      "outputs": [],
      "source": [
        "!wget -O aclImdb_v1.tar.gz -q http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "#!rm aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BBD5WGO6AseE"
      },
      "outputs": [],
      "source": [
        "# convert raw data into dataset and split into train, valid and test sets\n",
        "\n",
        "def gettext(fname):\n",
        "  with open(fname) as f:\n",
        "    lines = f.readlines()\n",
        "  return lines  \n",
        "\n",
        "\n",
        "label = lambda fname: fname.split(\"/\")[-2]\n",
        "\n",
        "classes = ['pos', 'neg']\n",
        "\n",
        "def unify_data(classes=classes):\n",
        "  trainlist = []\n",
        "  testlist = []\n",
        "\n",
        "  for cls in classes:\n",
        "    train_cls = [(label(i), *gettext(i)) for _, i in enumerate(glob(f'./aclImdb/train/{cls}/*.txt'))] #I used enumerate for testing and it is not necessary here\n",
        "    test_cls = [(label(i), *gettext(i)) for _, i in enumerate(glob(f'./aclImdb/test/{cls}/*.txt'))]\n",
        "    trainlist.extend(train_cls)  # we need extend not append\n",
        "    testlist.extend(test_cls)\n",
        "    random.shuffle(trainlist)\n",
        "    random.shuffle(testlist)\n",
        "\n",
        "  return trainlist, testlist\n",
        "\n",
        "trainset, testset = unify_data(classes)\n",
        "\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, items):\n",
        "        super().__init__()\n",
        "        self.items = items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]     # this is a tupple label and corresponding text\n",
        "        return item[0], item[1]    # this returns label and text\n",
        "\n",
        "\n",
        "train_dataset = SentimentDataset(trainset)\n",
        "test_dataset = SentimentDataset(testset)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "train_dataset, valid_dataset = random_split(\n",
        "    list(train_dataset), [20000, 5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhLFiZJ0AseF",
        "outputId": "fee21055-9df4-4c30-e240-27d1f97d685c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab-size: 69527\n"
          ]
        }
      ],
      "source": [
        "# create tokenizer and find unique words (tokens)\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        " \n",
        "    \n",
        "print('Vocab-size:', len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNmQTbu7AseF",
        "outputId": "ce4c31cd-1cc2-4161-ec6e-6f33ace41e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11, 7, 35, 468]\n"
          ]
        }
      ],
      "source": [
        "# encode each unique token into integers\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "vocab = vocab(ordered_dict)\n",
        "\n",
        "vocab.insert_token(\"<pad>\", 0)\n",
        "vocab.insert_token(\"<unk>\", 1)\n",
        "vocab.set_default_index(1)\n",
        "\n",
        "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEV0hRGEAseG"
      },
      "outputs": [],
      "source": [
        "# define the functions for transformation\n",
        "\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
        "\n",
        "\n",
        "# collate the encode and transformation function\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), \n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True)\n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)\n",
        "\n",
        "# create dataloaders for batching of the datasets\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "\n",
        "# test how dataloader works\n",
        "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
        "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
        "\n",
        "# test a 2 samples with 4==embeding size\n",
        "embedding = nn.Embedding(num_embeddings=10, \n",
        "                         embedding_dim=3, \n",
        "                         padding_idx=0)\n",
        "text_encoded_input = torch.LongTensor([[5,4,3,2],[0,2,4,7]])\n",
        "\n",
        "#print all tests\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(length_batch)\n",
        "print(text_batch.shape)\n",
        "\n",
        "print(embedding(text_encoded_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "qsKEb0BZAseJ"
      },
      "outputs": [],
      "source": [
        "#@title model\n",
        "\n",
        "# ## An example of building a RNN model\n",
        "# ## with simple RNN layer\n",
        "\n",
        "# # Fully connected neural network with one hidden layer\n",
        "# class RNN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size):\n",
        "#         super().__init__()\n",
        "#         self.rnn = nn.RNN(input_size, \n",
        "#                           hidden_size, \n",
        "#                           num_layers=2, \n",
        "#                           batch_first=True)\n",
        "#         #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "#         #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         _, hidden = self.rnn(x)\n",
        "#         out = hidden[-1, :, :]\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "\n",
        "# model = RNN(64, 32) \n",
        "\n",
        "# print(model) \n",
        " \n",
        "# model(torch.randn(5, 3, 64)) \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, \n",
        "                                      embed_dim, \n",
        "                                      padding_idx=0) \n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
        "                           batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "         \n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 20\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rq0LprUsAseL"
      },
      "outputs": [],
      "source": [
        "#@title training\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)[:, 0]\n",
        "        loss = loss_fn(pred, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        " \n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)[:, 0]\n",
        "            loss = loss_fn(pred, label_batch)\n",
        "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "beta = float('inf')\n",
        "num_epochs = 10 \n",
        "\n",
        "# liveloss = PlotLosses()\n",
        "torch.manual_seed(1)\n",
        " \n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    if loss_valid < beta:\n",
        "      beta=acc_valid\n",
        "      torch.save(model.state_dict(), './model.pth')\n",
        "\n",
        "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n",
        "    # accuracies = {'loss': acc_train,  # here name should be 'loss' otherwise it plots separate chart\n",
        "    #       'val_loss': acc_valid}     # here name should be 'val_loss' otherwise it draws separate chart\n",
        "    # liveloss.update(accuracies)\n",
        "    # liveloss.send()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEGUvsKFQnXZ"
      },
      "outputs": [],
      "source": [
        "# load the the best saved model parameters if needed\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('./model.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6IqwOBLAseL"
      },
      "outputs": [],
      "source": [
        "acc_test, _ = evaluate(test_dl)\n",
        "print(f'test_accuracy: {acc_test:.4f}') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2herBjaIAseM"
      },
      "source": [
        "#### More on the bidirectional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBGwjssCAseM"
      },
      "source": [
        " * **Trying bidirectional recurrent layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XQgdIcaAseM"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, \n",
        "                                      embed_dim, \n",
        "                                      padding_idx=0) \n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
        "        _, (hidden, cell) = self.rnn(out)\n",
        "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "    \n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f6_Z259AseN"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "num_epochs = 10 \n",
        "\n",
        "torch.manual_seed(1)\n",
        " \n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKm_NqQTAseN"
      },
      "outputs": [],
      "source": [
        "test_dataset = IMDB(split='test')\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbzpkgVQAseO",
        "outputId": "f994cf4a-eeca-4e1e-b1cb-97f3ad599fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.8566\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate(test_dl)\n",
        "print(f'test_accuracy: {acc_test:.4f}') "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KM1jibzvMuhK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
