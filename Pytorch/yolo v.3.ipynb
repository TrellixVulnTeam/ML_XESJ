{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference : https://arxiv.org/abs/1804.02767\n",
        "\n",
        "Architecture: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b \n",
        "\n",
        "Full algorithm explanations: https://www.youtube.com/watch?v=5e5pjeojznk&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=25 \n",
        "\n",
        "For learning the implementation (not this implementation though): https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-4/ (all parts https://blog.paperspace.com/tag/series-yolo/)"
      ],
      "metadata": {
        "id": "Vt8hv3u4xkax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports { form-width: \"20%\" }\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from time import time\n",
        "import importlib\n",
        "from tqdm import tqdm  #if you just import tqdm you will have to call it like tqdm.tqdm()\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from collections import OrderedDict, Iterable, defaultdict\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "yYBaAubXjW0b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download data { form-width: \"20%\" }\n",
        "\n",
        "\n",
        "datasets.VOCDetection(root='/content', year='2007', image_set='train', download=True) \n",
        "\n",
        "# !wget --quiet http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar .\n",
        "!tar -xf VOCtest_06-Nov-2007.tar\n",
        "!rm VOCtest_06-Nov-2007.tar\n",
        "\n",
        "#DO THIS! otherwise because of long path you will get FileNotFoundError\n",
        "!cp -r /content/VOCdevkit/VOC2007/* /content/ \n",
        "\n",
        "#just for cleaning AND put this in separate cell otherwise it won't work\n",
        "shutil.rmtree('/content/VOCdevkit')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RK1-WWGuL7dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title write class names file { form-width: \"20%\" }\n",
        "\n",
        "%%writefile voc_classes.txt\n",
        "aeroplane\n",
        "bicycle\n",
        "bird\n",
        "boat\n",
        "bottle\n",
        "bus\n",
        "car\n",
        "cat\n",
        "chair\n",
        "cow\n",
        "diningtable\n",
        "dog\n",
        "horse\n",
        "motorbike\n",
        "person\n",
        "pottedplant\n",
        "sheep\n",
        "sofa\n",
        "train\n",
        "tvmonitor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "kh61MDedXxgq",
        "outputId": "c06b0edd-b82c-4776-f41f-37040385f4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing voc_classes.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title write color names file { form-width: \"20%\" }\n",
        "\n",
        "a = np.array([[ 50,   0, 255],\n",
        "       [127, 255,   0],\n",
        "       [255, 229,   0],\n",
        "       [  0, 102, 255],\n",
        "       [255,   0, 229],\n",
        "       [  0, 255, 178],\n",
        "       [127,   0, 255],\n",
        "       [  0,  25, 255],\n",
        "       [204,   0, 255],\n",
        "       [255,   0,   0],\n",
        "       [ 51, 255,   0],\n",
        "       [  0, 178, 255],\n",
        "       [255,  76,   0],\n",
        "       [255,   0,  76],\n",
        "       [  0, 255, 255],\n",
        "       [  0, 255, 102],\n",
        "       [255, 153,   0],\n",
        "       [203, 255,   0],\n",
        "       [255,   0, 152],\n",
        "       [  0, 255,  25]], dtype=np.int32) #float is also possible\n",
        "\n",
        "# here each triplet describes RGB, red green blue, ranging from 0 to 255\n",
        "# notice the number of colours corresponds to the number of classes\n",
        "# notice one value is allways zero. This is just because we picked most intensive colours to be noticable\n",
        "# you can play with it using colour picker\n",
        "np.save('voc_colors.npy', a)"
      ],
      "metadata": {
        "id": "AOBtyXxBZkQv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title write cleaned training image names file for VOC { form-width: \"20%\" }\n",
        "\n",
        "%%writefile ./ImageSets/Main/train.txt\n",
        "000012\n",
        "000017\n",
        "000023\n",
        "000026\n",
        "000032\n",
        "000033\n",
        "000034\n",
        "000035\n",
        "000036\n",
        "000042\n",
        "000044\n",
        "000047\n",
        "000048\n",
        "000061\n",
        "000064\n",
        "000066\n",
        "000073\n",
        "000077\n",
        "000078\n",
        "000083\n",
        "000089\n",
        "000091\n",
        "000104\n",
        "000112\n",
        "000122\n",
        "000129\n",
        "000133\n",
        "000134\n",
        "000138\n",
        "000140\n",
        "000141\n",
        "000147\n",
        "000153\n",
        "000154\n",
        "000159\n",
        "000161\n",
        "000162\n",
        "000163\n",
        "000164\n",
        "000171\n",
        "000173\n",
        "000174\n",
        "000187\n",
        "000189\n",
        "000192\n",
        "000193\n",
        "000194\n",
        "000198\n",
        "000200\n",
        "000207\n",
        "000209\n",
        "000219\n",
        "000220\n",
        "000222\n",
        "000225\n",
        "000228\n",
        "000235\n",
        "000242\n",
        "000250\n",
        "000256\n",
        "000259\n",
        "000262\n",
        "000263\n",
        "000276\n",
        "000278\n",
        "000282\n",
        "000288\n",
        "000294\n",
        "000296\n",
        "000306\n",
        "000307\n",
        "000311\n",
        "000312\n",
        "000317\n",
        "000320\n",
        "000325\n",
        "000331\n",
        "000334\n",
        "000337\n",
        "000344\n",
        "000347\n",
        "000349\n",
        "000355\n",
        "000359\n",
        "000367\n",
        "000370\n",
        "000372\n",
        "000379\n",
        "000382\n",
        "000387\n",
        "000391\n",
        "000394\n",
        "000395\n",
        "000400\n",
        "000404\n",
        "000406\n",
        "000407\n",
        "000411\n",
        "000416\n",
        "000430\n",
        "000431\n",
        "000438\n",
        "000446\n",
        "000450\n",
        "000454\n",
        "000463\n",
        "000468\n",
        "000469\n",
        "000470\n",
        "000474\n",
        "000476\n",
        "000477\n",
        "000484\n",
        "000489\n",
        "000496\n",
        "000503\n",
        "000508\n",
        "000516\n",
        "000518\n",
        "000519\n",
        "000522\n",
        "000524\n",
        "000525\n",
        "000526\n",
        "000528\n",
        "000535\n",
        "000537\n",
        "000541\n",
        "000544\n",
        "000549\n",
        "000550\n",
        "000552\n",
        "000554\n",
        "000555\n",
        "000559\n",
        "000565\n",
        "000577\n",
        "000583\n",
        "000589\n",
        "000590\n",
        "000592\n",
        "000597\n",
        "000605\n",
        "000609\n",
        "000612\n",
        "000620\n",
        "000622\n",
        "000625\n",
        "000632\n",
        "000633\n",
        "000635\n",
        "000648\n",
        "000654\n",
        "000657\n",
        "000671\n",
        "000672\n",
        "000680\n",
        "000685\n",
        "000688\n",
        "000689\n",
        "000695\n",
        "000699\n",
        "000700\n",
        "000709\n",
        "000710\n",
        "000711\n",
        "000726\n",
        "000729\n",
        "000731\n",
        "000733\n",
        "000739\n",
        "000740\n",
        "000753\n",
        "000754\n",
        "000761\n",
        "000764\n",
        "000767\n",
        "000768\n",
        "000770\n",
        "000774\n",
        "000793\n",
        "000796\n",
        "000804\n",
        "000805\n",
        "000810\n",
        "000812\n",
        "000818\n",
        "000820\n",
        "000822\n",
        "000823\n",
        "000827\n",
        "000828\n",
        "000829\n",
        "000830\n",
        "000845\n",
        "000849\n",
        "000850\n",
        "000851\n",
        "000859\n",
        "000860\n",
        "000865\n",
        "000867\n",
        "000871\n",
        "000887\n",
        "000888\n",
        "000889\n",
        "000892\n",
        "000898\n",
        "000899\n",
        "000900\n",
        "000902\n",
        "000904\n",
        "000906\n",
        "000908\n",
        "000912\n",
        "000915\n",
        "000919\n",
        "000929\n",
        "000936\n",
        "000943\n",
        "000950\n",
        "000951\n",
        "000954\n",
        "000958\n",
        "000962\n",
        "000964\n",
        "000965\n",
        "000966\n",
        "000967\n",
        "000977\n",
        "000980\n",
        "000987\n",
        "000989\n",
        "000991\n",
        "000993\n",
        "000996\n",
        "000997\n",
        "000999\n",
        "001001\n",
        "001002\n",
        "001008\n",
        "001010\n",
        "001011\n",
        "001014\n",
        "001015\n",
        "001024\n",
        "001036\n",
        "001043\n",
        "001050\n",
        "001057\n",
        "001060\n",
        "001064\n",
        "001068\n",
        "001071\n",
        "001073\n",
        "001077\n",
        "001078\n",
        "001079\n",
        "001082\n",
        "001101\n",
        "001106\n",
        "001112\n",
        "001113\n",
        "001119\n",
        "001127\n",
        "001129\n",
        "001130\n",
        "001140\n",
        "001147\n",
        "001151\n",
        "001152\n",
        "001156\n",
        "001158\n",
        "001168\n",
        "001171\n",
        "001172\n",
        "001174\n",
        "001182\n",
        "001191\n",
        "001194\n",
        "001204\n",
        "001205\n",
        "001207\n",
        "001209\n",
        "001212\n",
        "001214\n",
        "001226\n",
        "001229\n",
        "001230\n",
        "001234\n",
        "001237\n",
        "001239\n",
        "001240\n",
        "001248\n",
        "001258\n",
        "001263\n",
        "001268\n",
        "001269\n",
        "001270\n",
        "001273\n",
        "001279\n",
        "001287\n",
        "001294\n",
        "001299\n",
        "001304\n",
        "001309\n",
        "001312\n",
        "001314\n",
        "001315\n",
        "001323\n",
        "001325\n",
        "001326\n",
        "001327\n",
        "001332\n",
        "001333\n",
        "001334\n",
        "001345\n",
        "001346\n",
        "001348\n",
        "001364\n",
        "001365\n",
        "001378\n",
        "001384\n",
        "001385\n",
        "001388\n",
        "001390\n",
        "001393\n",
        "001395\n",
        "001402\n",
        "001404\n",
        "001405\n",
        "001406\n",
        "001408\n",
        "001409\n",
        "001414\n",
        "001418\n",
        "001420\n",
        "001421\n",
        "001426\n",
        "001427\n",
        "001434\n",
        "001436\n",
        "001442\n",
        "001450\n",
        "001451\n",
        "001453\n",
        "001455\n",
        "001457\n",
        "001468\n",
        "001470\n",
        "001479\n",
        "001480\n",
        "001483\n",
        "001485\n",
        "001486\n",
        "001488\n",
        "001492\n",
        "001494\n",
        "001498\n",
        "001499\n",
        "001501\n",
        "001504\n",
        "001512\n",
        "001515\n",
        "001517\n",
        "001521\n",
        "001524\n",
        "001526\n",
        "001528\n",
        "001529\n",
        "001532\n",
        "001539\n",
        "001548\n",
        "001555\n",
        "001556\n",
        "001557\n",
        "001559\n",
        "001563\n",
        "001576\n",
        "001579\n",
        "001586\n",
        "001590\n",
        "001593\n",
        "001594\n",
        "001597\n",
        "001604\n",
        "001607\n",
        "001610\n",
        "001611\n",
        "001612\n",
        "001622\n",
        "001630\n",
        "001633\n",
        "001636\n",
        "001643\n",
        "001649\n",
        "001650\n",
        "001651\n",
        "001654\n",
        "001661\n",
        "001662\n",
        "001669\n",
        "001673\n",
        "001676\n",
        "001680\n",
        "001683\n",
        "001684\n",
        "001688\n",
        "001690\n",
        "001699\n",
        "001707\n",
        "001708\n",
        "001711\n",
        "001713\n",
        "001714\n",
        "001717\n",
        "001721\n",
        "001723\n",
        "001729\n",
        "001732\n",
        "001733\n",
        "001734\n",
        "001738\n",
        "001739\n",
        "001741\n",
        "001750\n",
        "001752\n",
        "001754\n",
        "001758\n",
        "001759\n",
        "001761\n",
        "001765\n",
        "001766\n",
        "001768\n",
        "001777\n",
        "001780\n",
        "001787\n",
        "001789\n",
        "001800\n",
        "001806\n",
        "001809\n",
        "001810\n",
        "001821\n",
        "001825\n",
        "001828\n",
        "001832\n",
        "001834\n",
        "001836\n",
        "001840\n",
        "001841\n",
        "001843\n",
        "001845\n",
        "001853\n",
        "001854\n",
        "001858\n",
        "001861\n",
        "001864\n",
        "001870\n",
        "001881\n",
        "001892\n",
        "001894\n",
        "001896\n",
        "001898\n",
        "001902\n",
        "001903\n",
        "001904\n",
        "001906\n",
        "001915\n",
        "001922\n",
        "001928\n",
        "001930\n",
        "001937\n",
        "001938\n",
        "001941\n",
        "001950\n",
        "001952\n",
        "001954\n",
        "001960\n",
        "001963\n",
        "001971\n",
        "001977\n",
        "001978\n",
        "001980\n",
        "001981\n",
        "001985\n",
        "001989\n",
        "001995\n",
        "001999\n",
        "002001\n",
        "002002\n",
        "002004\n",
        "002006\n",
        "002012\n",
        "002015\n",
        "002020\n",
        "002025\n",
        "002027\n",
        "002034\n",
        "002037\n",
        "002039\n",
        "002042\n",
        "002043\n",
        "002047\n",
        "002049\n",
        "002051\n",
        "002055\n",
        "002056\n",
        "002061\n",
        "002068\n",
        "002069\n",
        "002095\n",
        "002096\n",
        "002104\n",
        "002108\n",
        "002116\n",
        "002117\n",
        "002120\n",
        "002126\n",
        "002132\n",
        "002134\n",
        "002139\n",
        "002151\n",
        "002153\n",
        "002155\n",
        "002156\n",
        "002158\n",
        "002166\n",
        "002170\n",
        "002172\n",
        "002176\n",
        "002178\n",
        "002179\n",
        "002180\n",
        "002182\n",
        "002186\n",
        "002187\n",
        "002191\n",
        "002192\n",
        "002193\n",
        "002194\n",
        "002196\n",
        "002197\n",
        "002199\n",
        "002208\n",
        "002212\n",
        "002215\n",
        "002219\n",
        "002221\n",
        "002224\n",
        "002234\n",
        "002237\n",
        "002238\n",
        "002241\n",
        "002247\n",
        "002249\n",
        "002253\n",
        "002255\n",
        "002256\n",
        "002260\n",
        "002265\n",
        "002277\n",
        "002279\n",
        "002280\n",
        "002284\n",
        "002287\n",
        "002291\n",
        "002293\n",
        "002306\n",
        "002307\n",
        "002310\n",
        "002311\n",
        "002315\n",
        "002318\n",
        "002320\n",
        "002321\n",
        "002323\n",
        "002334\n",
        "002335\n",
        "002342\n",
        "002347\n",
        "002350\n",
        "002354\n",
        "002355\n",
        "002359\n",
        "002362\n",
        "002368\n",
        "002373\n",
        "002384\n",
        "002392\n",
        "002401\n",
        "002403\n",
        "002405\n",
        "002410\n",
        "002411\n",
        "002413\n",
        "002419\n",
        "002420\n",
        "002423\n",
        "002433\n",
        "002436\n",
        "002439\n",
        "002442\n",
        "002443\n",
        "002445\n",
        "002448\n",
        "002458\n",
        "002461\n",
        "002465\n",
        "002466\n",
        "002468\n",
        "002471\n",
        "002472\n",
        "002478\n",
        "002480\n",
        "002481\n",
        "002483\n",
        "002490\n",
        "002494\n",
        "002496\n",
        "002500\n",
        "002501\n",
        "002502\n",
        "002512\n",
        "002514\n",
        "002518\n",
        "002519\n",
        "002533\n",
        "002534\n",
        "002539\n",
        "002544\n",
        "002545\n",
        "002547\n",
        "002554\n",
        "002555\n",
        "002558\n",
        "002559\n",
        "002569\n",
        "002571\n",
        "002572\n",
        "002579\n",
        "002590\n",
        "002594\n",
        "002595\n",
        "002599\n",
        "002603\n",
        "002609\n",
        "002611\n",
        "002625\n",
        "002627\n",
        "002634\n",
        "002635\n",
        "002645\n",
        "002647\n",
        "002648\n",
        "002653\n",
        "002662\n",
        "002664\n",
        "002666\n",
        "002669\n",
        "002680\n",
        "002682\n",
        "002683\n",
        "002684\n",
        "002691\n",
        "002697\n",
        "002702\n",
        "002704\n",
        "002710\n",
        "002713\n",
        "002715\n",
        "002722\n",
        "002730\n",
        "002735\n",
        "002737\n",
        "002738\n",
        "002744\n",
        "002745\n",
        "002749\n",
        "002755\n",
        "002757\n",
        "002759\n",
        "002763\n",
        "002765\n",
        "002766\n",
        "002774\n",
        "002778\n",
        "002779\n",
        "002782\n",
        "002783\n",
        "002791\n",
        "002795\n",
        "002796\n",
        "002801\n",
        "002804\n",
        "002807\n",
        "002816\n",
        "002817\n",
        "002820\n",
        "002826\n",
        "002834\n",
        "002841\n",
        "002844\n",
        "002845\n",
        "002848\n",
        "002855\n",
        "002858\n",
        "002864\n",
        "002866\n",
        "002867\n",
        "002868\n",
        "002869\n",
        "002870\n",
        "002873\n",
        "002881\n",
        "002899\n",
        "002906\n",
        "002914\n",
        "002919\n",
        "002931\n",
        "002934\n",
        "002937\n",
        "002939\n",
        "002953\n",
        "002956\n",
        "002957\n",
        "002958\n",
        "002962\n",
        "002969\n",
        "002975\n",
        "002976\n",
        "002987\n",
        "002988\n",
        "002989\n",
        "002990\n",
        "002992\n",
        "002995\n",
        "003002\n",
        "003003\n",
        "003007\n",
        "003011\n",
        "003013\n",
        "003024\n",
        "003027\n",
        "003034\n",
        "003042\n",
        "003047\n",
        "003051\n",
        "003053\n",
        "003061\n",
        "003063\n",
        "003066\n",
        "003074\n",
        "003077\n",
        "003083\n",
        "003085\n",
        "003088\n",
        "003092\n",
        "003100\n",
        "003103\n",
        "003105\n",
        "003106\n",
        "003107\n",
        "003108\n",
        "003110\n",
        "003116\n",
        "003122\n",
        "003124\n",
        "003133\n",
        "003134\n",
        "003135\n",
        "003138\n",
        "003140\n",
        "003145\n",
        "003146\n",
        "003147\n",
        "003149\n",
        "003150\n",
        "003155\n",
        "003157\n",
        "003159\n",
        "003161\n",
        "003163\n",
        "003165\n",
        "003169\n",
        "003175\n",
        "003181\n",
        "003183\n",
        "003184\n",
        "003185\n",
        "003188\n",
        "003202\n",
        "003204\n",
        "003205\n",
        "003211\n",
        "003214\n",
        "003229\n",
        "003231\n",
        "003233\n",
        "003236\n",
        "003240\n",
        "003242\n",
        "003244\n",
        "003247\n",
        "003253\n",
        "003254\n",
        "003259\n",
        "003260\n",
        "003261\n",
        "003269\n",
        "003270\n",
        "003273\n",
        "003279\n",
        "003280\n",
        "003282\n",
        "003284\n",
        "003290\n",
        "003292\n",
        "003303\n",
        "003308\n",
        "003320\n",
        "003330\n",
        "003331\n",
        "003336\n",
        "003337\n",
        "003338\n",
        "003339\n",
        "003343\n",
        "003349\n",
        "003350\n",
        "003354\n",
        "003355\n",
        "003356\n",
        "003359\n",
        "003363\n",
        "003365\n",
        "003367\n",
        "003369\n",
        "003373\n",
        "003374\n",
        "003379\n",
        "003380\n",
        "003382\n",
        "003392\n",
        "003395\n",
        "003396\n",
        "003401\n",
        "003406\n",
        "003408\n",
        "003412\n",
        "003413\n",
        "003416\n",
        "003417\n",
        "003420\n",
        "003421\n",
        "003424\n",
        "003430\n",
        "003433\n",
        "003436\n",
        "003439\n",
        "003441\n",
        "003450\n",
        "003452\n",
        "003466\n",
        "003477\n",
        "003484\n",
        "003487\n",
        "003489\n",
        "003491\n",
        "003493\n",
        "003496\n",
        "003497\n",
        "003499\n",
        "003500\n",
        "003506\n",
        "003508\n",
        "003509\n",
        "003510\n",
        "003511\n",
        "003522\n",
        "003524\n",
        "003525\n",
        "003529\n",
        "003539\n",
        "003548\n",
        "003549\n",
        "003550\n",
        "003551\n",
        "003555\n",
        "003564\n",
        "003565\n",
        "003575\n",
        "003576\n",
        "003577\n",
        "003585\n",
        "003586\n",
        "003588\n",
        "003596\n",
        "003599\n",
        "003603\n",
        "003604\n",
        "003605\n",
        "003608\n",
        "003609\n",
        "003614\n",
        "003621\n",
        "003622\n",
        "003625\n",
        "003627\n",
        "003628\n",
        "003629\n",
        "003634\n",
        "003635\n",
        "003642\n",
        "003644\n",
        "003645\n",
        "003646\n",
        "003656\n",
        "003658\n",
        "003662\n",
        "003663\n",
        "003664\n",
        "003671\n",
        "003678\n",
        "003679\n",
        "003681\n",
        "003688\n",
        "003694\n",
        "003695\n",
        "003698\n",
        "003699\n",
        "003700\n",
        "003704\n",
        "003705\n",
        "003713\n",
        "003714\n",
        "003732\n",
        "003735\n",
        "003740\n",
        "003743\n",
        "003748\n",
        "003749\n",
        "003751\n",
        "003752\n",
        "003758\n",
        "003759\n",
        "003763\n",
        "003767\n",
        "003773\n",
        "003779\n",
        "003781\n",
        "003784\n",
        "003786\n",
        "003788\n",
        "003790\n",
        "003792\n",
        "003797\n",
        "003806\n",
        "003807\n",
        "003811\n",
        "003817\n",
        "003818\n",
        "003824\n",
        "003827\n",
        "003828\n",
        "003830\n",
        "003834\n",
        "003835\n",
        "003847\n",
        "003849\n",
        "003856\n",
        "003859\n",
        "003860\n",
        "003861\n",
        "003865\n",
        "003866\n",
        "003874\n",
        "003879\n",
        "003887\n",
        "003889\n",
        "003890\n",
        "003898\n",
        "003899\n",
        "003907\n",
        "003912\n",
        "003913\n",
        "003921\n",
        "003932\n",
        "003935\n",
        "003936\n",
        "003939\n",
        "003945\n",
        "003949\n",
        "003953\n",
        "003956\n",
        "003961\n",
        "003969\n",
        "003970\n",
        "003971\n",
        "003974\n",
        "003983\n",
        "003987\n",
        "003988\n",
        "003991\n",
        "003993\n",
        "003997\n",
        "003998\n",
        "004005\n",
        "004008\n",
        "004009\n",
        "004013\n",
        "004014\n",
        "004016\n",
        "004017\n",
        "004019\n",
        "004023\n",
        "004028\n",
        "004033\n",
        "004034\n",
        "004035\n",
        "004037\n",
        "004046\n",
        "004052\n",
        "004058\n",
        "004067\n",
        "004091\n",
        "004093\n",
        "004095\n",
        "004100\n",
        "004106\n",
        "004111\n",
        "004120\n",
        "004121\n",
        "004129\n",
        "004131\n",
        "004133\n",
        "004136\n",
        "004137\n",
        "004138\n",
        "004140\n",
        "004146\n",
        "004149\n",
        "004152\n",
        "004158\n",
        "004163\n",
        "004164\n",
        "004168\n",
        "004169\n",
        "004170\n",
        "004171\n",
        "004189\n",
        "004190\n",
        "004196\n",
        "004200\n",
        "004209\n",
        "004215\n",
        "004220\n",
        "004221\n",
        "004223\n",
        "004224\n",
        "004228\n",
        "004231\n",
        "004232\n",
        "004237\n",
        "004241\n",
        "004242\n",
        "004244\n",
        "004247\n",
        "004253\n",
        "004255\n",
        "004256\n",
        "004263\n",
        "004269\n",
        "004270\n",
        "004271\n",
        "004272\n",
        "004273\n",
        "004280\n",
        "004281\n",
        "004283\n",
        "004287\n",
        "004291\n",
        "004292\n",
        "004296\n",
        "004300\n",
        "004303\n",
        "004307\n",
        "004315\n",
        "004318\n",
        "004322\n",
        "004325\n",
        "004327\n",
        "004333\n",
        "004338\n",
        "004339\n",
        "004345\n",
        "004347\n",
        "004359\n",
        "004360\n",
        "004361\n",
        "004365\n",
        "004367\n",
        "004370\n",
        "004371\n",
        "004372\n",
        "004376\n",
        "004379\n",
        "004386\n",
        "004387\n",
        "004389\n",
        "004391\n",
        "004392\n",
        "004404\n",
        "004434\n",
        "004436\n",
        "004439\n",
        "004441\n",
        "004452\n",
        "004470\n",
        "004471\n",
        "004479\n",
        "004481\n",
        "004484\n",
        "004496\n",
        "004500\n",
        "004502\n",
        "004508\n",
        "004510\n",
        "004514\n",
        "004517\n",
        "004519\n",
        "004520\n",
        "004524\n",
        "004526\n",
        "004537\n",
        "004540\n",
        "004544\n",
        "004548\n",
        "004549\n",
        "004551\n",
        "004553\n",
        "004562\n",
        "004563\n",
        "004565\n",
        "004566\n",
        "004570\n",
        "004571\n",
        "004576\n",
        "004579\n",
        "004584\n",
        "004587\n",
        "004591\n",
        "004595\n",
        "004597\n",
        "004604\n",
        "004605\n",
        "004607\n",
        "004611\n",
        "004612\n",
        "004622\n",
        "004623\n",
        "004625\n",
        "004627\n",
        "004628\n",
        "004631\n",
        "004634\n",
        "004636\n",
        "004643\n",
        "004644\n",
        "004648\n",
        "004651\n",
        "004656\n",
        "004671\n",
        "004675\n",
        "004679\n",
        "004683\n",
        "004685\n",
        "004686\n",
        "004687\n",
        "004691\n",
        "004693\n",
        "004694\n",
        "004701\n",
        "004702\n",
        "004705\n",
        "004706\n",
        "004710\n",
        "004714\n",
        "004715\n",
        "004718\n",
        "004723\n",
        "004735\n",
        "004737\n",
        "004742\n",
        "004743\n",
        "004747\n",
        "004748\n",
        "004753\n",
        "004754\n",
        "004760\n",
        "004773\n",
        "004776\n",
        "004779\n",
        "004782\n",
        "004783\n",
        "004790\n",
        "004792\n",
        "004793\n",
        "004794\n",
        "004797\n",
        "004799\n",
        "004801\n",
        "004808\n",
        "004815\n",
        "004823\n",
        "004828\n",
        "004830\n",
        "004832\n",
        "004836\n",
        "004837\n",
        "004841\n",
        "004842\n",
        "004846\n",
        "004848\n",
        "004849\n",
        "004857\n",
        "004869\n",
        "004873\n",
        "004876\n",
        "004879\n",
        "004882\n",
        "004885\n",
        "004897\n",
        "004898\n",
        "004902\n",
        "004905\n",
        "004907\n",
        "004910\n",
        "004911\n",
        "004913\n",
        "004929\n",
        "004946\n",
        "004951\n",
        "004955\n",
        "004958\n",
        "004961\n",
        "004962\n",
        "004966\n",
        "004968\n",
        "004972\n",
        "004973\n",
        "004974\n",
        "004976\n",
        "004984\n",
        "004987\n",
        "004990\n",
        "004991\n",
        "004992\n",
        "004995\n",
        "005001\n",
        "005004\n",
        "005006\n",
        "005007\n",
        "005016\n",
        "005018\n",
        "005020\n",
        "005023\n",
        "005024\n",
        "005026\n",
        "005027\n",
        "005029\n",
        "005032\n",
        "005033\n",
        "005045\n",
        "005047\n",
        "005052\n",
        "005057\n",
        "005058\n",
        "005061\n",
        "005065\n",
        "005068\n",
        "005071\n",
        "005073\n",
        "005078\n",
        "005084\n",
        "005086\n",
        "005090\n",
        "005093\n",
        "005094\n",
        "005097\n",
        "005101\n",
        "005107\n",
        "005108\n",
        "005114\n",
        "005121\n",
        "005122\n",
        "005124\n",
        "005129\n",
        "005130\n",
        "005134\n",
        "005138\n",
        "005143\n",
        "005153\n",
        "005156\n",
        "005168\n",
        "005169\n",
        "005171\n",
        "005173\n",
        "005177\n",
        "005181\n",
        "005183\n",
        "005186\n",
        "005189\n",
        "005190\n",
        "005191\n",
        "005202\n",
        "005203\n",
        "005208\n",
        "005215\n",
        "005217\n",
        "005219\n",
        "005223\n",
        "005231\n",
        "005236\n",
        "005244\n",
        "005245\n",
        "005246\n",
        "005257\n",
        "005258\n",
        "005259\n",
        "005260\n",
        "005262\n",
        "005269\n",
        "005273\n",
        "005283\n",
        "005285\n",
        "005288\n",
        "005290\n",
        "005292\n",
        "005297\n",
        "005303\n",
        "005304\n",
        "005307\n",
        "005311\n",
        "005318\n",
        "005327\n",
        "005336\n",
        "005337\n",
        "005338\n",
        "005344\n",
        "005345\n",
        "005351\n",
        "005358\n",
        "005360\n",
        "005363\n",
        "005368\n",
        "005369\n",
        "005373\n",
        "005374\n",
        "005387\n",
        "005388\n",
        "005389\n",
        "005391\n",
        "005396\n",
        "005404\n",
        "005405\n",
        "005406\n",
        "005408\n",
        "005410\n",
        "005413\n",
        "005414\n",
        "005417\n",
        "005420\n",
        "005424\n",
        "005433\n",
        "005440\n",
        "005445\n",
        "005448\n",
        "005450\n",
        "005451\n",
        "005453\n",
        "005455\n",
        "005457\n",
        "005467\n",
        "005478\n",
        "005483\n",
        "005487\n",
        "005489\n",
        "005496\n",
        "005499\n",
        "005508\n",
        "005509\n",
        "005511\n",
        "005514\n",
        "005515\n",
        "005519\n",
        "005524\n",
        "005526\n",
        "005527\n",
        "005536\n",
        "005541\n",
        "005542\n",
        "005544\n",
        "005547\n",
        "005563\n",
        "005566\n",
        "005568\n",
        "005574\n",
        "005579\n",
        "005582\n",
        "005585\n",
        "005591\n",
        "005592\n",
        "005599\n",
        "005600\n",
        "005601\n",
        "005603\n",
        "005605\n",
        "005609\n",
        "005611\n",
        "005624\n",
        "005625\n",
        "005630\n",
        "005631\n",
        "005636\n",
        "005637\n",
        "005639\n",
        "005644\n",
        "005648\n",
        "005654\n",
        "005658\n",
        "005668\n",
        "005669\n",
        "005680\n",
        "005686\n",
        "005695\n",
        "005697\n",
        "005699\n",
        "005700\n",
        "005704\n",
        "005705\n",
        "005710\n",
        "005713\n",
        "005715\n",
        "005718\n",
        "005728\n",
        "005730\n",
        "005731\n",
        "005735\n",
        "005738\n",
        "005740\n",
        "005742\n",
        "005752\n",
        "005756\n",
        "005757\n",
        "005764\n",
        "005765\n",
        "005769\n",
        "005780\n",
        "005782\n",
        "005783\n",
        "005784\n",
        "005786\n",
        "005789\n",
        "005803\n",
        "005805\n",
        "005806\n",
        "005813\n",
        "005814\n",
        "005817\n",
        "005821\n",
        "005824\n",
        "005826\n",
        "005831\n",
        "005836\n",
        "005838\n",
        "005840\n",
        "005843\n",
        "005850\n",
        "005851\n",
        "005859\n",
        "005860\n",
        "005861\n",
        "005864\n",
        "005867\n",
        "005873\n",
        "005881\n",
        "005884\n",
        "005885\n",
        "005888\n",
        "005889\n",
        "005893\n",
        "005895\n",
        "005899\n",
        "005901\n",
        "005903\n",
        "005905\n",
        "005908\n",
        "005909\n",
        "005910\n",
        "005911\n",
        "005918\n",
        "005920\n",
        "005923\n",
        "005930\n",
        "005938\n",
        "005947\n",
        "005948\n",
        "005951\n",
        "005960\n",
        "005961\n",
        "005964\n",
        "005971\n",
        "005980\n",
        "005983\n",
        "005984\n",
        "005990\n",
        "005992\n",
        "006004\n",
        "006009\n",
        "006011\n",
        "006020\n",
        "006023\n",
        "006025\n",
        "006030\n",
        "006033\n",
        "006038\n",
        "006043\n",
        "006061\n",
        "006065\n",
        "006066\n",
        "006067\n",
        "006070\n",
        "006073\n",
        "006074\n",
        "006078\n",
        "006079\n",
        "006088\n",
        "006091\n",
        "006095\n",
        "006096\n",
        "006100\n",
        "006103\n",
        "006104\n",
        "006105\n",
        "006123\n",
        "006128\n",
        "006130\n",
        "006131\n",
        "006134\n",
        "006135\n",
        "006140\n",
        "006141\n",
        "006156\n",
        "006158\n",
        "006162\n",
        "006166\n",
        "006170\n",
        "006171\n",
        "006172\n",
        "006174\n",
        "006175\n",
        "006176\n",
        "006177\n",
        "006179\n",
        "006180\n",
        "006181\n",
        "006183\n",
        "006187\n",
        "006189\n",
        "006196\n",
        "006208\n",
        "006210\n",
        "006221\n",
        "006223\n",
        "006224\n",
        "006225\n",
        "006229\n",
        "006230\n",
        "006236\n",
        "006238\n",
        "006243\n",
        "006247\n",
        "006250\n",
        "006251\n",
        "006261\n",
        "006262\n",
        "006264\n",
        "006267\n",
        "006270\n",
        "006272\n",
        "006275\n",
        "006279\n",
        "006285\n",
        "006289\n",
        "006290\n",
        "006291\n",
        "006299\n",
        "006304\n",
        "006305\n",
        "006320\n",
        "006329\n",
        "006341\n",
        "006344\n",
        "006349\n",
        "006352\n",
        "006353\n",
        "006362\n",
        "006363\n",
        "006366\n",
        "006367\n",
        "006369\n",
        "006371\n",
        "006374\n",
        "006375\n",
        "006381\n",
        "006382\n",
        "006395\n",
        "006400\n",
        "006411\n",
        "006417\n",
        "006418\n",
        "006419\n",
        "006427\n",
        "006429\n",
        "006433\n",
        "006434\n",
        "006436\n",
        "006438\n",
        "006447\n",
        "006448\n",
        "006455\n",
        "006458\n",
        "006459\n",
        "006462\n",
        "006466\n",
        "006470\n",
        "006472\n",
        "006474\n",
        "006475\n",
        "006476\n",
        "006482\n",
        "006483\n",
        "006486\n",
        "006495\n",
        "006499\n",
        "006501\n",
        "006503\n",
        "006506\n",
        "006515\n",
        "006523\n",
        "006524\n",
        "006536\n",
        "006547\n",
        "006548\n",
        "006549\n",
        "006550\n",
        "006551\n",
        "006556\n",
        "006560\n",
        "006564\n",
        "006569\n",
        "006595\n",
        "006597\n",
        "006602\n",
        "006605\n",
        "006609\n",
        "006610\n",
        "006612\n",
        "006622\n",
        "006626\n",
        "006627\n",
        "006635\n",
        "006636\n",
        "006637\n",
        "006638\n",
        "006648\n",
        "006652\n",
        "006654\n",
        "006658\n",
        "006660\n",
        "006674\n",
        "006684\n",
        "006689\n",
        "006694\n",
        "006695\n",
        "006697\n",
        "006698\n",
        "006703\n",
        "006704\n",
        "006706\n",
        "006707\n",
        "006708\n",
        "006714\n",
        "006726\n",
        "006727\n",
        "006731\n",
        "006734\n",
        "006735\n",
        "006736\n",
        "006738\n",
        "006740\n",
        "006748\n",
        "006753\n",
        "006755\n",
        "006766\n",
        "006773\n",
        "006777\n",
        "006781\n",
        "006782\n",
        "006784\n",
        "006794\n",
        "006805\n",
        "006806\n",
        "006810\n",
        "006822\n",
        "006824\n",
        "006825\n",
        "006833\n",
        "006836\n",
        "006839\n",
        "006840\n",
        "006844\n",
        "006845\n",
        "006847\n",
        "006848\n",
        "006849\n",
        "006852\n",
        "006858\n",
        "006864\n",
        "006866\n",
        "006868\n",
        "006869\n",
        "006874\n",
        "006883\n",
        "006887\n",
        "006893\n",
        "006896\n",
        "006899\n",
        "006900\n",
        "006909\n",
        "006910\n",
        "006911\n",
        "006912\n",
        "006914\n",
        "006916\n",
        "006917\n",
        "006919\n",
        "006930\n",
        "006931\n",
        "006939\n",
        "006943\n",
        "006947\n",
        "006948\n",
        "006950\n",
        "006958\n",
        "006959\n",
        "006968\n",
        "006971\n",
        "006976\n",
        "006983\n",
        "007002\n",
        "007003\n",
        "007006\n",
        "007007\n",
        "007011\n",
        "007016\n",
        "007018\n",
        "007023\n",
        "007025\n",
        "007029\n",
        "007033\n",
        "007036\n",
        "007039\n",
        "007040\n",
        "007045\n",
        "007050\n",
        "007062\n",
        "007064\n",
        "007072\n",
        "007073\n",
        "007075\n",
        "007078\n",
        "007079\n",
        "007080\n",
        "007088\n",
        "007089\n",
        "007090\n",
        "007092\n",
        "007093\n",
        "007095\n",
        "007105\n",
        "007108\n",
        "007113\n",
        "007121\n",
        "007125\n",
        "007128\n",
        "007129\n",
        "007130\n",
        "007133\n",
        "007138\n",
        "007150\n",
        "007152\n",
        "007154\n",
        "007159\n",
        "007163\n",
        "007166\n",
        "007168\n",
        "007177\n",
        "007180\n",
        "007182\n",
        "007184\n",
        "007185\n",
        "007193\n",
        "007194\n",
        "007197\n",
        "007205\n",
        "007213\n",
        "007214\n",
        "007219\n",
        "007222\n",
        "007223\n",
        "007234\n",
        "007241\n",
        "007243\n",
        "007250\n",
        "007256\n",
        "007261\n",
        "007263\n",
        "007271\n",
        "007279\n",
        "007285\n",
        "007289\n",
        "007295\n",
        "007298\n",
        "007305\n",
        "007308\n",
        "007322\n",
        "007323\n",
        "007325\n",
        "007327\n",
        "007334\n",
        "007336\n",
        "007351\n",
        "007361\n",
        "007365\n",
        "007369\n",
        "007370\n",
        "007373\n",
        "007375\n",
        "007381\n",
        "007385\n",
        "007389\n",
        "007394\n",
        "007396\n",
        "007398\n",
        "007410\n",
        "007411\n",
        "007413\n",
        "007417\n",
        "007419\n",
        "007421\n",
        "007425\n",
        "007431\n",
        "007437\n",
        "007446\n",
        "007454\n",
        "007458\n",
        "007466\n",
        "007467\n",
        "007468\n",
        "007474\n",
        "007477\n",
        "007479\n",
        "007481\n",
        "007483\n",
        "007490\n",
        "007491\n",
        "007493\n",
        "007497\n",
        "007503\n",
        "007513\n",
        "007519\n",
        "007521\n",
        "007524\n",
        "007530\n",
        "007535\n",
        "007536\n",
        "007538\n",
        "007540\n",
        "007544\n",
        "007565\n",
        "007566\n",
        "007570\n",
        "007572\n",
        "007575\n",
        "007578\n",
        "007586\n",
        "007590\n",
        "007594\n",
        "007600\n",
        "007601\n",
        "007606\n",
        "007611\n",
        "007619\n",
        "007621\n",
        "007629\n",
        "007631\n",
        "007633\n",
        "007637\n",
        "007653\n",
        "007654\n",
        "007655\n",
        "007663\n",
        "007667\n",
        "007683\n",
        "007685\n",
        "007692\n",
        "007696\n",
        "007697\n",
        "007699\n",
        "007704\n",
        "007713\n",
        "007718\n",
        "007721\n",
        "007729\n",
        "007731\n",
        "007735\n",
        "007736\n",
        "007740\n",
        "007748\n",
        "007749\n",
        "007751\n",
        "007753\n",
        "007762\n",
        "007767\n",
        "007775\n",
        "007777\n",
        "007781\n",
        "007790\n",
        "007791\n",
        "007795\n",
        "007803\n",
        "007809\n",
        "007810\n",
        "007814\n",
        "007819\n",
        "007820\n",
        "007821\n",
        "007831\n",
        "007836\n",
        "007838\n",
        "007840\n",
        "007847\n",
        "007853\n",
        "007854\n",
        "007859\n",
        "007863\n",
        "007864\n",
        "007872\n",
        "007876\n",
        "007878\n",
        "007883\n",
        "007885\n",
        "007898\n",
        "007900\n",
        "007901\n",
        "007905\n",
        "007908\n",
        "007910\n",
        "007911\n",
        "007914\n",
        "007915\n",
        "007923\n",
        "007925\n",
        "007926\n",
        "007932\n",
        "007939\n",
        "007940\n",
        "007953\n",
        "007959\n",
        "007963\n",
        "007964\n",
        "007968\n",
        "007974\n",
        "007976\n",
        "007980\n",
        "007991\n",
        "007996\n",
        "008001\n",
        "008004\n",
        "008005\n",
        "008008\n",
        "008012\n",
        "008017\n",
        "008019\n",
        "008026\n",
        "008037\n",
        "008040\n",
        "008042\n",
        "008043\n",
        "008044\n",
        "008049\n",
        "008051\n",
        "008053\n",
        "008062\n",
        "008063\n",
        "008064\n",
        "008067\n",
        "008072\n",
        "008075\n",
        "008076\n",
        "008079\n",
        "008082\n",
        "008083\n",
        "008084\n",
        "008093\n",
        "008095\n",
        "008096\n",
        "008098\n",
        "008106\n",
        "008108\n",
        "008116\n",
        "008117\n",
        "008121\n",
        "008127\n",
        "008130\n",
        "008137\n",
        "008139\n",
        "008142\n",
        "008150\n",
        "008163\n",
        "008164\n",
        "008166\n",
        "008169\n",
        "008174\n",
        "008186\n",
        "008188\n",
        "008197\n",
        "008199\n",
        "008202\n",
        "008203\n",
        "008204\n",
        "008213\n",
        "008216\n",
        "008218\n",
        "008223\n",
        "008226\n",
        "008232\n",
        "008235\n",
        "008248\n",
        "008250\n",
        "008252\n",
        "008253\n",
        "008254\n",
        "008260\n",
        "008261\n",
        "008262\n",
        "008263\n",
        "008269\n",
        "008272\n",
        "008280\n",
        "008282\n",
        "008296\n",
        "008301\n",
        "008302\n",
        "008310\n",
        "008311\n",
        "008312\n",
        "008313\n",
        "008315\n",
        "008316\n",
        "008317\n",
        "008322\n",
        "008332\n",
        "008336\n",
        "008338\n",
        "008341\n",
        "008342\n",
        "008346\n",
        "008351\n",
        "008360\n",
        "008372\n",
        "008374\n",
        "008381\n",
        "008384\n",
        "008385\n",
        "008388\n",
        "008391\n",
        "008397\n",
        "008398\n",
        "008403\n",
        "008409\n",
        "008422\n",
        "008425\n",
        "008426\n",
        "008427\n",
        "008437\n",
        "008442\n",
        "008443\n",
        "008445\n",
        "008449\n",
        "008452\n",
        "008453\n",
        "008456\n",
        "008462\n",
        "008465\n",
        "008466\n",
        "008467\n",
        "008468\n",
        "008470\n",
        "008475\n",
        "008477\n",
        "008478\n",
        "008482\n",
        "008483\n",
        "008495\n",
        "008506\n",
        "008517\n",
        "008523\n",
        "008529\n",
        "008530\n",
        "008533\n",
        "008536\n",
        "008549\n",
        "008550\n",
        "008558\n",
        "008559\n",
        "008568\n",
        "008581\n",
        "008585\n",
        "008587\n",
        "008588\n",
        "008595\n",
        "008596\n",
        "008602\n",
        "008610\n",
        "008615\n",
        "008617\n",
        "008618\n",
        "008628\n",
        "008633\n",
        "008645\n",
        "008655\n",
        "008663\n",
        "008665\n",
        "008670\n",
        "008676\n",
        "008688\n",
        "008691\n",
        "008699\n",
        "008702\n",
        "008706\n",
        "008710\n",
        "008720\n",
        "008723\n",
        "008725\n",
        "008727\n",
        "008731\n",
        "008732\n",
        "008738\n",
        "008741\n",
        "008744\n",
        "008748\n",
        "008750\n",
        "008755\n",
        "008756\n",
        "008757\n",
        "008760\n",
        "008764\n",
        "008768\n",
        "008770\n",
        "008771\n",
        "008776\n",
        "008783\n",
        "008784\n",
        "008790\n",
        "008794\n",
        "008806\n",
        "008809\n",
        "008811\n",
        "008813\n",
        "008814\n",
        "008815\n",
        "008819\n",
        "008838\n",
        "008840\n",
        "008841\n",
        "008847\n",
        "008856\n",
        "008862\n",
        "008865\n",
        "008872\n",
        "008878\n",
        "008879\n",
        "008883\n",
        "008885\n",
        "008886\n",
        "008891\n",
        "008900\n",
        "008905\n",
        "008909\n",
        "008920\n",
        "008923\n",
        "008926\n",
        "008929\n",
        "008930\n",
        "008932\n",
        "008933\n",
        "008936\n",
        "008939\n",
        "008944\n",
        "008948\n",
        "008958\n",
        "008960\n",
        "008961\n",
        "008962\n",
        "008966\n",
        "008967\n",
        "008968\n",
        "008969\n",
        "008970\n",
        "008971\n",
        "008973\n",
        "008975\n",
        "008978\n",
        "008979\n",
        "008980\n",
        "008985\n",
        "008987\n",
        "008988\n",
        "008989\n",
        "008995\n",
        "008999\n",
        "009000\n",
        "009004\n",
        "009005\n",
        "009016\n",
        "009018\n",
        "009020\n",
        "009027\n",
        "009029\n",
        "009032\n",
        "009036\n",
        "009042\n",
        "009045\n",
        "009049\n",
        "009058\n",
        "009059\n",
        "009063\n",
        "009066\n",
        "009068\n",
        "009073\n",
        "009078\n",
        "009080\n",
        "009086\n",
        "009098\n",
        "009099\n",
        "009100\n",
        "009106\n",
        "009108\n",
        "009114\n",
        "009117\n",
        "009121\n",
        "009123\n",
        "009136\n",
        "009144\n",
        "009148\n",
        "009153\n",
        "009160\n",
        "009161\n",
        "009166\n",
        "009173\n",
        "009175\n",
        "009181\n",
        "009184\n",
        "009185\n",
        "009191\n",
        "009196\n",
        "009197\n",
        "009200\n",
        "009205\n",
        "009208\n",
        "009209\n",
        "009214\n",
        "009215\n",
        "009218\n",
        "009227\n",
        "009230\n",
        "009238\n",
        "009242\n",
        "009245\n",
        "009251\n",
        "009252\n",
        "009255\n",
        "009259\n",
        "009269\n",
        "009270\n",
        "009271\n",
        "009272\n",
        "009283\n",
        "009285\n",
        "009287\n",
        "009288\n",
        "009289\n",
        "009290\n",
        "009295\n",
        "009296\n",
        "009299\n",
        "009306\n",
        "009307\n",
        "009308\n",
        "009316\n",
        "009318\n",
        "009324\n",
        "009325\n",
        "009327\n",
        "009333\n",
        "009336\n",
        "009339\n",
        "009342\n",
        "009343\n",
        "009358\n",
        "009359\n",
        "009362\n",
        "009365\n",
        "009377\n",
        "009386\n",
        "009388\n",
        "009389\n",
        "009392\n",
        "009393\n",
        "009394\n",
        "009398\n",
        "009406\n",
        "009407\n",
        "009409\n",
        "009410\n",
        "009411\n",
        "009413\n",
        "009417\n",
        "009418\n",
        "009419\n",
        "009420\n",
        "009421\n",
        "009422\n",
        "009424\n",
        "009429\n",
        "009432\n",
        "009434\n",
        "009446\n",
        "009458\n",
        "009460\n",
        "009463\n",
        "009465\n",
        "009466\n",
        "009469\n",
        "009476\n",
        "009488\n",
        "009490\n",
        "009491\n",
        "009496\n",
        "009497\n",
        "009499\n",
        "009504\n",
        "009508\n",
        "009512\n",
        "009515\n",
        "009516\n",
        "009518\n",
        "009520\n",
        "009523\n",
        "009524\n",
        "009526\n",
        "009528\n",
        "009537\n",
        "009541\n",
        "009542\n",
        "009545\n",
        "009549\n",
        "009551\n",
        "009557\n",
        "009562\n",
        "009566\n",
        "009573\n",
        "009576\n",
        "009577\n",
        "009579\n",
        "009584\n",
        "009585\n",
        "009587\n",
        "009596\n",
        "009600\n",
        "009605\n",
        "009609\n",
        "009613\n",
        "009614\n",
        "009615\n",
        "009618\n",
        "009621\n",
        "009623\n",
        "009629\n",
        "009634\n",
        "009637\n",
        "009638\n",
        "009644\n",
        "009650\n",
        "009654\n",
        "009656\n",
        "009659\n",
        "009664\n",
        "009666\n",
        "009668\n",
        "009671\n",
        "009679\n",
        "009684\n",
        "009691\n",
        "009693\n",
        "009702\n",
        "009703\n",
        "009707\n",
        "009709\n",
        "009713\n",
        "009717\n",
        "009718\n",
        "009721\n",
        "009729\n",
        "009733\n",
        "009734\n",
        "009735\n",
        "009749\n",
        "009755\n",
        "009756\n",
        "009762\n",
        "009763\n",
        "009774\n",
        "009776\n",
        "009789\n",
        "009790\n",
        "009792\n",
        "009797\n",
        "009800\n",
        "009805\n",
        "009807\n",
        "009808\n",
        "009810\n",
        "009813\n",
        "009825\n",
        "009828\n",
        "009830\n",
        "009832\n",
        "009834\n",
        "009839\n",
        "009842\n",
        "009845\n",
        "009848\n",
        "009851\n",
        "009852\n",
        "009855\n",
        "009859\n",
        "009860\n",
        "009867\n",
        "009868\n",
        "009869\n",
        "009872\n",
        "009874\n",
        "009877\n",
        "009878\n",
        "009879\n",
        "009882\n",
        "009884\n",
        "009887\n",
        "009896\n",
        "009904\n",
        "009911\n",
        "009918\n",
        "009920\n",
        "009926\n",
        "009938\n",
        "009940\n",
        "009942\n",
        "009944\n",
        "009945\n",
        "009949\n",
        "009959\n",
        "009961"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "545a3aca-7103-48df-dcc4-fa2d6e12c113",
        "id": "6b62mCvG9wVC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./ImageSets/Main/train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee4T5D9z06S-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title data handling { form-width: \"20%\" }\n",
        "\n",
        "class TransformAnnotation(object):\n",
        "    def __init__(self, keep_difficult=False):\n",
        "        self.keep_difficult = keep_difficult\n",
        "\n",
        "    def __call__(self, target):\n",
        "        res = []\n",
        "        for obj in target.iter('object'):\n",
        "            difficult = int(obj.find('difficult').text) == 1\n",
        "            if not self.keep_difficult and difficult:\n",
        "                continue\n",
        "            name = obj[0].text.lower().strip()\n",
        "            bbox = obj[4]\n",
        "            bndbox = [int(bb.text)-1 for bb in bbox]\n",
        "            res += [bndbox + [name]]\n",
        "        return res\n",
        "    \n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, path, input_shape=(416, 416), mode='train', dataset='voc'):\n",
        "        self.path = path\n",
        "        self.mode = mode\n",
        "        self.S = input_shape[0]\n",
        "        self.transform = Compose([Resize(input_shape), ToTensor()])\n",
        "        self.target_transform = TransformAnnotation()\n",
        "        \n",
        "        f_path = os.path.join(path, 'ImageSets/Main/train.txt')\n",
        "        self.files = [i.strip('\\n') for i in open(f_path).readlines()]\n",
        "        random.seed(42)\n",
        "        random.shuffle(self.files)\n",
        "        \n",
        "        self.ip = os.path.join(path, 'JPEGImages/')\n",
        "        self.lp = os.path.join(path, 'Annotations/')\n",
        "        \n",
        "        self.classes = [c.strip() for c in open(f'{dataset}_classes.txt').readlines()]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        f = self.files[i]\n",
        "        x = Image.open(self.ip + f + '.jpg').convert('RGB')\n",
        "        W, H = x.size\n",
        "        x = self.transform(x)\n",
        "        if self.mode=='test':\n",
        "            return x\n",
        "        else:\n",
        "            y = ET.parse(self.lp + f + '.xml').getroot()\n",
        "            y = self.target_transform(y)\n",
        "            for yi in y:\n",
        "                x1, y1, x2, y2, c = yi\n",
        "                ci = self.classes.index(c)\n",
        "                w, h = x2-x1, y2-y1\n",
        "                cx, cy = x2-w/2, y2-h/2\n",
        "                yi[:] = [cx, cy, w, h, ci]\n",
        "            \n",
        "            scale = torch.tensor([1/W, 1/H, 1/W, 1/H, 1])\n",
        "            y = torch.tensor(y)*scale\n",
        "            return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def classes(self,):\n",
        "        return self.classes\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title calculators { form-width: \"20%\" }\n",
        " \n",
        "def toTensor(image, size):\n",
        "    image_ = image.resize(size)\n",
        "    x = ToTensor()(image_).unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "def IoUnion(boxA, boxB):\n",
        "    Acx, Acy, Aw, Ah = boxA[:4]\n",
        "    Bcx, Bcy, Bw, Bh = boxB[:4]\n",
        "    \n",
        "    x1 = max(Acx - Aw/2, Bcx - Bw/2)\n",
        "    x2 = min(Acx + Aw/2, Bcx + Bw/2)\n",
        "    y1 = max(Acy - Ah/2, Bcy - Bh/2)\n",
        "    y2 = min(Acy + Ah/2, Bcy + Bh/2)\n",
        "\n",
        "    w_cross = max(x2 - x1, 0)\n",
        "    h_cross = max(y2 - y1, 0)\n",
        "\n",
        "    Sa = Aw * Ah\n",
        "    Sb = Bw * Bh\n",
        "    intersection = w_cross * h_cross\n",
        "    union = Sa + Sa - intersection\n",
        "    iou = intersection/union\n",
        "    return iou\n",
        "\n",
        "def SoftIoUnion(boxesA, boxesB):\n",
        "\n",
        "    Acx, Acy, Aw, Ah = boxesA[0], boxesA[1], boxesA[2], boxesA[3]\n",
        "    Bcx, Bcy, Bw, Bh = boxesB[0], boxesB[1], boxesB[2], boxesB[3]\n",
        "    \n",
        "    x1 = torch.max(Acx - Aw/2, Bcx - Bw/2)\n",
        "    x2 = torch.min(Acx + Aw/2, Bcx + Bw/2)\n",
        "    y1 = torch.max(Acy - Ah/2, Bcy - Bh/2)\n",
        "    y2 = torch.min(Acy + Ah/2, Bcy + Bh/2)\n",
        "\n",
        "    W_ab = torch.max(x2 - x1, torch.zeros(len(x2)))\n",
        "    H_ab = torch.max(y2 - y1, torch.zeros(len(y2)))\n",
        "    \n",
        "    Sa = Aw * Ah\n",
        "    Sb = Bw * Bh\n",
        "    intersection = W_ab * H_ab\n",
        "    union = Sa + Sb - intersection\n",
        "    return intersection/union\n",
        "    \n",
        "# be aware, that this particular NMS function will be used only for the pruning of predictions\n",
        "# NMS involved in training is separately implemented as a part of the model below \n",
        "# be aware, in NMS, we find IoU between two predictions bounding boxes, instead of prediction vs ground truth \n",
        "# we iteratively take box(es) with highest objectnes score and eliminate all other boxes that overlap \n",
        "# with it more than 0.5, basically we prune the (nearest=0.5 IoU) neigbourhood. \n",
        "def NonMaxSuppression(boxes: torch.tensor, IoU:float =0.5):  # boxes: torch.tensor, thresh_iou : float\n",
        "    if len(boxes) == 0:\n",
        "        return boxes\n",
        "\n",
        "    objectnesScores = [(1-b[4]) for b in boxes]    # b[4] contains the objectnes score for the box. \n",
        "    sorted_idx = np.argsort(objectnesScores)       # best scores are first as 1-b[4] gives lowest value for highest score as scores are between 0-1  \n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box_i = boxes[sorted_idx[i]]               #takes the box with highest(modified to lowest as trick) score\n",
        "        if objectnesScores[i] > -1:                # you will understand this in the end of the 2nd loop\n",
        "            preds.append(box_i)                    #put the best scored box in a separate list that was empthy        \n",
        "            for j in range(i+1, len(boxes)):       #loop in all following boxes\n",
        "                if objectnesScores[j] > -1:        # you will understand this in the end of the 2nd loop\n",
        "                    box_j = boxes[sorted_idx[j]]   #just take the following boxes to test if the \n",
        "                    if IoUnion(box_i, box_j) > IoU:  #IoU of following boxes vs best score box is more than 0.5\n",
        "                        objectnesScores[j] = -1    #if IoU with non best scoring box is above 0.5 it's eliminated\n",
        "    return preds\n",
        "\n",
        "def YoloHead(preds):\n",
        "    preds = np.array(preds)\n",
        "    # Select Boxes, objectnesScores and Classes\n",
        "    boxes = preds[:, :4]\n",
        "    objectnesScores = preds[:, 4]\n",
        "    classes = preds[:, -1].astype('int')\n",
        "    # Boxes to Corners\n",
        "    cx, cy, w, h = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
        "    x1, y1 = cx - w/2, cy - h/2\n",
        "    x2, y2 = cx + w/2, cy + h/2\n",
        "    boxes = np.stack((x1, y1, x2, y2), axis=1)\n",
        "    \n",
        "    return objectnesScores, boxes, classes\n",
        "\n",
        "def DrawBoxes(img, objectnesScores, boxes, classes, dataset='voc'):\n",
        "    # Copy Image (to exclude changes in the original image)\n",
        "    image = img.copy()\n",
        "    W, H = image.size\n",
        "    scale = np.array([W, H, W, H])\n",
        "    thickness = (W + H)//300\n",
        "    # Import Classes\n",
        "    font = ImageFont.load_default() \n",
        "    class_names = [c.strip() for c in open(f'{dataset}_classes.txt').readlines()]\n",
        "    colors = np.load(f'{dataset}_colors.npy') #not (same, allow_pickle=True)\n",
        "    # Cycle through the Classes\n",
        "    for i, c in reversed(list(enumerate(classes))):\n",
        "        color = colors[c]\n",
        "        cls = class_names[c]\n",
        "        box = boxes[i]\n",
        "        score = objectnesScores[i]\n",
        "        # Box`s Label\n",
        "        label = '{} {:.2f}'.format(cls, score)\n",
        "        # Draw Image\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        size = draw.textsize(label, font)\n",
        "        # Choose Frontiers\n",
        "        left, top, right, bottom = box *scale\n",
        "        # Exclude out of the image situations\n",
        "        left = max(0, int(left))\n",
        "        top = max(0, int(top))\n",
        "        right = min(W, int(right))\n",
        "        bottom = min(H, int(bottom))\n",
        "\n",
        "        if top - size[1] >= 0:\n",
        "            origin = np.array([left, top - size[1]])\n",
        "        else:\n",
        "            origin = np.array([left, top + 1])\n",
        "            \n",
        "        # Draw Frame\n",
        "        for k in range(thickness):\n",
        "            draw.rectangle([left + k, top + k, right - k, bottom - k], outline=tuple(color)) #outline=tuple(color) if custom colors\n",
        "        # Draw Label\n",
        "        draw.rectangle([tuple(origin), tuple(origin + size)], fill=tuple(color)) #fill=tuple(color) if custom colors\n",
        "        draw.text(tuple(origin), label, fill=(0, 0, 0), font=font)   \n",
        "        del draw\n",
        "    return image\n",
        "\n",
        "def PictureDetection(model, img, size=(416,416), threshold=.25, IoU=.5, dataset='voc'):\n",
        "    if type(img) is str:\n",
        "        image = Image.open(img).convert('RGB')\n",
        "        x = toTensor(image, size)\n",
        "    else:\n",
        "        image = ToPILImage()(img)\n",
        "        x = ToTensor()(image.resize(size))\n",
        "    preds = model.predict(x, threshold)[0]\n",
        "    if len(preds) != 0:\n",
        "        preds = NonMaxSuppression(preds, IoU)\n",
        "        objectnesScores, boxes, classes = YoloHead(preds)\n",
        "        image = DrawBoxes(image, objectnesScores, boxes, classes, dataset)\n",
        "    return image\n",
        "\n",
        "def VideoDetection(model, img, size=(416,416), threshold=.25, IoU=.5, dataset='voc'):\n",
        "    image = ToPILImage()(img)\n",
        "    x = ToTensor()(image.resize(size))\n",
        "    preds = model.predict(x, threshold)[0]\n",
        "    preds = NonMaxSuppression(preds, IoU)\n",
        "    if len(preds) != 0:\n",
        "        objectnesScores, boxes, classes = YoloHead(preds)\n",
        "        image = DrawBoxes(image, objectnesScores, boxes, classes, dataset)\n",
        "    return image\n",
        "\n",
        "def PlotSample(x, y, dataset='voc'):\n",
        "    x = ToPILImage()(x)\n",
        "    y = y.numpy()\n",
        "    W, H = x.size\n",
        "    scale = np.array((W, H, W, H))\n",
        "    thickness = (W + H)//300\n",
        "    font = ImageFont.load_default() \n",
        "    classes = [c.strip() for c in open(f'{dataset}_classes.txt').readlines()]\n",
        "    colors = np.load(f'{dataset}_colors.npy')\n",
        "\n",
        "    for yi in y:\n",
        "        draw = ImageDraw.Draw(x)\n",
        "        cls=classes[int(yi[-1])]\n",
        "        color=colors[int(yi[-1])]\n",
        "        cx, cy, w, h = yi[ :4]*scale\n",
        "        x1 = cx - w/2\n",
        "        y1 = cy - h/2\n",
        "        x2 = cx + w/2\n",
        "        y2 = cy + h/2\n",
        "        size = draw.textsize(cls, font)  #draw.textsize(cls, font)\n",
        "        \n",
        "        if y1 - size[1] >= 0:\n",
        "            origin = np.array([x1, y1-size[1]])\n",
        "        else:\n",
        "            origin = np.array([x1, y1+1])\n",
        "        \n",
        "        for k in range(thickness):\n",
        "            draw.rectangle([x1+k, y1+k, x2-k, y2-k], outline=tuple(color))\n",
        "        # Draw Label\n",
        "        draw.rectangle([tuple(origin), tuple(origin+size)], fill=tuple(color))\n",
        "        draw.text(tuple(origin), cls, fill=(0, 0, 0), font=font) #draw.text(origin, cls, fill=(0, 0, 0), font=font)\n",
        "        del draw\n",
        "    return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "i1MH7h1A15-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title model { form-width: \"20%\" }   \n",
        "\n",
        "\n",
        "class YoloLayer(torch.nn.Module):\n",
        "    def __init__(self, anchors, stride, num_classes): # This layer is used at 3 different stages giving 3 different scales of an image, which is specific for yolo v.3\n",
        "        super().__init__()\n",
        "        self.anchors=torch.FloatTensor(anchors)/stride\n",
        "        self.stride=stride\n",
        "        self.num_classes=num_classes\n",
        "\n",
        "    #here we extract boxes from the output of the network (output holding objectnes, class, ancor predictions)\n",
        "    #we do this at 3 different stages giving us 3 different scales of an image, which is specific for yolo v.3\n",
        "    def get_region_boxes(self, output, threshold):         \n",
        "        if output.dim() == 3: output = output.unsqueeze(0)  \n",
        "        device = output.device                             # torch.device(torch_device)\n",
        "        anchors = self.anchors.to(device)\n",
        "        \n",
        "        B, c, H, W = output.size()\n",
        "        A = len(anchors)\n",
        "        C = self.num_classes\n",
        "        Ad = B*A*H*W\n",
        "\n",
        "        assert c == (C+5)*A\n",
        "\n",
        "        #from output (in C+5 this 5 are x,y,w,h,box & Gx size is Ad) we make [(C+5) X Ad]\n",
        "        output = output.view(B*A, C+5, H*W).transpose(0,1).contiguous().view(C+5, Ad)\n",
        "\n",
        "        Gx = torch.arange(0, W).repeat(B*A, H).view(Ad).to(device)\n",
        "        Gy = torch.arange(0, H).repeat(W, 1).t().repeat(B*A, 1).view(Ad).to(device)\n",
        "        \n",
        "        ix = torch.LongTensor(range(0,2)).to(device)\n",
        "        Aw = anchors.index_select(1, ix[0]).repeat(1, B, H*W).view(Ad) #select from indexing dim 1, 0th element(-s) \n",
        "        Ah = anchors.index_select(1, ix[1]).repeat(1, B, H*W).view(Ad)\n",
        "\n",
        "        # Gx and Gy represent left corner of the grid box, while output[0], output[1] are predicted anchor x,y\n",
        "        # Aw, AH are real anchorbox dimensions relative to anchor and \n",
        "        # output[2], output[3] are predicted anchor box dimensions relative to anchor\n",
        "        xs = torch.sigmoid(output[0]) + Gx   #we use sigmoid because predicted x,y are relative to Gx,Gy and\n",
        "        ys = torch.sigmoid(output[1]) + Gy   #left top corner Gx,Gy + some fraction. Without sigmoid this breakes\n",
        "        ws = torch.exp(output[2]) * Aw.detach()  # output[2] predicted Aw groud truth\n",
        "        hs = torch.exp(output[3]) * Ah.detach()\n",
        "        box_conf = torch.sigmoid(output[4])  # objectnes score is passed in sigmoid to interpret it as probability\n",
        "\n",
        "        cls_conf = torch.nn.Softmax(dim=1)(output[5: C+5].transpose(0,1)).detach()\n",
        "        cls_prob, cls_id = torch.max(cls_conf, 1)\n",
        "        cls_prob = cls_prob.view(-1)\n",
        "        cls_id = cls_id.view(-1)\n",
        "\n",
        "        xs = xs.to('cpu')\n",
        "        ys = ys.to('cpu')\n",
        "        ws = ws.to('cpu')\n",
        "        hs = hs.to('cpu')\n",
        "        box_conf = box_conf.to('cpu') \n",
        "        cls_prob = cls_prob.to('cpu')\n",
        "        cls_id = cls_id.to('cpu')\n",
        "\n",
        "\n",
        "        boxes = [[] for i in range(B)]\n",
        "        \n",
        "        # first we filter output by objectnes score (confidence of having object(-s part) in receptive field)\n",
        "        # and we ignore below treshold scored output boxes -- confidence tresholding NOT IoU !!!\n",
        "        idx = torch.LongTensor(range(0,len(box_conf)))\n",
        "        for i in idx[box_conf > threshold]:   #just take only those boxes with confidence score also aboeve some given score\n",
        "            cx = xs[i]                        # NOT IoU threshold !!! do not mix this and IoU tresholds\n",
        "            cy = ys[i]\n",
        "            w = ws[i]\n",
        "            h = hs[i]\n",
        "            \n",
        "            box = [cx/W, cy/H, w/W, h/H, box_conf[i], cls_prob[i], cls_id[i]]\n",
        "            box = [i.item() for i in box]   \n",
        "            \n",
        "            batch = int(i.item()/(A*H*W))    # A*H*W is number of all anchors for each cell for one picture, so\n",
        "            boxes[batch].append(box)         # each pictures bboxes will accumulate together\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    \n",
        "    def build_targets(self, pred_boxes, target, anchors, H, W): \n",
        "      #     all this is for: \n",
        "      #     1. create negative zero-masks to push down best predicted boxes overlaping with ground truth boxes!  \n",
        "      #        As they are false concurents for ground truths. Call this modified non-maximum supression, NMS,\n",
        "      #        because ground truth acts like maximum value element and we supress all best concurent boxes.     \n",
        "      #     2. create positive target (mask=1 etc) masks from ground truths.\n",
        "      #     3. Losses. MSE for box and object confidence (objectivnes) losses and CrossEntropy for class loss\n",
        "      #        Here we push up energy to 1-s to positive 1-s target masks and implicitely also push down \n",
        "      #        false concurents to 0-s as are zero-masked using modified non-maximum supression, NMS.\n",
        "\n",
        "\n",
        "        ignore_threshold = 0.5\n",
        "\n",
        "        # Works faster on CPU than on GPU.\n",
        "        dev = torch.device('cpu')\n",
        "        pred_boxes = pred_boxes.to(dev)\n",
        "        target = target.to(dev)\n",
        "        anchors = anchors.to(dev)\n",
        "\n",
        "        B = target.size(0)\n",
        "        A, As = anchors.size()\n",
        "        Ad = A*H*W\n",
        "        \n",
        "        box_mask   = torch.zeros(B, A, H, W)\n",
        "        conf_mask  = torch.ones (B, A, H, W)\n",
        "        cls_mask   = torch.zeros(B, A, H, W)\n",
        "        t_boxes    = torch.zeros(4, B, A, H, W)\n",
        "        t_conf     = torch.zeros(B, A, H, W)\n",
        "        t_cls      = torch.zeros(B, A, H, W)\n",
        "\n",
        "        # first modified/training specific non-max supresion NMS via zero-masking\n",
        "        # NMS explanation https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/\n",
        "        # second one-masking of rescaled ground truths\n",
        "        for b in range(B):                          #iterate in target target item length. target=(obj,classes,anchors) \n",
        "            p_box = pred_boxes[b*Ad: (b+1)*Ad].t()  #take predicted boxes per single image\n",
        "            ious = torch.zeros(Ad)                  #prepare zero mask able to cover whole image\n",
        "            box = target[b].view(-1,5)              #take each target boxes per single image\n",
        "\n",
        "            # this part is for comparing (iou) targets vs predictions and to ignore predictions\n",
        "            # under predefined treshold by setting their confidence masks to zeros\n",
        "            for t in range(box.size(0)):    # iter through all boxes of one target image \n",
        "                if box[t][1] == 0:\n",
        "                    break\n",
        "                cx = box[t][0] * W          #take each target box metrics (of one image) \n",
        "                cy = box[t][1] * H          #and rescale it to predicted output size for IoU almost = NMS later etc.\n",
        "                w  = box[t][2] * W\n",
        "                h  = box[t][3] * H\n",
        "                t_box = torch.FloatTensor([cx, cy, w, h]).repeat(Ad, 1).t() #single target box from target picture\n",
        "                ious = torch.max(ious, SoftIoUnion(p_box, t_box))           #iou target vs predicted\n",
        "            ignore = ious > ignore_threshold                           \n",
        "            conf_mask[b][ignore.view(A, H, W)] = 0  #we eliminate predicted boxes, nearest neigbourhood(above treshold IoU) \n",
        "                                                    #of ground truth, because we backpropagate based on ground truth\n",
        "                                                    #eliminated predicted boxes, are actually nearest to ground truth\n",
        "                                                    #this is trainings type on non-max suppression NMS\n",
        "            # here we create positive score=1 etc. masks grouped by most relevant anchors\n",
        "            # theis is not a prediction part, everything is given and relevant anchors are just\n",
        "            # anchors inside speciific interest cells\n",
        "            for t in range(box.size(0)):            # again, iter through all boxes of one target image \n",
        "                if box[t][1] == 0:\n",
        "                    break\n",
        "                    \n",
        "                cx = box[t][0] * W                  # again rescaling everything to the scale of predicted boxes\n",
        "                cy = box[t][1] * H\n",
        "                ci, cj = int(cx), int(cy) \n",
        "                w  = (box[t][2] * W).float()\n",
        "                h  = (box[t][3] * H).float()      \n",
        "\n",
        "                tmp_box = torch.FloatTensor([0, 0, w, h]).repeat(A, 1).t()#mask x,y. take w,h of each target box\n",
        "                #then we make ground truth anchor box info comparable with 0,0,w,h shape of each target box:\n",
        "                a_box = torch.cat((torch.zeros(A, As), anchors), 1).t() \n",
        "\n",
        "                #then we select groud truth boxes that correspond to ground truth anchors info(the most) best_a\n",
        "                _, best_a = torch.max( SoftIoUnion(tmp_box, a_box), 0)  \n",
        "\n",
        "                box_mask  [b][best_a][cj][ci] = 1 #we mask with anchor info verified groundtruth boxes with ones\n",
        "                conf_mask [b][best_a][cj][ci] = 1\n",
        "                cls_mask  [b][best_a][cj][ci] = 1\n",
        "                \n",
        "                t_boxes[0][b][best_a][cj][ci] = cx - ci #this is again to keep bbox center coordinates between 0-1\n",
        "                t_boxes[1][b][best_a][cj][ci] = cy - cj #remember we did sigmoid for same purpose. This is because\n",
        "                #we bounding box center coordinates are calculated relative to 'grid cell'-s left top corner\n",
        "                #and this 'grid cell' is basically the receptive field of neuron. So we need just fractions.\n",
        "\n",
        "                t_boxes[2][b][best_a][cj][ci] = math.log(w/anchors[best_a][0])\n",
        "                t_boxes[3][b][best_a][cj][ci] = math.log(h/anchors[best_a][1])\n",
        "                t_conf    [b][best_a][cj][ci] = 1\n",
        "                t_cls     [b][best_a][cj][ci] = box[t][4]\n",
        "\n",
        "        return box_mask, conf_mask, cls_mask, t_boxes, t_conf, t_cls\n",
        "\n",
        "\n",
        "    def get_loss(self, output, target, return_single_value=True):\n",
        "        device = output.device\n",
        "\n",
        "        anchors = self.anchors.to(device)\n",
        "\n",
        "        B, c, H, W = output.data.size()\n",
        "        A = len(anchors)\n",
        "        C = self.num_classes\n",
        "        Ad = B*A*H*W\n",
        "\n",
        "        output = output.view(B, A, (C+5), H, W)\n",
        "\n",
        "        ix = torch.LongTensor(range(0,5)).to(device)\n",
        "        p_boxes = output.index_select(2, ix[0:4]).view(B*A, -1, H*W).transpose(0,1).contiguous().view(4, Ad)\n",
        "        \n",
        "        p_boxes[0:2] = p_boxes[0:2].sigmoid()\n",
        "        p_boxes[2:4] = p_boxes[2:4].exp()\n",
        "        p_conf = output.index_select(2, ix[4]).view(B, A, H, W).sigmoid()\n",
        "\n",
        "        Gx = torch.arange(0, W).repeat(B*A, H).view(Ad).to(device) #torch.arange(0, W)-->ascending from 0 to W \n",
        "        Gy = torch.arange(0, H).repeat(W, 1).t().repeat(B*A, 1).view(Ad).to(device)\n",
        "        Aw = anchors.index_select(1, ix[0]).repeat(1, B*H*W).view(Ad)\n",
        "        Ah = anchors.index_select(1, ix[1]).repeat(1, B*H*W).view(Ad)\n",
        "\n",
        "        pred_boxes = torch.FloatTensor(4, Ad).to(device)\n",
        "        pred_boxes[0] = p_boxes[0] + Gx\n",
        "        pred_boxes[1] = p_boxes[1] + Gy\n",
        "        pred_boxes[2] = p_boxes[2] * Aw\n",
        "        pred_boxes[3] = p_boxes[3] * Ah \n",
        "        pred_boxes = pred_boxes.transpose(0,1).contiguous().view(-1,4)\n",
        "\n",
        "        box_mask, conf_mask, cls_mask, t_boxes, t_conf, t_cls = \\\n",
        "            self.build_targets(pred_boxes.detach(), target.detach(), anchors.detach(), H, W)\n",
        "\n",
        "        Gc = torch.arange(5, C+5).long().to(device)\n",
        "        p_cls  = output.index_select(2, Gc)\n",
        "        p_cls  = p_cls.view(B*A, C, H*W).transpose(1,2).contiguous().view(Ad, C)\n",
        "        cls_mask = (cls_mask == 1)\n",
        "        t_cls = t_cls[cls_mask].long().view(-1)\n",
        "        cls_mask = cls_mask.view(-1, 1).repeat(1, C).to(device)\n",
        "        p_cls = p_cls[cls_mask].view(-1, C)\n",
        "        \n",
        "        t_boxes = t_boxes.view(4, Ad).to(device)\n",
        "        t_conf = t_conf.to(device)\n",
        "        t_cls = t_cls.to(device)\n",
        "        box_mask = box_mask.view(Ad).to(device)\n",
        "        conf_mask = conf_mask.to(device)\n",
        "\n",
        "        box_loss = torch.nn.MSELoss(reduction='sum')(p_boxes *box_mask, t_boxes *box_mask)\n",
        "        conf_loss = torch.nn.MSELoss(reduction='sum')(p_conf *conf_mask, t_conf *conf_mask)\n",
        "        cls_loss = torch.nn.CrossEntropyLoss(reduction='sum')(p_cls, t_cls) if p_cls.size(0) > 0 else 0\n",
        "        loss = box_loss/2 + conf_loss + cls_loss\n",
        "\n",
        "        if math.isnan(loss.item()):\n",
        "            print(p_conf, t_conf)            \n",
        "            raise ValueError('YoloLayer has isnan in loss')\n",
        "        \n",
        "        if return_single_value:\n",
        "            return loss\n",
        "        else:\n",
        "            return [loss, box_loss/2, conf_loss, cls_loss]\n",
        "\n",
        "\n",
        "class Yolov3Base(nn.Module, metaclass=ABCMeta):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_loss_layers(self):\n",
        "        return [self.yolo_0, self.yolo_1]\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, c, h, w = x.shape\n",
        "        assert c==3 and h%32==0 and w%32==0, f\"Tensor shape should be [bs, 3, x*32, y*32], but was {x.shape}\"\n",
        "        xb = self.forward_backbone(x)\n",
        "        return self.forward_yolo(xb)\n",
        "\n",
        "    def predict(self, x, threshold=0.25):\n",
        "        self.eval()\n",
        "        if x.dim() == 3:\n",
        "            x = x.unsqueeze(0) \n",
        "        outputs = self.forward(x)\n",
        "        return self.boxes_from_output(outputs, threshold)\n",
        "    \n",
        "    def boxes_from_output(self, outputs, threshold=0.25):\n",
        "        boxes = [[] for j in range(outputs[0].size(0))]\n",
        "        for i, layer in enumerate(self.get_loss_layers()):\n",
        "            layer_boxes = layer.get_region_boxes(outputs[i], threshold=threshold)\n",
        "            for j, layer_box in enumerate(layer_boxes):\n",
        "                boxes[j] += layer_box\n",
        "        return boxes\n",
        "\n",
        "    def freeze_backbone(self, requires_grad=False):\n",
        "        for p in self.backbone.parameters():\n",
        "            p.requires_grad=requires_grad\n",
        "    def unfreeze(self):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = True\n",
        "    def freeze_info(self, print_all=False):\n",
        "        d = defaultdict(set)\n",
        "        print(\"Layer: param.requires_grad\")\n",
        "        for name, param in self.named_parameters():\n",
        "            if print_all:\n",
        "                print(f\"{name}: {param.requires_grad}\")\n",
        "            else:\n",
        "                d[name.split('.')[0]].add(param.requires_grad)\n",
        "        if not print_all:\n",
        "            for k,v in d.items():\n",
        "                print(k, ': ', v)        \n",
        "\n",
        "    def load_backbone(self, h5_path):\n",
        "        state_old = self.state_dict()\n",
        "        state_new = torch.load(h5_path)\n",
        "\n",
        "        skipped_layers = []\n",
        "        for k in list(state_new.keys()):\n",
        "            if state_old[k].shape != state_new[k].shape:\n",
        "                skipped_layers.append(k)\n",
        "                del state_new[k]\n",
        "\n",
        "        return self.load_state_dict(state_new, strict=False), skipped_layers\n",
        "\n",
        "\n",
        "class ConvBN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None):\n",
        "        super().__init__()\n",
        "        if padding is None:\n",
        "            padding = (kernel_size - 1)//2\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.01)\n",
        "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, stride=2):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "    def forward(self, x):\n",
        "        assert(x.data.dim() == 4)\n",
        "        return nn.Upsample(scale_factor=self.stride, mode='nearest')(x)\n",
        "\n",
        "class Yolov3UpsamplePrep(nn.Module):\n",
        "    def __init__(self, filters_list, in_filters, out_filters):\n",
        "        super().__init__()\n",
        "        self.branch = nn.ModuleList([\n",
        "                        ConvBN(in_filters, filters_list[0], 1),\n",
        "                        ConvBN(filters_list[0], filters_list[1], kernel_size=3),\n",
        "                        ConvBN(filters_list[1], filters_list[0], kernel_size=1),\n",
        "                        ConvBN(filters_list[0], filters_list[1], kernel_size=3),\n",
        "                        ConvBN(filters_list[1], filters_list[0], kernel_size=1),])\n",
        "        self.for_yolo = nn.ModuleList([\n",
        "                        ConvBN(filters_list[0], filters_list[1], kernel_size=3),\n",
        "                        nn.Conv2d(filters_list[1], out_filters, kernel_size=1, stride=1,\n",
        "                                   padding=0, bias=True)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.branch: x = m(x)\n",
        "        branch_out = x\n",
        "        for m in self.for_yolo: x = m(x)\n",
        "        return branch_out, x\n",
        "\n",
        "class DarknetBlock(nn.Module):\n",
        "    def __init__(self, ch_in):\n",
        "        super().__init__()\n",
        "        ch_hid = ch_in//2\n",
        "        self.conv1 = ConvBN(ch_in, ch_hid, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv2 = ConvBN(ch_hid, ch_in, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x): return self.conv2(self.conv1(x)) + x\n",
        "\n",
        "\n",
        "class Darknet(nn.Module):\n",
        "    def __init__(self, num_blocks, start_nf=32):\n",
        "        super().__init__()\n",
        "        nf = start_nf\n",
        "        self.base = ConvBN(3, nf, kernel_size=3, stride=1)\n",
        "        self.layers = []\n",
        "        for i, nb in enumerate(num_blocks):\n",
        "            dn_layer = self.make_group_layer(nf, nb, stride=2)\n",
        "            self.add_module(f\"darknet_{i}\", dn_layer)\n",
        "            self.layers.append(dn_layer)\n",
        "            nf *= 2\n",
        "\n",
        "    def make_group_layer(self, ch_in, num_blocks, stride=2):\n",
        "        layers = [ConvBN(ch_in, ch_in*2, stride=stride)]\n",
        "        for i in range(num_blocks): layers.append(DarknetBlock(ch_in*2))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = [self.base(x)]\n",
        "        for l in self.layers:\n",
        "            y.append(l(y[-1]))\n",
        "        return y\n",
        "\n",
        "\n",
        "class Yolov3(Yolov3Base):\n",
        "    def __init__(self, num_classes=80):\n",
        "        super().__init__()\n",
        "        self.backbone = Darknet([1,2,8,8,4])\n",
        "        \n",
        "        A = 3\n",
        "        self.yolo_0_pre = Yolov3UpsamplePrep([512, 1024], 1024, A*(num_classes+5))\n",
        "        self.yolo_0 = YoloLayer(anchors=[(116.,  90.), (156., 198.), (373., 326.)], stride=32, num_classes=num_classes)\n",
        "\n",
        "        self.yolo_1_c = ConvBN(512, 256, 1)\n",
        "        self.yolo_1_prep = Yolov3UpsamplePrep([256, 512], 512+256, A*(num_classes+5))\n",
        "        self.yolo_1 = YoloLayer(anchors=[(30., 61.), (62., 45.), (59., 119.)], stride=16, num_classes=num_classes)\n",
        "\n",
        "        self.yolo_2_c = ConvBN(256, 128, 1)\n",
        "        self.yolo_2_prep = Yolov3UpsamplePrep([128, 256], 256+128, A*(num_classes+5))\n",
        "        self.yolo_2 = YoloLayer(anchors=[(10., 13.), (16., 30.), (33., 23.)], stride=8, num_classes=num_classes)\n",
        "\n",
        "    def get_loss_layers(self):\n",
        "        return [self.yolo_0, self.yolo_1, self.yolo_2]\n",
        "\n",
        "    def forward_yolo(self, xb):\n",
        "        x, y0 = self.yolo_0_pre(xb[-1])\n",
        "\n",
        "        x = self.yolo_1_c(x)\n",
        "        x = nn.Upsample(scale_factor=2, mode='nearest')(x)\n",
        "        x = torch.cat([x, xb[-2]], 1)\n",
        "        x, y1 = self.yolo_1_prep(x)\n",
        "\n",
        "        x = self.yolo_2_c(x)\n",
        "        x = nn.Upsample(scale_factor=2, mode='nearest')(x)\n",
        "        x = torch.cat([x, xb[-3]], 1)\n",
        "        x, y2 = self.yolo_2_prep(x)\n",
        "        \n",
        "        return [y0, y1, y2]\n",
        "\n",
        "\n",
        "#------------------------------- here starts tiny yolo \n",
        "\n",
        "\n",
        "class MaxPoolStride1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.pad(x, (0,1,0,1), mode='replicate'), 2, stride=1)\n",
        "        return x\n",
        "\n",
        "class Yolov3TinyBackbone(nn.Module):\n",
        "    def __init__(self, input_channels=3):\n",
        "        super().__init__()\n",
        "        self.layers =  nn.Sequential(OrderedDict([\n",
        "            ('0_convbatch',     ConvBN(input_channels, 16, 3, 1, 1)),\n",
        "            ('1_max',           nn.MaxPool2d(2, 2)),\n",
        "            ('2_convbatch',     ConvBN(16, 32, 3, 1, 1)),\n",
        "            ('3_max',           nn.MaxPool2d(2, 2)),\n",
        "            ('4_convbatch',     ConvBN(32, 64, 3, 1, 1)),\n",
        "            ('5_max',           nn.MaxPool2d(2, 2)),\n",
        "            ('6_convbatch',     ConvBN(64, 128, 3, 1, 1)),\n",
        "            ('7_max',           nn.MaxPool2d(2, 2)),\n",
        "            ('8_convbatch',     ConvBN(128, 256, 3, 1, 1)),\n",
        "            ('9_max',           nn.MaxPool2d(2, 2)),\n",
        "            ('10_convbatch',    ConvBN(256, 512, 3, 1, 1)),\n",
        "            ('11_max',          MaxPoolStride1()),\n",
        "            ('12_convbatch',    ConvBN(512, 1024, 3, 1, 1)),\n",
        "            ('13_convbatch',    ConvBN(1024, 256, 1, 1, 0))]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        idx=9\n",
        "        x0 = self.layers[ :idx](x)\n",
        "        x1 = self.layers[idx: ](x0)\n",
        "        return x0, x1\n",
        "\n",
        "\n",
        "class Yolov3Tiny(Yolov3Base):\n",
        "\n",
        "    def __init__(self, num_classes, use_wrong_anchors=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.return_out_boxes = False\n",
        "        self.skip_backbone = False\n",
        "\n",
        "        self.backbone = Yolov3TinyBackbone()\n",
        "\n",
        "        Ar = 3\n",
        "        self.yolo_0_pre = nn.Sequential(OrderedDict([\n",
        "            ('14_convbatch',    ConvBN(256, 512, 3, 1, 1)),\n",
        "            ('15_conv',         nn.Conv2d(512, (num_classes+5)*Ar, 1, 1, 0))]))\n",
        "        \n",
        "        anchors0=[(81.,82.), (135.,169.), (344.,319.)]\n",
        "        self.yolo_0 = YoloLayer(anchors=anchors0, stride=32, num_classes=num_classes)\n",
        "\n",
        "        self.up_1 = nn.Sequential(OrderedDict([\n",
        "            ('17_convbatch',    ConvBN(256, 128, 1, 1, 0)),\n",
        "            ('18_upsample',     Upsample(2))]))\n",
        "\n",
        "        self.yolo_1_pre = nn.Sequential(OrderedDict([\n",
        "            ('19_convbatch',    ConvBN(128+256, 256, 3, 1, 1)),\n",
        "            ('20_conv',         nn.Conv2d(256, (num_classes+5)*Ar, 1, 1, 0))]))\n",
        "\n",
        "        if use_wrong_anchors:\n",
        "            anchors1 = [(23.,27.),  (37.,58.),  (81.,82.)]\n",
        "        else: \n",
        "            anchors1 = [(10.,14.),  (23.,27.),  (37.,58.)]\n",
        "\n",
        "        self.yolo_1 = YoloLayer(anchors=anchors1, stride=16, num_classes=num_classes)\n",
        "\n",
        "    def get_loss_layers(self):\n",
        "        return [self.yolo_0, self.yolo_1]\n",
        "\n",
        "    def forward_yolo(self, xb):\n",
        "        x0, x1 = xb[0], xb[1]\n",
        "        y0 = self.yolo_0_pre(x1)\n",
        "\n",
        "        x_up = self.up_1(x1)\n",
        "        x_up = torch.cat((x_up, x0), 1)\n",
        "        y1 = self.yolo_1_pre(x_up)\n",
        "        return [y0, y1]\n"
      ],
      "metadata": {
        "id": "mX6AZ3Bb2Ddn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title set up for training { form-width: \"20%\" }\n",
        "\n",
        "path = '/content/' \n",
        "\n",
        "train_set = VOCDataset(path, input_shape=(320, 320), mode='train', dataset='voc')\n",
        "train_loaded = DataLoader(train_set, batch_size=1, shuffle=False, drop_last=True)\n",
        "\n",
        "model = Yolov3Tiny(num_classes=len(train_set.classes))         \n",
        "\n",
        "x, y = train_set.__getitem__(10)\n",
        "PlotSample(x, y, dataset='voc')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KndIK_PsVTMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training { form-width: \"20%\" }\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters()) \n",
        "\n",
        "def YoloLoss(model, pred, target):\n",
        "    layers=model.get_loss_layers()\n",
        "    loss=0.\n",
        "    for i, layer in enumerate(layers):\n",
        "        loss += layer.get_loss(pred[i], target, return_single_value=True)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def Train(model, criterion, optimizer, train_loaded, epochs=5):\n",
        "    train_losses=[]\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        losses=0.\n",
        "        for x, y in tqdm(train_loaded):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(model, pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses+=loss\n",
        "        train_losses.append(losses/len(train_loaded))\n",
        "        print('Epoch {}/{} --- Loss: {:.4f}'.format(epoch+1, epochs, train_losses[-1]))\n",
        "    return train_losses\n",
        "\n",
        "loss = Train(model, YoloLoss, optimizer, train_loaded, epochs=20) # epochs=20 gives good result"
      ],
      "metadata": {
        "id": "3QBSwY-f2DqC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plotting the loss { form-width: \"20%\" }\n",
        "\n",
        "print(loss[0].item())\n",
        "\n",
        "items = [l.item() for l in loss]\n",
        "plt.plot(items)\n",
        "plt.title('Loss');"
      ],
      "metadata": {
        "id": "_ec1FboggxbN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title save and load trained { form-width: \"20%\" }\n",
        "\n",
        "torch.save(model.state_dict(), 'yolov3_tiny.pth') #torch.save(model.state_dict(), 'yolov3_tiny.h5')\n",
        "\n",
        "model = Yolov3Tiny(num_classes=20)    # voc has 20 and coco 80 clases\n",
        "model.load_state_dict(torch.load('/content/yolov3_tiny.pth')) # model.load_state_dict(torch.load('/content/yolov3_tiny.h5'))"
      ],
      "metadata": {
        "id": "sjoCihMg2D2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef4b359-4b6f-4f0e-a742-84d3b58786e6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title detect { form-width: \"20%\" }\n",
        "\n",
        "image = '/content/JPEGImages/000200.jpg'\n",
        "model.to('cpu') # or vice versa put image to GPU but you'll need to use PIL first\n",
        "\n",
        "%time\n",
        "PictureDetection(model, image, size=(320,320), dataset='voc')"
      ],
      "metadata": {
        "id": "Dh4lUbai2EaT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title did not like results here more training { form-width: \"20%\" }\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters()) \n",
        "# model.to(device) \n",
        "\n",
        "# loss = Train(model, YoloLoss, optimizer, train_loaded, epochs=1)"
      ],
      "metadata": {
        "id": "7R994_w57O0I",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}