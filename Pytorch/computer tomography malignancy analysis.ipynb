{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data available: \n",
        "\n",
        "first download Nbia data retriever software than the data\n",
        "\n",
        "https://luna16.grand-challenge.org/Data/ \n",
        "\n",
        "https://luna16.grand-challenge.org/Download/\n",
        "\n",
        "Note! If page gives 'Not Found' prompt you need to press join button.\n",
        "\n",
        "Data is too big, so we will use only one single part, subset of it.\n",
        "\n",
        "R E F E R E N C E: \n",
        "\n",
        "Deep Learning with PyTorch,  by Eli Stevens, Luca Antiga, and Thomas Viehmann"
      ],
      "metadata": {
        "id": "yUe7j_9EKH8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download data\n",
        "\n",
        "## here we use only one subset of data: subset0\n",
        "\n",
        "# The candidates.csv file contains information about all lumps that potentially look like\n",
        "# nodules, whether those lumps are malignant, benign tumors, or something else altogether\n",
        "\n",
        "# The annotations.csv file contains size based information about nearly 1200 of the candidates that\n",
        "# have been flagged as nodules. \n",
        "\n",
        "!wget -O annotations.csv -q https://zenodo.org/record/3723295/files/annotations.csv?download=1\n",
        "!wget -O candidates.csv -q https://zenodo.org/record/3723295/files/candidates.csv?download=1\n",
        "#!wget -O sampleSubmission.csv -q https://zenodo.org/record/3723295/files/sampleSubmission.csv?download=1\n",
        "\n",
        "!wget -O candidates_V2.zip -q https://zenodo.org/record/3723295/files/candidates_V2.zip?download=1\n",
        "!wget -O evaluationScript.zip -q https://zenodo.org/record/3723295/files/evaluationScript.zip?download=1\n",
        "!wget -O seg-lungs-LUNA16.zip -q https://zenodo.org/record/3723295/files/seg-lungs-LUNA16.zip?download=1\n",
        "!wget -O subset0.zip -q https://zenodo.org/record/3723295/files/subset0.zip?download=1\n",
        "\n",
        "!brew install p7zip\n",
        "                                 ### https://unix.stackexchange.com/questions/115825/extra-bytes-error-when-unzipping-a-file\n",
        "!7za x candidates_V2.zip         # instead of !unzip candidates_V2.zip\n",
        "!7za x evaluationScript.zip      # stead of !unzip evaluationScript.zip\n",
        "!7za x seg-lungs-LUNA16.zip      # stead of !unzip seg-lungs-LUNA16.zip\n",
        "!7za x subset0.zip               # stead of !unzip subset0.zip \n",
        "\n",
        "# !rm candidates_V2.zip\n",
        "# !rm evaluationScript.zip\n",
        "# !rm seg-lungs-LUNA16.zip\n",
        "# !rm subset0.zip   "
      ],
      "metadata": {
        "id": "-chqJ4ZLNykZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title installs and imports\n",
        "\n",
        "## some solutions taken from here: \n",
        "## https://github.com/JonathanSum/pytorch-Deep-Learning_colab/blob/master/p2ch10_explore_data.ipynb\n",
        "\n",
        "!pip install -q diskcache\n",
        "!pip install -q SimpleITK\n",
        "!pip install cassandra-driver\n",
        "\n",
        "import cassandra\n",
        "from cassandra.cqltypes import BytesType\n",
        "from diskcache import FanoutCache, Disk,core\n",
        "from diskcache.core import io\n",
        "from io import BytesIO\n",
        "from diskcache.core import MODE_BINARY\n",
        "\n",
        "import SimpleITK as sitk\n",
        "\n",
        "import logging\n",
        "import logging.handlers\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import random\n",
        "\n",
        "import gzip\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import scipy.ndimage.morphology as morph\n",
        "import scipy.ndimage.measurements as measurements\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import datetime\n",
        "import hashlib\n",
        "import shutil\n",
        "import socket\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import SGD, Adam\n"
      ],
      "metadata": {
        "id": "pqDbrDYnElwQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title conversions and tools for efficiency { form-width: \"20%\" }\n",
        "\n",
        "# to convert xyz values into voxels manually, we have to follow these 4 steps:\n",
        "      # 1 Flip the coordinates from IRC [index,raw,column] to CRI [column,raw,index], to align with XYZ.\n",
        "      # 2 Scale the indices with the voxel sizes.\n",
        "      # 3 Matrix-multiply with the directions matrix, using @ in Python.\n",
        "      # 4 Add the offset for the origin.\n",
        "\n",
        "# The metadata we need to convert from patient coordinates (_xyz) to array coordinates (_irc)\n",
        "#  is contained in the MetaIO file alongside the CT data itself\n",
        "\n",
        "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col']) \n",
        "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
        "\n",
        "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
        "    cri_a = np.array(coord_irc)[::-1]                       #Swaps the order while we convert to a NumPy array\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a  # The last three steps of 4 step plan, all in one line\n",
        "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
        "    return XyzTuple(*coords_xyz)\n",
        "\n",
        "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coord_a = np.array(coord_xyz)\n",
        "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a  # Inverse of the last three steps of 4 step plan\n",
        "    cri_a = np.round(cri_a)\n",
        "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))            # Shuffle and convert to integers\n",
        "\n",
        "\n",
        "def importstr(module_str, from_=None):\n",
        "    \"\"\"\n",
        "    >>> importstr('os')\n",
        "    <module 'os' from '.../os.pyc'>\n",
        "    >>> importstr('math', 'fabs')\n",
        "    <built-in function fabs>\n",
        "    \"\"\"\n",
        "    if from_ is None and ':' in module_str:\n",
        "        module_str, from_ = module_str.rsplit(':')\n",
        "\n",
        "    module = __import__(module_str)\n",
        "    for sub_str in module_str.split('.')[1:]:\n",
        "        module = getattr(module, sub_str)\n",
        "\n",
        "    if from_:\n",
        "        try:\n",
        "            return getattr(module, from_)\n",
        "        except:\n",
        "            raise ImportError('{}.{}'.format(module_str, from_))\n",
        "    return module\n",
        "\n",
        "\n",
        "\n",
        "def prhist(ary, prefix_str=None, **kwargs):\n",
        "    if prefix_str is None:\n",
        "        prefix_str = ''\n",
        "    else:\n",
        "        prefix_str += ' '\n",
        "\n",
        "    count_ary, bins_ary = np.histogram(ary, **kwargs)\n",
        "    for i in range(count_ary.shape[0]):\n",
        "        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n",
        "    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n",
        "\n",
        "\n",
        "def enumerateWithEstimate(\n",
        "        iter,\n",
        "        desc_str,\n",
        "        start_ndx=0,\n",
        "        print_ndx=4,\n",
        "        backoff=None,\n",
        "        iter_len=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    In terms of behavior, `enumerateWithEstimate` is almost identical\n",
        "    to the standard `enumerate` (the differences are things like how\n",
        "    our function returns a generator, while `enumerate` returns a\n",
        "    specialized `<enumerate object at 0x...>`).\n",
        "\n",
        "    However, the side effects (logging, specifically) are what make the\n",
        "    function interesting.\n",
        "\n",
        "    :param iter: `iter` is the iterable that will be passed into\n",
        "        `enumerate`. Required.\n",
        "\n",
        "    :param desc_str: This is a human-readable string that describes\n",
        "        what the loop is doing. The value is arbitrary, but should be\n",
        "        kept reasonably short. Things like `\"epoch 4 training\"` or\n",
        "        `\"deleting temp files\"` or similar would all make sense.\n",
        "\n",
        "    :param start_ndx: This parameter defines how many iterations of the\n",
        "        loop should be skipped before timing actually starts. Skipping\n",
        "        a few iterations can be useful if there are startup costs like\n",
        "        caching that are only paid early on, resulting in a skewed\n",
        "        average when those early iterations dominate the average time\n",
        "        per iteration.\n",
        "\n",
        "        NOTE: Using `start_ndx` to skip some iterations makes the time\n",
        "        spent performing those iterations not be included in the\n",
        "        displayed duration. Please account for this if you use the\n",
        "        displayed duration for anything formal.\n",
        "\n",
        "        This parameter defaults to `0`.\n",
        "\n",
        "    :param print_ndx: determines which loop interation that the timing\n",
        "        logging will start on. The intent is that we don't start\n",
        "        logging until we've given the loop a few iterations to let the\n",
        "        average time-per-iteration a chance to stablize a bit. We\n",
        "        require that `print_ndx` not be less than `start_ndx` times\n",
        "        `backoff`, since `start_ndx` greater than `0` implies that the\n",
        "        early N iterations are unstable from a timing perspective.\n",
        "\n",
        "        `print_ndx` defaults to `4`.\n",
        "\n",
        "    :param backoff: This is used to how many iterations to skip before\n",
        "        logging again. Frequent logging is less interesting later on,\n",
        "        so by default we double the gap between logging messages each\n",
        "        time after the first.\n",
        "\n",
        "        `backoff` defaults to `2` unless iter_len is > 1000, in which\n",
        "        case it defaults to `4`.\n",
        "\n",
        "    :param iter_len: Since we need to know the number of items to\n",
        "        estimate when the loop will finish, that can be provided by\n",
        "        passing in a value for `iter_len`. If a value isn't provided,\n",
        "        then it will be set by using the value of `len(iter)`.\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if iter_len is None:\n",
        "        iter_len = len(iter)\n",
        "\n",
        "    if backoff is None:\n",
        "        backoff = 2\n",
        "        while backoff ** 7 < iter_len:\n",
        "            backoff *= 2\n",
        "\n",
        "    assert backoff >= 2\n",
        "    while print_ndx < start_ndx * backoff:\n",
        "        print_ndx *= backoff\n",
        "\n",
        "    start_ts = time.time()\n",
        "    for (current_ndx, item) in enumerate(iter):\n",
        "        yield (current_ndx, item)\n",
        "        if current_ndx == print_ndx:\n",
        "            # ... <1>\n",
        "            duration_sec = ((time.time() - start_ts)\n",
        "                            / (current_ndx - start_ndx + 1)\n",
        "                            * (iter_len-start_ndx)\n",
        "                            )\n",
        "\n",
        "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
        "            done_td = datetime.timedelta(seconds=duration_sec)\n",
        "\n",
        "            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n",
        "                desc_str,\n",
        "                current_ndx,\n",
        "                iter_len,\n",
        "                str(done_dt).rsplit('.', 1)[0],\n",
        "                str(done_td).rsplit('.', 1)[0],\n",
        "            ))\n",
        "\n",
        "            print_ndx *= backoff\n",
        "\n",
        "        if current_ndx + 1 == start_ndx:\n",
        "            start_ts = time.time()\n",
        "\n",
        "\n",
        "\n",
        "####### logging\n",
        "root_logger = logging.getLogger()\n",
        "root_logger.setLevel(logging.INFO)\n",
        "\n",
        "# Some libraries attempt to add their own root logger handlers. This is\n",
        "# annoying and so we get rid of them.\n",
        "for handler in list(root_logger.handlers):\n",
        "    root_logger.removeHandler(handler)\n",
        "\n",
        "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
        "formatter = logging.Formatter(logfmt_str)\n",
        "\n",
        "streamHandler = logging.StreamHandler()\n",
        "streamHandler.setFormatter(formatter)\n",
        "streamHandler.setLevel(logging.DEBUG)\n",
        "\n",
        "root_logger.addHandler(streamHandler)\n",
        "\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "# log.setLevel(logging.WARN)\n",
        "log.setLevel(logging.INFO)\n",
        "# log.setLevel(logging.DEBUG)\n",
        "\n",
        "####### cache\n",
        "\n",
        "class GzipDisk(Disk):\n",
        "    def store(self, value, read, key=None):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "\n",
        "        :param value: value to convert\n",
        "        :param bool read: True when value is file-like object\n",
        "        :return: (size, mode, filename, value) tuple for Cache table\n",
        "        \"\"\"\n",
        "        # pylint: disable=unidiomatic-typecheck\n",
        "        if type(value) is BytesType:\n",
        "            if read:\n",
        "                value = value.read()\n",
        "                read = False\n",
        "\n",
        "            str_io = BytesIO()\n",
        "            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n",
        "\n",
        "            for offset in range(0, len(value), 2**30):\n",
        "                gz_file.write(value[offset:offset+2**30])\n",
        "            gz_file.close()\n",
        "\n",
        "            value = str_io.getvalue()\n",
        "\n",
        "        return super(GzipDisk, self).store(value, read)\n",
        "\n",
        "\n",
        "    def fetch(self, mode, filename, value, read):\n",
        "        \"\"\"\n",
        "        Override from base class diskcache.Disk.\n",
        "\n",
        "        Chunking is due to needing to work on pythons < 2.7.13:\n",
        "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
        "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
        "          compression and decompression operations did not properly handle results of\n",
        "          2 or 4 GiB.\n",
        "\n",
        "        :param int mode: value mode raw, binary, text, or pickle\n",
        "        :param str filename: filename of corresponding value\n",
        "        :param value: database value\n",
        "        :param bool read: when True, return an open file handle\n",
        "        :return: corresponding Python value\n",
        "        \"\"\"\n",
        "        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n",
        "\n",
        "        if mode == MODE_BINARY:\n",
        "            str_io = BytesIO(value)\n",
        "            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n",
        "            read_csio = BytesIO()\n",
        "\n",
        "            while True:\n",
        "                uncompressed_data = gz_file.read(2**30)\n",
        "                if uncompressed_data:\n",
        "                    read_csio.write(uncompressed_data)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            value = read_csio.getvalue()\n",
        "\n",
        "        return value\n",
        "\n",
        "def getCache(scope_str):\n",
        "    return FanoutCache('data-unversioned/cache/' + scope_str,\n",
        "                       disk=GzipDisk,\n",
        "                       shards=64,\n",
        "                       timeout=1,\n",
        "                       size_limit=3e11,\n",
        "                       # disk_min_file_size=2**20,\n",
        "                       )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IhFzxIgHE8FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title general data handling\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "# log.setLevel(logging.WARN)\n",
        "# log.setLevel(logging.INFO)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "raw_cache = getCache('general_raw')  #@@@@@@@@@@@@@@@@@@@  part2ch10_raw\n",
        "\n",
        "### task one : we combine candidates.csv and annotations.csv and take the infolist of nodule candidates from them\n",
        "\n",
        "CandidateInfoTuple = namedtuple(\n",
        "    'CandidateInfoTuple',\n",
        "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
        ")\n",
        "\n",
        "@functools.lru_cache(1)\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):\n",
        "    # We construct a set with all series_uids that are present on disk.\n",
        "    # This will let us use the data, even if we haven't downloaded all of\n",
        "    # the subsets yet.\n",
        "    mhd_list = glob.glob('/content/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
        "\n",
        "    diameter_dict = {}\n",
        "    with open('/content/annotations.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "\n",
        "            diameter_dict.setdefault(series_uid, []).append(\n",
        "                (annotationCenter_xyz, annotationDiameter_mm)\n",
        "            )\n",
        "\n",
        "    candidateInfo_list = []                         ## we do this to get nodule diameters if any is present\n",
        "    with open('/content/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            candidateDiameter_mm = 0.0\n",
        "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
        "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
        "                for i in range(3):\n",
        "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
        "                    # Divides the diameter by 2 to get the radius, and divides the radius by 2 to require that \n",
        "                    # the two nodule center points not be too far apart relative to the size of the nodule. \n",
        "                    # (This results in a bounding-box check, not a true distance check.)\n",
        "                    if delta_mm > annotationDiameter_mm / 4:\n",
        "                        break\n",
        "                else:\n",
        "                    candidateDiameter_mm = annotationDiameter_mm\n",
        "                    break\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(\n",
        "                isNodule_bool,\n",
        "                candidateDiameter_mm,\n",
        "                series_uid,\n",
        "                candidateCenter_xyz,\n",
        "            ))\n",
        "\n",
        "    candidateInfo_list.sort(reverse=True) #This means we have all of the actual nodule samples starting with the largest first, \n",
        "    return candidateInfo_list             #followed by all of the non-nodule samples (which don’t have nodule size information).\n",
        "\n",
        "\n",
        "### task two : as we already have info files about CT scans we load actual CTscan files with python object\n",
        "  # data is in .mhd for metadata and .raw image part. MetaIO is used format https://itk.org/Wiki/MetaIO/Documentation#Quick_Start\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            '/content/subset*/{}.mhd'.format(series_uid) # we do not have to care here which subset gets targeted\n",
        "        )[0]\n",
        "        # sitk.ReadImage implicitly consumes the .raw file in addition to the passed-in .mhd file.\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32) #convert the value type to np.float32\n",
        "\n",
        "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
        "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
        "        # The upper bound nukes any weird hotspots and clamps bone down\n",
        "        ct_a.clip(-1000, 1000, ct_a) # -1000 is an air and +1000 is bones already, so we are not interested\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
        "\n",
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(\n",
        "            center_xyz,\n",
        "            self.origin_xyz,\n",
        "            self.vxSize_xyz,\n",
        "            self.direction_a,\n",
        "        )\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc\n",
        "\n",
        "\n",
        "### task three : we bring CT data in as pytorch custom dataset + we extract and return each nodule candidate slice separately from any CT\n",
        "\n",
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,                   #if specify you will get same uid series belonging samples only   \n",
        "            ):\n",
        "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "\n",
        "        if series_uid:\n",
        "            self.candidateInfo_list = [\n",
        "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
        "            ]\n",
        "\n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "        elif val_stride > 0:\n",
        "            del self.candidateInfo_list[::val_stride] #Deletes the validation images (every val_stride-th item in the list) from \n",
        "            assert self.candidateInfo_list            #self.candidateInfo_list. We made a copy earlier so that we don’t alter the original list.\n",
        "\n",
        "        log.info(\"{!r}: {} {} samples\".format(\n",
        "            self,\n",
        "            len(self.candidateInfo_list),\n",
        "            \"validation\" if isValSet_bool else \"training\",\n",
        "        ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        # The return value candidate_a has shape (32,48,48); the axes are depth, height, and width.\n",
        "        candidate_a, center_irc = getCtRawCandidate(\n",
        "            candidateInfo_tup.series_uid,\n",
        "            candidateInfo_tup.center_xyz,\n",
        "            width_irc,\n",
        "        )\n",
        "\n",
        "        candidate_t = torch.from_numpy(candidate_a)\n",
        "        candidate_t = candidate_t.to(torch.float32)\n",
        "        candidate_t = candidate_t.unsqueeze(0)\n",
        "\n",
        "        pos_t = torch.tensor([\n",
        "                not candidateInfo_tup.isNodule_bool,\n",
        "                candidateInfo_tup.isNodule_bool\n",
        "            ],                                  \n",
        "            dtype=torch.long,\n",
        "        )  # nodule is there or not Zero or One\n",
        "\n",
        "        return (\n",
        "            candidate_t,                       # nodule candidate tensor itself\n",
        "            pos_t,                             # tensor([0, 1]) is there a nodule - no or yes \n",
        "            candidateInfo_tup.series_uid,      # ID like '1.3.6...287966244644280690737019247770'  \n",
        "            torch.tensor(center_irc),          # position of center of the nodule in the whole CT like: tensor([ 61, 350, 320]))\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JK89Pi-60wHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code for visualisations\n",
        "\n",
        "\n",
        "matplotlib.use('nbagg')\n",
        "\n",
        "clim=(-1000.0, 300)\n",
        "\n",
        "def findPositiveSamples(start_ndx=0, limit=100):\n",
        "    ds = LunaDataset()\n",
        "\n",
        "    positiveSample_list = []\n",
        "    for sample_tup in ds.candidateInfo_list:\n",
        "        if sample_tup.isNodule_bool:\n",
        "            print(len(positiveSample_list), sample_tup)\n",
        "            positiveSample_list.append(sample_tup)\n",
        "\n",
        "        if len(positiveSample_list) >= limit:\n",
        "            break\n",
        "\n",
        "    return positiveSample_list\n",
        "\n",
        "def showCandidate(series_uid, batch_ndx=None, **kwargs):\n",
        "    ds = LunaDataset(series_uid=series_uid, **kwargs)\n",
        "    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n",
        "\n",
        "    if batch_ndx is None:\n",
        "        if pos_list:\n",
        "            batch_ndx = pos_list[0]\n",
        "        else:\n",
        "            print(\"Warning: no positive samples found; using first negative sample.\")\n",
        "            batch_ndx = 0\n",
        "\n",
        "    ct = Ct(series_uid)\n",
        "    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n",
        "    ct_a = ct_t[0].numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 50))\n",
        "\n",
        "    group_list = [\n",
        "        [9, 11, 13],\n",
        "        [15, 16, 17],\n",
        "        [19, 21, 23],\n",
        "    ]\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n",
        "    subplot.set_title('index {}'.format(int(center_irc[0])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[int(center_irc[0])], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n",
        "    subplot.set_title('row {}'.format(int(center_irc[1])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,int(center_irc[1])], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n",
        "    subplot.set_title('col {}'.format(int(center_irc[2])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct.hu_a[:,:,int(center_irc[2])], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n",
        "    subplot.set_title('index {}'.format(int(center_irc[0])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n",
        "    subplot.set_title('row {}'.format(int(center_irc[1])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n",
        "    subplot.set_title('col {}'.format(int(center_irc[2])), fontsize=30)\n",
        "    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "        label.set_fontsize(20)\n",
        "    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for row, index_list in enumerate(group_list):\n",
        "        for col, index in enumerate(index_list):\n",
        "            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n",
        "            subplot.set_title('slice {}'.format(index), fontsize=30)\n",
        "            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n",
        "                label.set_fontsize(20)\n",
        "            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n",
        "\n",
        "\n",
        "    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dglhgK31KJaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see if data handling works properly\n",
        "\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
        "positiveInfo_list = [x for x in candidateInfo_list if x[0]]\n",
        "diameter_list = [x[1] for x in positiveInfo_list]\n",
        "\n",
        "for i in range(0, len(diameter_list), 100):\n",
        "    print('{:4} {:4.1f} mm'.format(i, diameter_list[i]))"
      ],
      "metadata": {
        "id": "CTyujke_KmBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see if code can get nodule samples correctly\n",
        "\n",
        "%matplotlib inline\n",
        "positiveSample_list = findPositiveSamples()\n",
        "series_uid = positiveSample_list[11][2]\n",
        "showCandidate(series_uid) "
      ],
      "metadata": {
        "id": "dVHEpv7wL7Py"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}